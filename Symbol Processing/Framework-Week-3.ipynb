{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Framework-Week-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# workshop folder, e.g. 'acmlab/workshops/project'\n","FOLDERNAME = 'primary_description'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_B2KowBG8Zx_","executionInfo":{"status":"ok","timestamp":1642980560407,"user_tz":480,"elapsed":15899,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"ffba88f5-32d3-4721-af54-6b699fa3a51a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/primary_description\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import os\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","from textblob import TextBlob\n","import nltk\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","\n","from IPython.display import display\n","\n","# import stanza\n","# stanza.install_corenlp()\n","# import spacy\n","# nltk.download('brown')"],"metadata":{"id":"l4pMi_3HCdxb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642980563210,"user_tz":480,"elapsed":2809,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"a71fa45c-29a8-45c6-fe31-63fbcd928df6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVkvU5UR6h6N"},"outputs":[],"source":["data = pd.read_json(\"Practice/cs.ai-ann0.json\")\n","# text_data = data.loc[\"text\"]\n","# text_data.to_dict()"]},{"cell_type":"code","source":["def extract_primary_description(entity): #Entity is a dictionary.\n","  output = {}\n","  for key in entity.keys():\n","    if entity[key]['label'] == \"PRIMARY\":\n","      output[key] = entity[key]\n","  return output\n"],"metadata":{"id":"fcac7jyqAIe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = nltk.WordNetLemmatizer()\n","def setup(text):\n","\n","  #word tokenizeing and part-of-speech tagger\n","  document = text\n","  tokens = [nltk.word_tokenize(sent) for sent in [document]]\n","  postag = [nltk.pos_tag(sent) for sent in tokens][0]\n","\n","  # Rule for NP chunk and VB Chunk\n","  grammar = r\"\"\"\n","      NBAR:\n","          {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n","          {<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?} # Verbs and Verb Phrases\n","          \n","      NP:\n","          {<NBAR>}\n","          {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n","          \n","  \"\"\"\n","  #Chunking\n","  cp = nltk.RegexpParser(grammar)\n","\n","  # the result is a tree\n","  tree = cp.parse(postag)\n","  return tree\n","\n","def leaves(tree):\n","    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n","    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):\n","        yield subtree.leaves()\n","        \n","def get_word_postag(word):\n","    if pos_tag([word])[0][1].startswith('J'):\n","        return wordnet.ADJ\n","    if pos_tag([word])[0][1].startswith('V'):\n","        return wordnet.VERB\n","    if pos_tag([word])[0][1].startswith('N'):\n","        return wordnet.NOUN\n","    else:\n","        return wordnet.NOUN\n","    \n","def normalise(word):\n","    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n","    # word = word.lower()\n","    # postag = get_word_postag(word)\n","    # word = lemmatizer.lemmatize(word, postag)\n","    return word\n","\n","def get_terms(tree):    \n","    for leaf in leaves(tree):\n","        terms = [normalise(w) for w,t in leaf]\n","        yield terms\n","\n","def getNounPhrases(text):\n","  wordsToRemove = ['be', 'is', 'are', 'was', 'were', 'been', 'being']\n","  tree = setup(text)\n","\n","  terms = get_terms(tree)\n","\n","  features = []\n","  for term in terms:\n","      _term = ''\n","      for word in term:\n","        _term += ' ' + word\n","\n","      if not any(x in _term.split() for x in wordsToRemove) and '\\\\' not in _term:\n","        features.append(_term.strip())\n","\n","  res = []\n","  res += re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","  res += re.findall(r\"(?<=\\$\\srepresent\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","  res += re.findall(r\"(?<=\\$\\srepresents\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","  res += re.findall(r\"(?<=\\$\\sdenote\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","  res += re.findall(r\"(?<=\\$\\sdenotes\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","  res += re.findall(r\"(?<=\\$\\sis\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|\\\\|and)\", text)\n","  res += re.findall(r\"(?<=\\sof\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\)\", text)\n","  result1 = re.findall(r\"(?<=\\sthe\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\)\", text)\n","  for i in range(len(result1)):\n","    result1[i] = 'the ' + result1[i]\n","  res += result1\n","\n","  result2 = re.findall(r\"(?<=\\sThe\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\)\", text)\n","  for i in range(len(result2)):\n","    result2[i] = 'The ' + result2[i]\n","  res += result2\n","\n","  result3 = re.findall(r\"(?<=\\sa\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\)\", text)\n","  for i in range(len(result3)):\n","    result3[i] = 'a ' + result3[i]\n","  res += result3\n","\n","\n","  for phrase in res:\n","    features.append(phrase.strip())\n","      \n","  return features"],"metadata":{"id":"od1I_w4VNa0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_recall(noun_list, expected):\n","  total = 0\n","  count = 0\n","  primary_descriptor_list = []\n","  for key in expected.keys():\n","    primary_descriptor_list.append(expected[key][\"text\"])\n","  total = len(primary_descriptor_list)  \n","  for descriptor in primary_descriptor_list:\n","    if descriptor in noun_list:  \n","      count += 1\n","      noun_list.remove(descriptor)\n","  return count / total\n","  \n","\n","  "],"metadata":{"id":"Z1SxEEev56bu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Let $x$ be the number of bins. Let $\\alpha + \\beta$ be the sum of two parameters, which are then used for analysis.\"\n","# text = \"Let $x$ be the number of bins. Let $\\alpha + \\beta$ be the sum of two parameters.\"\n","# res = re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.)\", text)\n","# text = \"Let $x$ be the number of bins :\"\n","res = re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iUolarFB9NG","executionInfo":{"status":"ok","timestamp":1642980563991,"user_tz":480,"elapsed":9,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"e57c4c03-f937-49a6-f85f-c500fdab5a3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['the number of bins', 'the sum of two parameters']\n"]}]},{"cell_type":"code","source":["text = \"Let $x$ represent the number of bins, where $\\alpha + \\beta$ represents the sum of two parameters, which are then used for analysis.\"\n","# text = \"Let $x$ be the number of bins. Let $\\alpha + \\beta$ be the sum of two parameters.\"\n","# res = re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.)\", text)\n","# text = \"Let $x$ be the number of bins :\"\n","res = re.findall(r\"(?<=\\$\\srepresent\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7JB_FVNSi4x","executionInfo":{"status":"ok","timestamp":1642980936724,"user_tz":480,"elapsed":205,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"9a74364c-6c2b-4e60-b3f7-b11303819407"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['the number of bins']\n"]}]},{"cell_type":"code","source":["text = data.loc[\"text\"][1]\n","print(text)\n","expected = extract_primary_description(data.loc[\"entity\"][1])\n","print(\"Noun phrases\", getNounPhrases(text))\n","print(\"Expected\", expected)\n","calculate_recall(getNounPhrases(text), expected)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGnXEBAo7J6d","executionInfo":{"status":"ok","timestamp":1642982442900,"user_tz":480,"elapsed":159,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"3c1f9547-53ec-4ac5-e5ac-efce14795f74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We fit the parameters according to the procedure described in \\cite{EkanadhamKarklin15} .\n","Estimating the entire trajectory $\\thetastraj$ for each student simultaneously with item parameters is very expensive and difficult to do in real - time .\n","To simplify the approach , we learn parameters in two stages : \\begin{enumerate}  \\item We learn the            according to a standard 1PO IRT model (see Section~\\ref{sec:irtlearning} ) on the training student population and freeze these during validation .\n","           .           \n","For the second step , we combine the approximation :            & P ( \\{(s', i, r, t') \\in D: s'=s, t'\\leq t\\}|\\theta_{s,t} ) \\approx \\nonumber \\\\ &\\prod_{(s',i,r,t') \\in D: s'=s, t'\\leq t} P ( ( s' , i , r , t ' ) | \\theta_{s,t} ) \\end{align} with \\eqref{eq:wiener} , integrating out previous proficiencies of the student to get a tractable approximation of the log posterior over the student 's current proficiency given previous responses :\n","\\begin{align}  \\log P(\\theta_{s,t} | D ) &\\ approx            } [ r            _{t'} (            -\\beta_i ) ) + \\ nonumber \\\\ & ( 1 - r )            _{t'} (            -\\beta_i ))) ]\\, , \\end{align} where $\\tilde{\\alpha}_{t'}=\\left(1 + \\gamma^2(t-t')\\right)^{-1/2}$ .\n","The            's are essentially discounting the relative effect of older responses when estimating the current proficiency .\n","See \\cite{EkanadhamKarklin15} for details .\n","Noun phrases ['fit', 'parameters', 'according', 'procedure', 'described', 'EkanadhamKarklin15', 'Estimating', 'student', 'item parameters', 'do', 'time', 'simplify', 'approach', 'learn', 'parameters', 'stages', 'enumerate', 'learn', 'according', 'IRT model', 'see', 'sec', 'irtlearning', 'training student population', 'freeze', 'validation', 'second step', 'combine', 'approximation', 'P', 's', 'i', 'r', 't', 'D', \"s'=s\", 's', 't', 's', 'i', 'r', 't', 'D', \"s'=s\", 'P', 's', 'i', 'r', 't', '|', 's', 't', 'align', 'eq', 'wiener', 'integrating', 'previous proficiencies', 'student', 'get', 'tractable approximation', 'log posterior', 'student', 'current proficiency', 'given', 'previous responses', 'align', 's', 't', '| D', '[ r _', 't', '+', 'r', 't', 'align', '_', 't', '-1/2', 'essentially discounting', 'relative effect', 'responses', 'estimating', 'current proficiency', 'See', 'EkanadhamKarklin15', 'details']\n","Expected {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 101, 'end': 122, 'text': 'the entire trajectory'}}\n"]},{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["# print(getNounPhrases(\"Mostly, there just is no default way of determining the paragraph boundary and people tend to work with sentences. Still, the unit of a paragraph might be of a higher value than that of a sentence. Examples might be: coreference resolutions that overlap multiple sentences. Questions that find their answer throughout a whole paragraph. A reader that understands a paragraph better than an isolated sentence. It’s clear that the signal from a writer is best expressed in a paragraph.\"))"],"metadata":{"id":"_5iGVjccQaLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = data\n","# for column in df:\n","#     wantedOutput = extract_primary_description(df[column][\"entity\"])\n","#     paragraph = df[column][\"text\"]\n","#     print(\"PARAGRAPH:\\n\", paragraph)\n","#     print(\"NOUN PHRASES:\\n\", getNounPhrases(paragraph))\n","#     print(\"EXPECTED:\\n\", wantedOutput)\n","#     print(\"*\" * 280)\n","#     print(\"\")"],"metadata":{"id":"bhunPJLjSL0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def findNounsWithLocs(text):\n","  '''This function takes in a block of text, finds the nouns in it and then returns an array of 1s and 0s representing where those nouns are'''\n","  originalText = text\n","  #modify text here however we please in getNounPhrases\n","  nounList = getNounPhrases(text)\n","  start = 0\n","  predicted_array = np.zeros(len(originalText))\n","\n","  for word in nounList:\n","    nounStartLoc = originalText.find(word, start)\n","    nounEndLoc = nounStartLoc + len(word)\n","\n","    if abs(originalText.find('$', start) - nounStartLoc) < 25:\n","      predicted_array[nounStartLoc : nounEndLoc] = 1\n","\n","    start = nounEndLoc\n","\n","  return predicted_array"],"metadata":{"id":"WreKyj_aG2PL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(findNounsWithLocs(\"The bus is yellow.\"))"],"metadata":{"id":"ylM5UWpPIaHw","executionInfo":{"status":"ok","timestamp":1642980568624,"user_tz":480,"elapsed":134,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e56bccd2-7d8d-494f-c93a-678754f46c9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}]},{"cell_type":"code","source":["# for column in data:\n","#   paragraph = df[column][\"text\"]\n","#   print(\"PARAGRAPH:\\n\", paragraph)\n","#   print(\"NOUN PHRASES:\\n\", getNounPhrases(paragraph))\n","#   print(\"NOUN LOCATIONS:\\n\", findNounsWithLocs(paragraph))\n","#   print('*' * 280 + '\\n')"],"metadata":{"id":"luShAjMVOD-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_arrays(expected, predicted, len_text):\n","  expected_array = np.zeros(len_text)  \n","  predicted_array = np.zeros(len_text)\n","  for key in expected.keys():\n","    start_index = expected[key][\"start\"]\n","    end_index = expected[key][\"end\"]\n","    expected_array[start_index : end_index + 1] = 1\n","\n","  \n","  # for key in predicted.keys():\n","  #   start_index = predicted[key][\"start\"]\n","  #   end_index = predicted[key][\"end\"]\n","  #   predicted_array[start_index : end_index + 1] = 1\n","\n","  return (expected_array, predicted_array)"],"metadata":{"id":"hXlFMmaEC4ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare(expected, predicted): #Computes the F1 score between expected and predicted\n","  expected_array = np.array(len())"],"metadata":{"id":"UIlWJDCB-_AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def framework(df):\n","  results = []\n","  recalls = []\n","  for column in df:\n","    paragraph = df[column][\"text\"]\n","    expected = df[column][\"entity\"]\n","    primary_description = extract_primary_description(expected)\n","    predicted = paragraph\n","    if len(primary_description) != 0:\n","      recalls.append(calculate_recall(getNounPhrases(predicted), primary_description))\n","    # expected_array, predicted_array = create_arrays(primary_description, predicted, len(paragraph))\n","    expected_array, predicted_array = create_arrays(primary_description, predicted, len(paragraph))\n","    predicted_array = findNounsWithLocs(paragraph)\n","    result = f1_score(expected_array, predicted_array, average= \"binary\", zero_division = 1)\n","    results.append(result)\n","  print(\"Average F1 score is:\", sum(results) / len(results))\n","  print(\"Average recall: \", sum(recalls) / len(recalls))\n"],"metadata":{"id":"Fuy6yJ6569ZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def framework_recall(df):\n","  results = []\n","  recalls = []\n","  for column in df:\n","    paragraph = df[column][\"text\"]\n","    expected = df[column][\"entity\"]\n","    primary_description = extract_primary_description(expected)\n","    if len(primary_description) != 0:\n","      recalls.append(calculate_recall(getNounPhrases(paragraph), primary_description))\n","  print(\"Average recall: \", sum(recalls) / len(recalls))\n"],"metadata":{"id":"GdqC_gx7Vn0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\"The current F1 score of 0.355 arises when we leave the predicted_array all full of zeroes and compare with the expected.\"\n","# 0.261 is our current best implementing NLP techniques w/ <25\n","# framework(data)"],"metadata":{"id":"7JJo6VII_N-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataFiles = []\n","pwd = '/content/drive/My Drive/primary_description/Practice'\n","for filename in os.listdir(pwd):\n","    if filename.endswith(\"json\"): \n","        dataFiles.append(pwd + '/' + filename)\n","for filename in dataFiles:\n","  print(filename.replace('/content/drive/My Drive/primary_description/Practice/', \"\"))\n","  framework_recall(pd.read_json(filename))\n","  print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXXqZCuvWKKr","executionInfo":{"status":"ok","timestamp":1642984255069,"user_tz":480,"elapsed":31294,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"36cf3f6e-aa13-4ea0-c39e-3b931ed4c382"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["physics.atom_ph-ann10.json\n","Average recall:  0.5660383488144489\n","\n","q_bio.qm-ann11.json\n","Average recall:  0.5545261387190994\n","\n","cs.ai-ann0.json\n","Average recall:  0.5938299724651765\n","\n","cs.ai-ann3.json\n","Average recall:  0.6252281557784404\n","\n","econ.th-ann6.json\n","Average recall:  0.4732876712328767\n","\n","physics.atom_ph-ann8.json\n","Average recall:  0.5832093253968254\n","\n","econ.th-ann5.json\n","Average recall:  0.5371130952380953\n","\n","physics.atom_ph-ann9.json\n","Average recall:  0.5757469862422696\n","\n","math.co-ann7.json\n","Average recall:  0.5293902268014692\n","\n","cs.ai-ann2.json\n","Average recall:  0.5056761221234907\n","\n","econ.th-ann4.json\n","Average recall:  0.5415768107231523\n","\n"]}]},{"cell_type":"code","source":["dataFiles = []\n","pwd = '/content/drive/My Drive/primary_description/Practice'\n","for filename in os.listdir(pwd):\n","    if filename.endswith(\"json\"): \n","        dataFiles.append(pwd + '/' + filename)\n","for filename in dataFiles:\n","  print(filename.replace('/content/drive/My Drive/primary_description/Practice/', \"\"))\n","  framework(pd.read_json(filename))\n","  print()"],"metadata":{"id":"PktY8mjkTvfD","executionInfo":{"status":"ok","timestamp":1642980685650,"user_tz":480,"elapsed":75598,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a74b2a0e-e6d7-42a1-ed75-8bcb2a3787b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["physics.atom_ph-ann10.json\n","Average F1 score is: 0.24100718166495289\n","Average recall:  0.5606080209455965\n","\n","q_bio.qm-ann11.json\n","Average F1 score is: 0.24836546363431322\n","Average recall:  0.5369418510261161\n","\n","cs.ai-ann0.json\n","Average F1 score is: 0.2619597439213551\n","Average recall:  0.33751223974438266\n","\n","cs.ai-ann3.json\n","Average F1 score is: 0.292149992910222\n","Average recall:  0.58191018342821\n","\n","econ.th-ann6.json\n","Average F1 score is: 0.18309014547466393\n","Average recall:  0.4664383561643835\n","\n","physics.atom_ph-ann8.json\n","Average F1 score is: 0.10158271561332784\n","Average recall:  0.5832093253968254\n","\n","econ.th-ann5.json\n","Average F1 score is: 0.10843422731427918\n","Average recall:  0.3567708333333333\n","\n","physics.atom_ph-ann9.json\n","Average F1 score is: 0.2622680957747418\n","Average recall:  0.5737728422869935\n","\n","math.co-ann7.json\n","Average F1 score is: 0.27245963591418115\n","Average recall:  0.2672441053358214\n","\n","cs.ai-ann2.json\n","Average F1 score is: 0.2836446512781167\n","Average recall:  0.4970934675704413\n","\n","econ.th-ann4.json\n","Average F1 score is: 0.2304297395446351\n","Average recall:  0.2344285930871296\n","\n"]}]}]}