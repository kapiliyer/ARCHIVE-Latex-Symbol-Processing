{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Xrg4b6WoEA4U"},"source":["# Combined Framework for Symlink"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3402,"status":"ok","timestamp":1643581019415,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"_B2KowBG8Zx_","outputId":"7615fd34-94f4-4164-df63-ca7fcf12034d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1Ip0tGIOuwJNvhITLBA4pbOcMNg9Lv-mq/primary_description\n"]}],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# workshop folder, e.g. 'acmlab/workshops/project'\n","FOLDERNAME = 'primary_description'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uzBHIp-LFG5t"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4084,"status":"ok","timestamp":1643581026537,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"l4pMi_3HCdxb","outputId":"e371f9c1-4cb2-4eca-8ff4-0a30c3d1d1ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}],"source":["import numpy as np\n","import os\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","from textblob import TextBlob\n","import nltk\n","import re\n","import difflib\n","import json\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","\n","from IPython.display import display\n","\n","# import stanza\n","# stanza.install_corenlp()\n","# import spacy\n","# nltk.download('brown')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bRCzP3ooEO-r"},"source":["### Read in Data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1019,"status":"ok","timestamp":1643581028553,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"VVkvU5UR6h6N"},"outputs":[],"source":["data = pd.read_json(\"Practice/cs.ai-ann0.json\")\n","# text_data = data.loc[\"text\"]\n","# text_data.to_dict()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uauqOnMhERHj"},"source":["Functions for extracting symbols and primary descriptions"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1643581028554,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"fcac7jyqAIe3"},"outputs":[],"source":["def extract_primary_description(entity): #Entity is a dicitionary.\n","  output = {}\n","  for key in entity.keys():\n","    if entity[key]['label'] == \"PRIMARY\":\n","      output[key] = entity[key]\n","  return output\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1643581029878,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"AdWEsQMt7G7Z"},"outputs":[],"source":["def extract_symbol(entity): #Entity is a dicitionary.\n","  output = {}\n","  for key in entity.keys():\n","    if entity[key]['label'] == \"SYMBOL\":\n","      output[key] = entity[key]\n","  return output\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1643581030950,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"od1I_w4VNa0q"},"outputs":[],"source":["lemmatizer = nltk.WordNetLemmatizer()\n","def setup(text):\n","\n","  #word tokenizeing and part-of-speech tagger\n","  document = text\n","  tokens = [nltk.word_tokenize(sent) for sent in [document]]\n","  postag = [nltk.pos_tag(sent) for sent in tokens][0]\n","\n","  # Rule for NP chunk and VB Chunk\n","  grammar = r\"\"\"\n","      NBAR:\n","          {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n","          {<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?} # Verbs and Verb Phrases\n","          \n","      NP:\n","          {<NBAR>}\n","          {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n","          \n","  \"\"\"\n","  #Chunking\n","  cp = nltk.RegexpParser(grammar)\n","\n","  # the result is a tree\n","  tree = cp.parse(postag)\n","  return tree\n","\n","def leaves(tree):\n","    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n","    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):\n","        yield subtree.leaves()\n","        \n","def get_word_postag(word):\n","    if pos_tag([word])[0][1].startswith('J'):\n","        return wordnet.ADJ\n","    if pos_tag([word])[0][1].startswith('V'):\n","        return wordnet.VERB\n","    if pos_tag([word])[0][1].startswith('N'):\n","        return wordnet.NOUN\n","    else:\n","        return wordnet.NOUN\n","    \n","def normalise(word):\n","    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n","    # word = word.lower()\n","    # postag = get_word_postag(word)\n","    # word = lemmatizer.lemmatize(word, postag)\n","    return word\n","\n","def get_terms(tree):    \n","    for leaf in leaves(tree):\n","        terms = [normalise(w) for w,t in leaf]\n","        yield terms\n","\n","def getNounPhrases(text):\n","  wordsToRemove = ['be', 'is', 'are', 'was', 'were', 'been', 'being']\n","  tree = setup(text)\n","\n","  terms = get_terms(tree)\n","\n","  features = []\n","  for term in terms:\n","      _term = ''\n","      for word in term:\n","        _term += ' ' + word\n","\n","      if not any(x in _term.split() for x in wordsToRemove) and '\\\\' not in _term:\n","        features.append(_term.strip())\n","\n","  res = []\n","  res += re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|and)\", text)\n","  res += re.findall(r\"(?<=\\$\\srepresent\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|and)\", text)\n","  res += re.findall(r\"(?<=\\$\\srepresents\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|and)\", text)\n","  res += re.findall(r\"(?<=\\$\\sdenote\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|and)\", text)\n","  res += re.findall(r\"(?<=\\$\\sdenotes\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|and)\", text)\n","  res += re.findall(r\"(?<=\\$\\sis\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?|\\\\|and)\", text)\n","  res += re.findall(r\"(?<=\\sof\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\|and)\", text)\n","  result1 = re.findall(r\"(?<=\\sthe\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\|and)\", text)\n","  for i in range(len(result1)):\n","    result1[i] = 'the ' + result1[i]\n","  res += result1\n","\n","  result2 = re.findall(r\"(?<=\\sThe\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\)\", text)\n","  for i in range(len(result2)):\n","    result2[i] = 'The ' + result2[i]\n","  res += result2\n","\n","  result3 = re.findall(r\"(?<=\\sa\\s)(.*?)(?=\\$|\\.|\\,|\\;|\\:|\\!|\\?|\\\\)\", text)\n","  for i in range(len(result3)):\n","    result3[i] = 'a ' + result3[i]\n","  res += result3\n","\n","\n","  for phrase in res:\n","    features.append(phrase.strip())\n","      \n","  return features"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ob8_mOfUEaA9"},"source":["### Kapil's Code for extracting Symbols"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":301,"status":"ok","timestamp":1643581454083,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"TXj4LIy84e2b"},"outputs":[],"source":["def getSymbols(string): # given a string, returns a dictionary of each symbol, its start index, and its end index\n","  temp = {}\n","  # substrings surrounded by delimiter pairs are math text\n","  delimiters = {'\\\\(': '\\\\)', '$': '$', '$$': '$$', '\\\\begin{math}': '\\\\end{math}', '\\\\[': '\\\\]', '\\\\begin{displaymath}': '\\\\end{displaymath}', '\\\\begin{equation}': '\\\\end{equation}', '\\\\begin{align}': '\\\\end{align}', '\\\\begin{eqnarray}': '\\\\end{eqnarray}', '\\\\begin{array}': '\\\\end{array}', '(': ')'}\n","  # substrings to the left and right of a splitter are considered separate symbols\n","  splitters = ['=', '<', '>', '<=', '=>', '\\\\leq', '\\\\geq', '\\\\leftarrow', '\\\\rightarrow', '\\\\longleftarrow', '\\\\longrightarrow', '\\\\Leftarrow', '\\\\Rightarrow', '\\\\Longleftarrow', '\\\\Longrightarrow', '\\\\leftrightarrow', '\\\\Longleftrightarrow', '\\\\mapsto', '\\\\longmapsto', '\\\\neq', '\\equiv']\n","\n","  for start, end in delimiters.items(): # grab each pair of delimiters\n","    new = string # scan the complete string for each pair of delimiters\n","    while True:   \n","      a = new.find(start)\n","      a_shift = a + len(start) # first delimiter\n","      b = new.find(end, a_shift) # second delimiter\n","      if a == -1 or b == -1: # if pair of delimiters are not present, go to the next pair of delimiters\n","        break\n","      else: # if pair of delimiters are present, add the symbols and locations to the dict\n","\n","        if new[a_shift] == '_': # check for subscripts\n","          a_shift -= 3\n","          while a_shift != ' ':\n","            a_shift -= 1\n","          a_shift += 1\n","\n","        \n","\n","        result = new[a_shift:b]\n","        if (start != '(' or (result.isupper() and len(result.split())) <= 1) and result.strip() != '': # the (parentheses) delimiter pair only surround abbreviations, i.e. single full-caps words\n","          temp[result] = (a_shift, b)\n","        new = new[b + len(end):] # search for the same pair of delimiters located AFTER the pair we have just identified\n","  \n","  symbols = dict(temp) # make two copies -- one to iterate over and one to store the split symbols\n","\n","  for symbol, location in temp.items(): # recognize symbols separately in equalities/inequalities/implications\n","    (start, end) = location\n","    for splitter in splitters:\n","      if splitter in symbol:\n","        split = [sym.strip() for sym in symbol.split(splitter)]\n","        symbols[split[0]] = (start, start + len(split[0]))\n","        symbols[split[-1]] = (end - len(split[1]), end)\n","        try: # delete the unsplit symbol\n","          del symbols[symbol] \n","        except: # sometimes the unsplit symbol will have already been deleted from temp, because it contained more than one splitter\n","          pass \n","\n","  entity = {}\n","  num = 1\n","  for symbol in symbols.keys():\n","    eid = 'T' + str(num)\n","    num += 1\n","    entity[eid] = {}\n","    entity[eid]['eid'] = eid\n","    entity[eid]['label'] = 'SYMBOL'\n","    entity[eid]['start'] = symbols[symbol][0]\n","    entity[eid]['end'] = symbols[symbol][1]\n","    entity[eid]['text'] = symbol\n","\n","  return entity"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"icvc0jItEjEO"},"source":["## Calculate recall for descriptors"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":379,"status":"ok","timestamp":1643581457583,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"Z1SxEEev56bu"},"outputs":[],"source":["def calculate_recall_descriptors(noun_list, expected):\n","  total = 0\n","  count = 0\n","  primary_descriptor_list = []\n","  for key in expected.keys():\n","    primary_descriptor_list.append(expected[key][\"text\"])\n","  total = len(primary_descriptor_list)  \n","  for descriptor in primary_descriptor_list:\n","    if descriptor in noun_list:  \n","      count += 1\n","      noun_list.remove(descriptor)\n","  return count / total\n","  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yfQXPYoCEmX5"},"source":["## Test word rules"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1643581461785,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"-iUolarFB9NG","outputId":"2988d7ae-37fa-4344-8d36-c74d2db226f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["['the number of bins', 'the sum of two parameters']\n"]}],"source":["text = \"Let $x$ be the number of bins. Let $\\\\alpha + \\\\beta$ be the sum of two parameters, which are then used for analysis.\"\n","# text = \"Let $x$ be the number of bins. Let $\\alpha + \\beta$ be the sum of two parameters.\"\n","# res = re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.)\", text)\n","# text = \"Let $x$ be the number of bins :\"\n","res = re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","print(res)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1643581462181,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"r7JB_FVNSi4x","outputId":"77e81e41-2043-4fec-f3f6-fa666fcd7261"},"outputs":[{"name":"stdout","output_type":"stream","text":["['the number of bins']\n"]}],"source":["text = \"Let $x$ represent the number of bins, where $\\alpha + \\beta$ represents the sum of two parameters, which are then used for analysis.\"\n","# text = \"Let $x$ be the number of bins. Let $\\alpha + \\beta$ be the sum of two parameters.\"\n","# res = re.findall(r\"(?<=\\$\\sbe\\s)(.*?)(?=\\.)\", text)\n","# text = \"Let $x$ be the number of bins :\"\n","res = re.findall(r\"(?<=\\$\\srepresent\\s)(.*?)(?=\\.|\\,|\\;|\\:|\\!|\\?)\", text)\n","print(res)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1643581463551,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"dGnXEBAo7J6d","outputId":"66cc7ef9-3670-48b3-f697-a11688c7ad9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["We fit the parameters according to the procedure described in \\cite{EkanadhamKarklin15} .\n","Estimating the entire trajectory $\\thetastraj$ for each student simultaneously with item parameters is very expensive and difficult to do in real - time .\n","To simplify the approach , we learn parameters in two stages : \\begin{enumerate}  \\item We learn the            according to a standard 1PO IRT model (see Section~\\ref{sec:irtlearning} ) on the training student population and freeze these during validation .\n","           .           \n","For the second step , we combine the approximation :            & P ( \\{(s', i, r, t') \\in D: s'=s, t'\\leq t\\}|\\theta_{s,t} ) \\approx \\nonumber \\\\ &\\prod_{(s',i,r,t') \\in D: s'=s, t'\\leq t} P ( ( s' , i , r , t ' ) | \\theta_{s,t} ) \\end{align} with \\eqref{eq:wiener} , integrating out previous proficiencies of the student to get a tractable approximation of the log posterior over the student 's current proficiency given previous responses :\n","\\begin{align}  \\log P(\\theta_{s,t} | D ) &\\ approx            } [ r            _{t'} (            -\\beta_i ) ) + \\ nonumber \\\\ & ( 1 - r )            _{t'} (            -\\beta_i ))) ]\\, , \\end{align} where $\\tilde{\\alpha}_{t'}=\\left(1 + \\gamma^2(t-t')\\right)^{-1/2}$ .\n","The            's are essentially discounting the relative effect of older responses when estimating the current proficiency .\n","See \\cite{EkanadhamKarklin15} for details .\n","Noun phrases ['fit', 'parameters', 'according', 'procedure', 'described', 'EkanadhamKarklin15', 'Estimating', 'student', 'item parameters', 'do', 'time', 'simplify', 'approach', 'learn', 'parameters', 'stages', 'enumerate', 'learn', 'according', 'IRT model', 'see', 'sec', 'irtlearning', 'training student population', 'freeze', 'validation', 'second step', 'combine', 'approximation', 'P', 's', 'i', 'r', 't', 'D', \"s'=s\", 's', 't', 's', 'i', 'r', 't', 'D', \"s'=s\", 'P', 's', 'i', 'r', 't', '|', 's', 't', 'align', 'eq', 'wiener', 'integrating', 'previous proficiencies', 'student', 'get', 'tractable approximation', 'log posterior', 'student', 'current proficiency', 'given', 'previous responses', 'align', 's', 't', '| D', '[ r _', 't', '+', 'r', 't', 'align', '_', 't', '-1/2', 'essentially discounting', 'relative effect', 'responses', 'estimating', 'current proficiency', 'See', 'EkanadhamKarklin15', 'details', \"the student to get a tractable approximation of the log posterior over the student 's current proficiency given previous responses\", 'older responses when estimating the current proficiency', 'the parameters according to the procedure described in', 'the entire trajectory', 'the approach', 'the            according to a st', 'the training student population', 'the second step', 'the approximation', \"the student to get a tractable approximation of the log posterior over the student 's current proficiency given previous responses\", 'the relative effect of older responses when estimating the current proficiency', \"The            's are essentially discounting the relative effect of older responses when estimating the current proficiency\", 'a standard 1PO IRT model (see Section~', \"a tractable approximation of the log posterior over the student 's current proficiency given previous responses\"]\n","Expected {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 101, 'end': 122, 'text': 'the entire trajectory'}}\n"]},{"data":{"text/plain":["1.0"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["text = data.loc[\"text\"][1]\n","print(text)\n","expected = extract_primary_description(data.loc[\"entity\"][1])\n","print(\"Noun phrases\", getNounPhrases(text))\n","print(\"Expected\", expected)\n","calculate_recall_descriptors(getNounPhrases(text), expected)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5iGVjccQaLk"},"outputs":[],"source":["# print(getNounPhrases(\"Mostly, there just is no default way of determining the paragraph boundary and people tend to work with sentences. Still, the unit of a paragraph might be of a higher value than that of a sentence. Examples might be: coreference resolutions that overlap multiple sentences. Questions that find their answer throughout a whole paragraph. A reader that understands a paragraph better than an isolated sentence. Itâ€™s clear that the signal from a writer is best expressed in a paragraph.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhunPJLjSL0J"},"outputs":[],"source":["# df = data\n","# for column in df:\n","#     wantedOutput = extract_primary_description(df[column][\"entity\"])\n","#     paragraph = df[column][\"text\"]\n","#     print(\"PARAGRAPH:\\n\", paragraph)\n","#     print(\"NOUN PHRASES:\\n\", getNounPhrases(paragraph))\n","#     print(\"EXPECTED:\\n\", wantedOutput)\n","#     print(\"*\" * 280)\n","#     print(\"\")"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1643581465213,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"WreKyj_aG2PL"},"outputs":[],"source":["def findNounsWithLocs(text):\n","  '''This function takes in a block of text, finds the nouns in it and then returns an array of 1s and 0s representing where those nouns are'''\n","  originalText = text\n","  #modify text here however we please in getNounPhrases\n","  nounList = getNounPhrases(text)\n","  start = 0\n","  predicted_array = np.zeros(len(originalText))\n","\n","  for word in nounList:\n","    nounStartLoc = originalText.find(word, start)\n","    nounEndLoc = nounStartLoc + len(word)\n","\n","    if abs(originalText.find('$', start) - nounStartLoc) < 25:\n","      predicted_array[nounStartLoc : nounEndLoc] = 1\n","\n","    start = nounEndLoc\n","\n","  return predicted_array"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1643581466401,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"ylM5UWpPIaHw","outputId":"4c542097-bc4b-4507-ddd5-423d1ecdac6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}],"source":["print(findNounsWithLocs(\"The bus is yellow.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luShAjMVOD-H"},"outputs":[],"source":["# for column in data:\n","#   paragraph = df[column][\"text\"]\n","#   print(\"PARAGRAPH:\\n\", paragraph)\n","#   print(\"NOUN PHRASES:\\n\", getNounPhrases(paragraph))\n","#   print(\"NOUN LOCATIONS:\\n\", findNounsWithLocs(paragraph))\n","#   print('*' * 280 + '\\n')"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1643581467920,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"hXlFMmaEC4ix"},"outputs":[],"source":["def create_arrays(expected, predicted, len_text):\n","  expected_array = np.zeros(len_text)  \n","  predicted_array = np.zeros(len_text)\n","  for key in expected.keys():\n","    start_index = expected[key][\"start\"]\n","    end_index = expected[key][\"end\"]\n","    expected_array[start_index : end_index + 1] = 1\n","\n","  \n","  # for key in predicted.keys():\n","  #   start_index = predicted[key][\"start\"]\n","  #   end_index = predicted[key][\"end\"]\n","  #   predicted_array[start_index : end_index + 1] = 1\n","\n","  return (expected_array, predicted_array)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1643581469148,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"UIlWJDCB-_AK"},"outputs":[],"source":["def compare(expected, predicted): #Computes the F1 score between expected and predicted\n","  expected_array = np.array(len())"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1643581470492,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"Fuy6yJ6569ZV"},"outputs":[],"source":["def framework(df):\n","  results = []\n","  recalls = []\n","  for column in df:\n","    paragraph = df[column][\"text\"]\n","    expected = df[column][\"entity\"]\n","    primary_description = extract_primary_description(expected)\n","    predicted = paragraph\n","    if len(primary_description) != 0:\n","      recalls.append(calculate_recall_descriptors(getNounPhrases(predicted), primary_description))\n","    # expected_array, predicted_array = create_arrays(primary_description, predicted, len(paragraph))\n","    expected_array, predicted_array = create_arrays(primary_description, predicted, len(paragraph))\n","    predicted_array = findNounsWithLocs(paragraph)\n","    result = f1_score(expected_array, predicted_array, average= \"binary\", zero_division = 1)\n","    results.append(result)\n","  print(\"Average F1 score is:\", sum(results) / len(results))\n","  print(\"Average recall: \", sum(recalls) / len(recalls))\n"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1643581562634,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"mMrOIDiq6fqJ"},"outputs":[],"source":["def framework_recall(filename):\n","\n","  with open(filename) as json_file: \n","    test_json = json.load(json_file)\n","    \n","    test_number = 1\n","    num_right_total = 0\n","    total = 0\n","    wrong = []\n","    for key in test_json.values():\n","      practice = {}\n","      for val in key['entity'].keys():\n","        entity = key['entity'][val]\n","        if entity['label'] == 'SYMBOL':\n","          practice[entity['text'].replace('\\\\', '').replace('$', '').strip()] = (entity['start'], entity['end'])\n","      if practice == {}:\n","        continue\n","      symbols = getSymbols(key['text'])\n","      symbols_list = []\n","      for val in symbols.values():\n","        symbols_list.append(val['text'])\n","      num_right = len([symbol for symbol in symbols_list if symbol.replace('\\\\', '').replace('$', '').strip() in practice])\n","      percent_right = min(1, num_right / len(practice.keys()))\n","      # wrong += [practice_missed for practice_missed in practice if practice_missed not in [symbol.replace('\\\\', '').replace('$', '').strip() for symbol in symbols]]\n","      \n","      num_right_total += num_right\n","      total += len(practice.keys())\n","\n","      # print(test_number)\n","      # print('Percent right: ' + str(percent_right * 100))\n","      test_number += 1\n","\n","    # print('Total symbols tested: ' + str(total))\n","    # print('Total symbols correct: ' + str(num_right_total))\n","    print('Average symbol recall: ' + str(num_right_total / total)) \n","    # print('\\nWhat you did not identify:')\n","    # print(wrong)\n","\n","  results = []\n","  recalls = []\n","  df = pd.read_json(filename)\n","  for column in df:\n","    paragraph = df[column][\"text\"]\n","    expected = df[column][\"entity\"]\n","    primary_description = extract_primary_description(expected)\n","    if len(primary_description) != 0:\n","      recalls.append(calculate_recall_descriptors(getNounPhrases(paragraph), primary_description))\n","  print(\"Average descriptor recall: \", sum(recalls) / len(recalls))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JJo6VII_N-d"},"outputs":[],"source":["#\"The current F1 score of 0.355 arises when we leave the predicted_array all full of zeroes and compare with the expected.\"\n","# 0.261 is our current best implementing NLP techniques w/ <25\n","# framework(data)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31190,"status":"ok","timestamp":1643581518400,"user":{"displayName":"Kapil E Iyer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01728281975491735368"},"user_tz":480},"id":"fXXqZCuvWKKr","outputId":"917e54e6-b732-424d-d5ed-c7332f5ac3d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/physics.atom_ph-ann10.json\n","Average symbol recall: 0.6078291814946619\n","Average descriptor recall:  0.5662660355175455\n","\n","/q_bio.qm-ann11.json\n","Average symbol recall: 0.6092959042797975\n","Average descriptor recall:  0.5548967998040463\n","\n","/cs.ai-ann0.json\n","Average symbol recall: 0.8248275862068966\n","Average descriptor recall:  0.5866658568189179\n","\n","/cs.ai-ann3.json\n","Average symbol recall: 0.5426008968609866\n","Average descriptor recall:  0.6252281557784404\n","\n","/econ.th-ann6.json\n","Average symbol recall: 0.8324324324324325\n","Average descriptor recall:  0.4732876712328767\n","\n","/physics.atom_ph-ann8.json\n","Average symbol recall: 0.49296567248171075\n","Average descriptor recall:  0.5832093253968254\n","\n","/econ.th-ann5.json\n","Average symbol recall: 0.8915254237288136\n","Average descriptor recall:  0.5506547619047619\n","\n","/physics.atom_ph-ann9.json\n","Average symbol recall: 0.562708102108768\n","Average descriptor recall:  0.5757469862422696\n","\n","/math.co-ann7.json\n","Average symbol recall: 0.8506024096385543\n","Average descriptor recall:  0.5307440012321666\n","\n","/cs.ai-ann2.json\n","Average symbol recall: 0.4618055555555556\n","Average descriptor recall:  0.5056761221234907\n","\n","/econ.th-ann4.json\n","Average symbol recall: 0.9008130081300812\n","Average descriptor recall:  0.5466697259380188\n","\n"]}],"source":["dataFiles = []\n","pwd = '/content/drive/My Drive/primary_description/Practice'\n","for filename in os.listdir(pwd):\n","    if filename.endswith(\"json\"): \n","        dataFiles.append(pwd + '/' + filename)\n","for filename in dataFiles:\n","  print(filename.replace('/content/drive/My Drive/primary_description/Practice', \"\"))\n","  framework_recall(filename)\n","  print()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wvy8cN93PNpg"},"source":["This is for F1 score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PktY8mjkTvfD"},"outputs":[],"source":["# dataFiles = []\n","# pwd = '/content/drive/My Drive/primary_description/Practice'\n","# for filename in os.listdir(pwd):\n","#     if filename.endswith(\"json\"): \n","#         dataFiles.append(pwd + '/' + filename)\n","# for filename in dataFiles:\n","#   print(filename.replace('/content/drive/My Drive/primary_description/Practice/', \"\"))\n","#   framework(pd.read_json(filename))\n","#   print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LshDEbo8J7lQ"},"outputs":[],"source":["def get_descriptions_paragraph(paragraph):\n","    sentences = nltk.tokenize.sent_tokenize(paragraph)\n","    answer = {}\n","    cnt = 0\n","    for sent in sentences:\n","      features = getNounPhrases(sent)\n","      for feature in features:  ##TODO: Consider case where a word occurs multiple times in a sentence##\n","        if sent.find(feature) > -1:\n","          start = cnt + sent.find(feature)\n","          end = start + len(feature)\n","          answer[feature] = (start, end)\n","      cnt += len(sent)\n","    return {k: v for k, v in sorted(answer.items(), key=lambda item: item[1])}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OR2v2XsJPgRs"},"outputs":[],"source":["def descriptor_to_output(descriptors):\n","    entity = {}\n","    num = 1\n","    for desc in descriptors.keys():\n","      eid = 'T' + str(num)\n","      num += 1\n","      entity[eid] = {}\n","      entity[eid]['eid'] = eid\n","      entity[eid]['label'] = 'PRIMARY'\n","      entity[eid]['start'] = descriptors[desc][0]\n","      entity[eid]['end'] = descriptors[desc][1]\n","      entity[eid]['text'] = desc\n","\n","      return entity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaW5N1PpR9Ns"},"outputs":[],"source":["def merge(dict1, dict2):\n","    res = list(dict1.values()) + list(dict2.values())\n","    res = sorted(res, key=lambda d: d['start'])\n","    #correct still this point\n","    entityRes = {}\n","    num = 1\n","    for d in res:\n","      eid = 'T' + str(num)\n","      num += 1\n","      entityRes[eid] = d\n","      entityRes[eid]['eid'] = eid\n","    return entityRes\n","\n","# def merge(dict1, dict2):\n","#     res = {}\n","#     dict1_key_list = list(dict1.keys())\n","#     dict2_key_list = list(dict2.keys())\n","#     while len(dict1) > 0 or len(dict2) > 0:\n","#         if len(dict2) == 0:\n","#             res[first_descriptor] = dict1[first_descriptor]\n","#             del dict1[first_descriptor]\n","#             dict1_key_list.pop(0)\n","#         elif len(dict1) == 0:\n","#             res[first_symbol] = dict2[first_descriptor]\n","#             del dict2[first_symbol]\n","#             dict2_key_list.pop(0)\n","#         else:\n","#           first_descriptor = dict1_key_list[0]\n","#           first_symbol = dict2_key_list[0]\n","#           descriptor_start = dict1[first_descriptor]['start']\n","#           symbol_start = dict2[first_symbol]['start']\n","#           if descriptor_start < symbol_start:\n","#               res[first_descriptor] = dict1[first_descriptor]\n","#               del dict1[first_descriptor]\n","#               dict1_key_list.pop(0)\n","#           else:\n","#               res[first_symbol] = dict2[first_descriptor]\n","#               del dict2[first_symbol]\n","#               dict2_key_list.pop(0)\n","#     print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1643578990210,"user":{"displayName":"Siddarth Sharma","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00425362796909408112"},"user_tz":480},"id":"1mP0EZRaSB4S","outputId":"4e6a1980-e763-45a6-d83e-4029c60017dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'T1': {'eid': 'T6', 'label': 'PRIMARY', 'start': 6, 'end': 14, 'text': 'evaluate'}}\n","{'T1': {'eid': 'T1', 'label': 'SYMBOL', 'start': 2, 'end': 6, 'text': '\\\\tau'}, 'T2': {'eid': 'T2', 'label': 'SYMBOL', 'start': 2, 'end': 4, 'text': 'RT'}, 'T3': {'eid': 'T3', 'label': 'SYMBOL', 'start': 4, 'end': 8, 'text': 'n|RT'}, 'T4': {'eid': 'T4', 'label': 'SYMBOL', 'start': 4, 'end': 7, 'text': ' 0 '}, 'T5': {'eid': 'T5', 'label': 'SYMBOL', 'start': 5, 'end': 9, 'text': 'p(n)'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 6, 'end': 14, 'text': 'evaluate'}, 'T7': {'eid': 'T7', 'label': 'SYMBOL', 'start': 7, 'end': 10, 'text': '-RT'}, 'T8': {'eid': 'T8', 'label': 'SYMBOL', 'start': 9, 'end': 13, 'text': 'p(R)'}, 'T9': {'eid': 'T9', 'label': 'SYMBOL', 'start': 10, 'end': 30, 'text': '\\\\sum_R p(R)\\\\,p(n|RT)'}, 'T10': {'eid': 'T10', 'label': 'SYMBOL', 'start': 11, 'end': 18, 'text': 'p(n|RT)'}, 'T11': {'eid': 'T11', 'label': 'SYMBOL', 'start': 16, 'end': 23, 'text': 'g^{(2)}'}, 'T12': {'eid': 'T12', 'label': 'SYMBOL', 'start': 19, 'end': 37, 'text': '(RT)^n\\\\exp(-RT)/n!'}, 'T13': {'eid': 'T13', 'label': 'SYMBOL', 'start': 34, 'end': 38, 'text': '\\\\chi'}, 'T14': {'eid': 'T14', 'label': 'SYMBOL', 'start': 39, 'end': 72, 'text': '1+{\\\\rm var}(R)/\\\\langle R\\\\rangle^2'}, 'T15': {'eid': 'T15', 'label': 'SYMBOL', 'start': 47, 'end': 50, 'text': ' R '}, 'T16': {'eid': 'T16', 'label': 'SYMBOL', 'start': 59, 'end': 62, 'text': ' n '}, 'T17': {'eid': 'T17', 'label': 'SYMBOL', 'start': 65, 'end': 68, 'text': ' a '}, 'T18': {'eid': 'T18', 'label': 'SYMBOL', 'start': 70, 'end': 89, 'text': 'g^{(2)}(\\\\tau)\\\\sim 5'}, 'T19': {'eid': 'T19', 'label': 'SYMBOL', 'start': 82, 'end': 83, 'text': '2'}, 'T20': {'eid': 'T20', 'label': 'SYMBOL', 'start': 83, 'end': 86, 'text': ' b '}, 'T21': {'eid': 'T21', 'label': 'SYMBOL', 'start': 93, 'end': 113, 'text': '[g^{(2)}(\\\\tau)/\\\\chi]'}, 'T22': {'eid': 'T22', 'label': 'SYMBOL', 'start': 94, 'end': 97, 'text': ' 2 '}, 'T23': {'eid': 'T23', 'label': 'SYMBOL', 'start': 117, 'end': 130, 'text': 'g^{(2)}(\\\\tau)'}, 'T24': {'eid': 'T24', 'label': 'SYMBOL', 'start': 161, 'end': 162, 'text': 'R'}, 'T25': {'eid': 'T25', 'label': 'SYMBOL', 'start': 276, 'end': 277, 'text': 'n'}, 'T26': {'eid': 'T26', 'label': 'SYMBOL', 'start': 360, 'end': 361, 'text': 'T'}, 'T27': {'eid': 'T27', 'label': 'SYMBOL', 'start': 362, 'end': 395, 'text': '\\\\langle n\\\\rangle/\\\\langle R\\\\rangle'}}\n"]}],"source":["df = pd.read_json(\"Practice/physics.atom_ph-ann10.json\")\n","paragraph = df.loc[\"text\"][10]\n","descriptors = get_descriptions_paragraph(paragraph)\n","dict1 = descriptor_to_output(descriptors)\n","dict2 = getSymbols(paragraph)\n","# print(dict1)\n","# print(dict2)\n","merged = merge(dict1, dict2)\n","print(dict1)\n","# print('*'*280)\n","# print(dict2)\n","# print('*'*280)\n","print(merged)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2NBpF-o8bOY"},"outputs":[],"source":["# We have dicts of symbols and descriptors - let's connect them via relations\n","\n","# direct, co-referred to, count\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjAWQT8eVAC2"},"outputs":[],"source":["# df = pd.read_json(\"Practice/econ.th-ann6.json\")\n","# paragraph = df.loc[\"text\"][6]\n","# descriptors = get_descriptions_paragraph(paragraph)\n","# dict1 = descriptor_to_output(descriptors)\n","# dict2 = getSymbols(paragraph)\n","# merged = merge(dict1, dict2)\n","# print(dict1)\n","# print(merged)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXAPrzcWc_58"},"outputs":[],"source":["import backbone.py"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Combined Framework.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
