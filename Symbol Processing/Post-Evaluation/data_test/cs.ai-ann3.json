{
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_48": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_48",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_48",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_48",
    "text": "The synthetic motherset was generated by producing 10,000 candidate normals and 10,000 candidate anomalies from two different multivariate distributions with the intention of being able to manipulate all problem dimensions with ease .\nThe candidate normals are drawn from a multivariate gaussian with a covariance matrix of $I$ ; that is , each feature is drawn from the standard normal distribution independently of the others .\nThe anomalies are drawn uniformly from the hyper-cube defined by the range $(-4,4)$ in each dimension .\nBoth distributions have ten dimensions ; that is , each point exists in $R^{10}$ ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_49": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_49",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_49",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_49",
    "text": "Our heuristic procedure begins by training a Random Forest as in \\cite{Breiman} to solve the multi-class classification problem .\nThen we calculate the amount of confusion between each pair of classes .\nFor each data point $x_i$ , the Random Forest computes an estimate of $P(\\hat{y_i}=k|x_i)$ , the predicted probability that $x_i$ belongs to class $k$ .\nWe construct a confusion matrix $C$ in which cell $C_{j,k}$ contains the sum of $P(\\hat{y_i}=k|x_i)$ for all $x_i$ whose true class $y_i = j$ .\nWe then define a graph in which each node is a class and each edge ( between two classes $j$ and $k$ ) has a weight equal to $C[j,k]+C[k,j]$ .\nThis is the ( un-normalized ) probability that a data point in class $j$ will be confused with a data point in class $k$ or vice versa .\nWe then compute the maximum weight spanning tree of this ( complete ) graph to identify a graph of `` most - confusable ' ' relationships between pairs of classes .\nWe then two - color this tree so that no adjacent nodes have the same color .\nThe two colors define the two classes of points ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_39": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_39",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_39",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_39",
    "text": "These models yield $\\hat{R^2}$ measures of 0.7299 ( for logit ( AUC ) ) and 0.8009 ( for log ( LIFT ) ) , a strong indication that the variables we are considering are adequate to explain micro-experiment outcomes .\nThe mixed effects models also provide coefficients measuring the effect of each problem dimension on each algorithm .\nSimilar to \\cref{fig:ar,fig:pd,fig:cl,fig:ir} , \\cref{fig:arco,fig:pdco,fig:clco,fig:irco} show these coefficients across each problem dimension ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_11": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_11",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_11",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_11",
    "text": "The well - known Local Outlier Factor algorithm ( Breunig , et al. \\cite{Breunig:2000} ) computes the outlier score of a point $x_i$ by computing its average distance to its $k$ nearest neighbors .\nIt normalizes this distance by computing the average distance of each of those neighbors to {\\ it their } $k$ nearest neighbors .\nSo , roughly speaking , a point is believed to be more anomalous if it is significantly farther from its neighbors than they are from each other ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_10": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_10",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_10",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_10",
    "text": "As proposed by Tax and Duin \\cite{Tax:2004} and similar in concept to One - Class SVM , Support Vector Data Description finds the smallest hypersphere in kernel space that encloses $1-\\delta$ of the data .\nAs above , the outlier scores produced by this algorithm are determined by the residual after each point is projected onto the decision surface ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_38": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_38",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_38",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_38",
    "text": "So far the $\\hat{R^2}$ of our best models are 0.5019 and 0.6382 which are respectable measures but leave a lot of variance unexplained .\nWe constructed several mixed effect models to see if we could better explain our results with the same data .\nOur best models kept our problem dimensions as fixed effects and treated choice of algorithm and motherset as random effect groups .\nEach member of each random effect group models its interaction with the fixed effects .\nAdditionally , choice of motherset also models its own interaction with choice of algorithm .\n$$metric \\sim (rf + pd + cl + ir | algo) + (rf + pd + cl + ir + algo | mset)$$"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_12": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_12",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_12",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_12",
    "text": "Angle - Based Outlier Detection as proposed by Kriegel , et al in \\cite{abod} in its full form is an algorithm of cubic complexity as follows .\nFor each point $x_i$ , consider all pairs of other points $(x_j,x_k)\\in X,i\\ne j\\ne k$ and compute the angle between them relative to to point $x_i$ .\nThe sample variance of these angles determines the outlier score , with \\emph{lower} variances indicating anomalous points .\nBecause of the run - time complexity , two simple approximations were suggested by the authors .\nThe first is to sub sample the data and use this as the reference set for computing angles .\nThe other is to only consider the angles among the $k$ nearest neighbors to $x_i$ .\nIn initial experiments we found the latter to outperform the former and so that is the strategy employed in this study ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_8": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_8",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_8",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_8",
    "text": "Another approach to density estimation is to fit a Gaussian mixture model ( GMM ) using the EM algorithm .\nA single GMM is not very robust , and like $k$ - means clustering it requires that we select a value of $k$ Gaussian mixture components .\nTo improve robustness , we computed an ensemble of GMMs for many values of $k$ , discarded models that did not fit the data well , and then combined the predicted densities of the remaining models .\nAs above , the outlier scores produced are based on the negative log - likelihood of each point according to the final models ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_9": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_9",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_9",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_9",
    "text": "The One - Class SVM algorithm ( Scholkopf et al. \\cite{Scholkopf:99} ) uses a Support - Vector Machine to search for a kernel - space decision boundary that separates fraction $1-\\delta$ of the data from the kernel - space origin .\nThe outlier scores produced by this algorithm are determined by the residual after each point is projected onto the decision surface .\nPoints outside the decision boundary will have positive residuals , where interior points will have negative residuals ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_13": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_13",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_13",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_13",
    "text": "Precision - at - $k$ or Recall - at - $k$ metrics are also common but run the risk of being application specific ( the appropriate selection of $k$ is determined by the context of the application ) and we feel not appropriate for a broad meta-analysis such as this , but we note that such metrics might be the more useful in a real - world setting ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_17": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_17",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_17",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_17",
    "text": "\\cref{tbl:hypo} summarizes the global benchmark failure rates for AUC and AP .\nThe `` Either ' ' column indicates benchmarks for which all algorithms failed under at least one of the two metrics .\n\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Significance Level\\label{tbl:hypo} }\n{ \\begin{tabular} { | r | | c | c | c | }\n\\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline\n$\\alpha=0.05$ & 0.2418 & 0.2815 & 0.3337 \\\\\n$\\alpha=0.01$ & 0.3282 & 0.4162 & 0.4609 \\\\\n$\\alpha=0.001$ & 0.4108 & 0.5087 & 0.5430 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_16": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_16",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_16",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_16",
    "text": "We performed such hypothesis tests for both AUC and AP and for $\\alpha\\in(0.05,0.01,0.001)$ for each micro-experiment result .\nFailure in this context refers to failing to reject the null hypothesis .\nTo summarize our tests , we first define the notion of \\textbf{benchmark failure} as a benchmark instance for which \\emph{all} algorithms failed .\nWhile benchmark failure is dependent on the algorithms used in this study , we still believe it is a good indication that we should not use the benchmark as evidence for later conclusions ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_14": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_14",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_14",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_14",
    "text": "Specifically , one can treat the AUC and AP of a random ranking of points as random variables each with parameters $n_{\\text{anom}}$ ( the number of anomalies in the benchmark ) and $n_{\\text{norm}}$ ( the number of normals ) and then compute the quantiles of interest for each of these distributions .\nConducting a test with significance $\\alpha$ is a matter of computing the $(1-\\alpha)$ - quantile of the appropriate random variable .\nRefer to \\cref{appendix:random} for an overview of how AUC and AP can be treated as random variables ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_28": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_28",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_28",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_28",
    "text": "To demonstrate the impact of our benchmark construction methodology on micro-experiment results we first want to examine results in a way that is agnostic to choice of algorithm .\nBecause we are only examining results from benchmarks where at least one algorithm produced statistically significant output , we know that if we only consider the best result from each benchmark we are always choosing a result that is better than random with high confidence .\nAs each benchmark construction factor has a well defined control group , we compute the mean difference in performance - of - best - algorithm between each level and the control group , and then place a $0.999$ confidence interval around this difference .\nThe results are displayed in \\cref{fig:mset,fig:ar,fig:pd,fig:cl,fig:ir} .\nObserve that the metrics logit ( AUC ) and log ( LIFT ) are not meant to be compared to each other , but are shown side by side for a more compact presentation ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_29": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_29",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_29",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_29",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric\\label{tbl:meanalgo} }\n{ \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean logit            } & \\textbf{Mean            } \\\\ \\hline \\hline abod & 0.9517 & 0.9009 \\\\ egmm & 0.9678 & 0.9081\\\\ iforest & \\textbf{1.0893} &            \\\\ loda & 0.8604 & 0.9156 \\\\ lof & 0.9632 & 0.9329\\\\ ocsvm & 0.5650 & 0.8608 \\\\ rkde & 0.9554 & 0.9132\\\\ svdd & 0.1538 & 0.2806 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_15": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_15",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_15",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_15",
    "text": "A more intuitive view of this process : if algorithm $a$ achieves an AUC 0 f 0.75 on benchmark $b$ and we reject the null hypothesis with $\\alpha=0.001$ , that would mean that with probability at least 0.999 a random ranking would achieve an AUC worse than 0.75 \\emph{on benchmark            } ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_18": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_18",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_18",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_18",
    "text": "The appropriate significance level for this study is debatable .\nSmaller $\\alpha$ trades away potential evidence ( by eliminating benchmarks ) for greater confidence that the results from the benchmarks under consideration are relevant .\nWe choose to apply the more stringent threshold of $\\alpha=0.001$ ; even though the failure rate at this level is rather high , it still leaves many benchmarks across all factors of interest ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_30": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_30",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_30",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_30",
    "text": "We have already described measures of \\textbf{clusteredness} and \\textbf{feature irrelevance} in \\cref{sec:method} that are continuous ; as with clusteredness we will apply a log transform to the feature irrelevance ratio .\n\\textbf{relative frequency} and \\textbf{point difficulty} are both in the range $[0,1]$ and so as with AUC we will apply the logit transform to them .\nThis gives us real valued representations of each problem dimension suitable for a linear model ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_24": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_24",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_24",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_24",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Feature Irrelevance Level (            )\\label{tbl:irfail} }\n{ \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline ir - 0 & 0.3065 & 0.4008 & 0.4327 \\\\ ir - 1 & 0.3785 & 0.4760 & 0.5098 \\\\ ir - 2 & \\textbf{0.4356} & \\textbf{0.5362} & \\textbf{0.5743} \\\\ ir - 3 & \\textbf{0.5224} & \\textbf{0.6215} & \\textbf{0.6549} \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_2": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_2",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_2",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_2",
    "text": "{ \\it Relative frequency} is the fraction of the incoming data points that are anomalies of interest. This value is the problem dimension that is most reliably reported in the literature already and has also been called ``plurality'' and ``contamination rate''. Little is done to examine the impact is has on results, however. The behavior of anomaly detection algorithms often changes with the relative frequency. If anomalies are very rare, then methods that pretend that all training points are ``normal'' and fit a model to them may do well. If anomalies are more common, then methods that attempt to fit a model of the anomalies may do well. In most experiments in the literature, the anomalies have a relative frequency between 0.01 and 0.1, but some go as high as 0.3 \\cite{kim:08,Liu:08} .\nMany security applications are estimated to have relative frequencies in the range of $10^{-5}$ or $10^{-6}$ ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_3": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_3",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_3",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_3",
    "text": "We propose to define point difficulty ( pd ) based on an oracle that knows the true generating processes underlying the `` normal ' ' and `` anomalous ' ' points .\nUsing this knowledge , the oracle can estimate the probability $P(y=\\mbox{normal}|x)$ that a data point $x$ was generated by the `` normal ' ' distribution or $P(y=\\mbox{anomaly}|x)$ that a data point $x$ was generated by the `` anomalous ' ' distribution .\nWe consider the point difficulty of any point to be the estimated probability that it belongs to the other class .\nThe point difficulty level of an entire benchmark is summarized as the mean of the point difficulty of all the points in the benchmark ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_25": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_25",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_25",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_25",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Algorithm Failure Rate by Metric (            )\\label{tbl:algofail} }\n{ \\begin{tabular} { | r | | c | c | c | }\n\\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline abod & 0.5898 & 0.6784 & 0.7000 \\\\ iforest & \\textbf{0.5520} & \\textbf{0.6514} &            \\\\ loda & 0.6187 & 0.6955 & 0.7194 \\\\ lof & 0.6016 & 0.7071 & 0.7331 \\\\ rkde & 0.6122 & 0.7030 & 0.7194 \\\\ ocsvm & 0.7218 & 0.7342 & 0.7960 \\\\ svdd & 0.8482 & 0.8868 & 0.9080 \\\\ egmm & 0.6188 & 0.7146 & 0.7303 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_31": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_31",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_31",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_31",
    "text": "The simplest fixed effect linear model to build would be to predict a metric given our four problem dimensions ( abbreviated $rf,pd,cl,ir$ ) , choice of motherset ( $mset$ ) and choice of algorithm ( $algo$ ) .\n$$metric \\sim rf + pd + cl + ir + mset + algo$$"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_19": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_19",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_19",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_19",
    "text": "\\begin{table}  \\resizebox{\\textwidth} { ! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Motherset (            )\\label{tbl:msetfail} }\n{ \\begin{tabular} { | r | | c | c | c | }\n\\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline synthetic & 0.2417 & 0.2553 & 0.2765 \\\\ abalone & 0.3838 & 0.4533 & 0.4841 \\\\ comm. and .crime & \\textbf{0.4924} & 0.5053 &            \\\\ concrete & \\textbf{0.5121} & \\textbf{0.5908} &            \\\\ fault & \\textbf{0.4178} & \\textbf{0.6105} &            \\\\ gas & 0.2767 & \\textbf{0.5178} & 0.5189 \\\\ imgseg & 0.2792 & 0.2569 & 0.3667 \\\\ landsat & \\textbf{0.5286} & \\textbf{0.5617} &            \\\\ letter .rec & \\textbf{0.6276} & \\textbf{0.7273} &            \\\\ magic .gamma & 0.3300 & 0.3322 & 0.3917 \\\\ opt .digits & \\textbf{0.5567} & \\textbf{0.7115} &            \\\\ pageb & 0.0468 & 0.1894 & 0.1915\\\\ particle & 0.2533 & 0.3678 & 0.4306 \\\\ shuttle & 0.0944 & 0.2711 & 0.2722 \\\\ skin & 0.0853 & 0.3673 & 0.3693 \\\\ spambase & 0.3622 & 0.4844 & 0.5178 \\\\ wave & \\textbf{0.4954} & \\textbf{0.6000} & \\textbf{0.6278} \\\\ wine & \\textbf{0.4860} & \\textbf{0.6355} & \\textbf{0.6554} \\\\ yearp & \\textbf{0.6822} & \\textbf{0.7239} & \\textbf{0.7572} \\\\ yeast & \\textbf{0.9733} & \\textbf{0.9789} & \\textbf{0.9900} \\\\ \\ hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_27": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_27",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_27",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_27",
    "text": "\\begin{itemize}  \\item \\textbf{AUC} --\nWe use the logit transform :\n$$\\text{logit}(AUC)=\\log\\left(\\frac{AUC}{1-AUC}\\right)$$\n\\item \\textbf{AP} --\nBecause AP does not have a constant expectation , one way to normalize AP is to compute the \\textbf{lift} which is the ratio of AP to its expectation .\nIt is commonly assumed that this expectation is equivalent to the anomaly rate , but \\cite{exactAP} shows that while this can be a good approximation it is not exactly correct .\nMore important it can be a bad approximation when relative frequency is low , which is the case for most of our benchmarks .\nWe compute lift using the exact expectation .\nTo map this ratio to all real numbers a sensible further transformation is to take the log of this lift :\n$$\\log(LIFT)=\\log\\left(\\frac{AP}{E[AP]}\\right)$$\n\\end{itemize}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_33": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_33",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_33",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_33",
    "text": "An ANOVA test on each of these models provides a $t$ - test for each variable in the model and an $F$ - test on the model itself .\nTo save space we do not report these individual values ; the results are easily summarized as \\emph{every} test has a $p$ - value well below $0.001$ ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_1": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_1",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_1",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_1",
    "text": "An anomaly detection algorithm takes as input the $N$ data points and produces as output a real - valued anomaly score for each point such that points with higher scores are believed to be more anomalous .\nNatural metrics of quality for anomaly predictions are the area under the ROC curve ( AUC ) and the Average Precision ( AP ; also known as the area under the precision - recall curve ) .\nIn some applications , we are only interested in the top $K$ highest - ranked points , in which case , natural metrics are the precision and recall at $K$ .\nIn other applications , we might choose a threshold and classify all points whose anomaly score exceeds the threshold as anomalies and all other points as nominal .\nIn such settings , common metrics are precision , recall , and F1 ( the harmonic mean of precision and recall ) .\nAccuracy or error rate are typically not very useful , because in most applications the anomalies constitute a very small fraction of the data ( e.g. , from 0.01\\% to 1\\% ) .\nIn this paper , we consider only the AUC and AP metrics ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_0": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_0",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_0",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_0",
    "text": "We study the following unsupervised anomaly detection setting .\nWe are given a collection of $N$ data points $x_1, \\ldots, x_N$ , each a $d$ - dimensional real - valued vector .\nThese data points are a mixture of `` nominal ' ' points and `` anomalous ' ' points .\nHowever , none of the points are labeled .\nThe goal is to identify the anomalous points ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_32": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_32",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_32",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_32",
    "text": "First we build this model for each metric , using our discrete construction factors and our real valued transformations and compare the models .\nOur metric for comparison is the $\\hat{R^2}$ goodness - of - fit measure , which is inversely related to the mean squared error of the model , which is the figure of merit each of these models is trying to optimize ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_26": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_26",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_26",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_26",
    "text": "Later in this section we will to analyze our micro-experiment results with various linear models , and summarizing the means of metrics like AUC and AP which are constrained to range $[0,1]$ can be problematic , especially in the case of AP which does not have a constant expectation .\nFor the remainder of this paper we transform both of these metrics so that they extend to the range of all real numbers in the following ways :"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_22": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_22",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_22",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_22",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! }\n{ \\begin{minipage}\n{ 0.5\\ text width } \\tbl{Benchmark Failure Rate by Metric and Point Difficulty Level (            )\\label{tbl:pdfail} }\n{ \\begin{tabular} { | r | | c | c | c | } \\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline pd - 0 & 0.2887 & 0.3951 & 0.4328 \\\\ pd - 1 & 0.2803 & 0.3988 & 0.4268 \\\\ pd - 2 & \\textbf{0.4252} & \\textbf{0.5257} & \\textbf{0.5576} \\\\ pd - 3 & \\textbf{0.5662} & \\textbf{0.6481} & \\textbf{0.6858} \\\\ pd - 4 & \\textbf{0.7540} & \\textbf{0.8041} & \\textbf{0.8437} \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_36": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_36",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_36",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_36",
    "text": "To better evaluate the importance of each variable on predicting our metrics we construct simpler models that exclude one of the variables from the model and then measure the difference in $\\hat{R^2}$ measures relative to our base model .\nWe also construct a model without all four problem dimension variables to measure the impact of all problem dimensions in aggregate .\n\\cref{tbl:rsq} shows these results .\nBoldfaced items are those with greater $\\hat{R^2}$ loss than the algorithm variable which suggests that the variable is \\emph{more important to your final outcome than your choice of algorithm is} ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_4": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_4",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_4",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_4",
    "text": "We binned this measure into five discrete levels :\n\\begin{itemize}  \\item \\emph{pd-0} : control group ; ( difficulty score $\\in(0,1))$ \\item \\emph{pd-1} : difficulty score $\\in(0,0.1\\overline{6})$ \\item \\emph{pd-2} : difficulty score $\\in[0.1\\overline{6},0.\\overline{3})$ \\item \\emph{pd-3} : difficulty score $\\in[0.\\overline{3},0.5)$ \\item \\emph{pd-4} : difficulty score $\\in[0.5,1)$ \\end{itemize}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_5": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_5",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_5",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_5",
    "text": "We propose to define semantic variation among anomalies with the following measure .\nThe { \\ it normalized clusteredness } ( nc ) of this set of points is defined as \\[\\log\\left(\\frac{\\hat{\\sigma} ^ 2_n } { \\hat{\\sigma} ^ 2_a }\\ right ) \\ ]\nwhere $\\hat{\\sigma}^2_n$ is the sample variance of the selected normal points and $\\hat{\\sigma}^2_a$ is the sample variance of the selected anomaly points .\nWhen normalized clusteredness is less than 0 , the anomaly points exhibit greater semantic variation than the normal points .\nWhen normalized clusteredness is greater than 0 , the anomaly points are more tightly packed than the normal points ( on average ) ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_37": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_37",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_37",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_37",
    "text": "\\begin{table}\n\\resizebox{\\textwidth} { ! } { \\begin{minipage} { 1.15 \\ text width } \\tbl{Changes in            When Different Variables Are Missing\\label{tbl:rsq} } { \\begin{tabular} { | r | | c | c | c | c | }\n\\hline & \\textbf{            } & \\textbf{            } & \\textbf{            } & \\textbf{            } \\\\ \\hline \\hline All Variables & 0.5019 & -- & 0.6382 & --\\\\ w/o Algorithm & 0.4512 & 0.0507 & 0.6013 & 0.0369 \\\\ w/o Motherset & \\textbf{0.2617} & \\textbf{0.2403} & \\textbf{0.5190} &            \\\\ w/o All Problem Dimensions & \\textbf{0.3221} & \\textbf{0.1799} & \\textbf{0.2359} & \\textbf{0.4022} \\\\ -- w/o Relative Frequency & \\textbf{0.4311} & \\textbf{0.0709} & \\textbf{0.4108} & \\textbf{0.2274} \\\\ -- w/o Point Difficulty & 0.4742 & 0.0277 & 0.6264 & 0.0117 \\\\ -- w/o Clusteredness & 0.4909 & 0.0111 & \\textbf{0.6004} & \\textbf{0.0378} \\\\ -- w /o Feature Irrelevance & 0.4831 & 0.0189 & 0.6279 & 0.0103 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_23": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_23",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_23",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_23",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Clustering Strategy (            )\\label{tbl:clfail} }\n{ \\begin{tabular} { | r | | c | c | c | }\n\\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline none & \\textbf{0.4189} & \\textbf{0.5201} & \\textbf{0.5479} \\\\ cluster & \\textbf{0.4431} & \\textbf{0.5891} & \\textbf{0.6036} \\\\ scatter & 0.3695 & 0.4146 & 0.4759 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_35": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_35",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_35",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_35",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.55\\ text width } \\tbl{Problem Dimension Coefficients by Metric\\label{tbl:basecoef} }\n{ \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{            } & \\textbf{            } \\\\ \\hline \\hline\n$\\text{logit}(\\text{relative frequency})$ & - 0.1994 & - 0.3527 \\\\\n$\\text{logit}(\\text{point difficulty})$ & - 0.3209 & - 0.2014 \\\\ $\\text{clusteredness}$ & - 0.1255 & - 0.2141 \\\\ $\\log(\\text{feature irrelevance})$ & - 0.2962 & - 0.1998 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_21": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_21",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_21",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_21",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Benchmark Failure Rate by Metric and Relative Frequency Level (            )\\label{tbl:arfail} }\n{ \\begin{tabular} { | r | | c | c | c | }\n\\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline rf - 0 & 0.3432 & 0.4212 & 0.4324 \\\\ rf - 1 & \\textbf{0.7286} & \\textbf{0.8464} & \\textbf{0.9081} \\\\ rf - 2 & \\textbf{0.5432} & \\textbf{0.6889} &            \\\\ rf - 3 & \\textbf{0.4395} & \\textbf{0.5786} & \\textbf{0.6395} \\\\ rf - 4 & 0.2350 & 0.3076 & 0.3234 \\\\ rf - 5 & 0.2533 & 0.3031 & 0.3139 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_7": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_7",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_7",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_7",
    "text": "To simplify the process of determining how many irrelevant features are needed , we compute an estimate of how many extra features will achieve a desired average distance ratio .\nNote that the expected distance between two vectors ( $\\alpha$ ) whose coordinates are drawn at random ( e.g. , from the unit interval or from a standard normal Gaussian ) grows in proportion to $\\sqrt{d}$ , where $d$ is the dimensionality of the data .\nHence , if a dataset already has $d$ dimensions and we want to estimate $d'$ , the number of dimensions needed to increase the average pairwise distance by a factor of $\\alpha$ , then we need\n$$\\hat{d'} = (\\alpha \\sqrt{d})^2$$\ndimensions , where $\\alpha\\in\\{1.0,1.2,1.5,2.0\\}$ for this study ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_6": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_6",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_6",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_6",
    "text": "\\begin{itemize}\n\\item \\emph{nc-0} : control group ; ( clusteredness not considered ) .\n\\item \\emph{nc-1} : nc $< 0$ ; ( scattered anomalies ) .\n\\item \\emph{nc-2} : nc $> 0$ ; ( clustered anomalies ) .\n\\end{itemize}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_20": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_20",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_20",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_20",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage}\n{ 0.5\\ text width } \\tbl{Benchmark Failure Rate by Metric and Motherset Origin (            )\\label{tbl:origfail} }\n{ \\begin{tabular} { | r | | c | c | c | }\n\\hline & \\textbf{AUC} & \\textbf{AP} & \\textbf{Either} \\\\ \\hline \\hline binary & 0.2490 & 0.3530 & 0.3914 \\\\ multiclass & \\textbf{0.4488} & \\textbf{0.5627} & \\textbf{0.5913} \\\\ regression & \\textbf{0.5159} & \\textbf{0.5830} & \\textbf{0.6219} \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_34": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_34",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_34",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_34",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\ text width } \\tbl{            of Linear Regression by Metric and Variable Type\\label{tbl:facvval} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{            } & \\textbf{            } \\\\ \\hline \\hline Discrete Variables & 0.4910 & 0.6251 \\\\ Real Variables & 0.5019 & 0.6382 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_53": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_53",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_53",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_53",
    "text": "As described in the main body of the paper we used a KNN approximation of the original algorithm .\nThe only parameter in such an implementation is the choice of $k$ , which we set at 0.005 of the data ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_47": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_47",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_47",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_47",
    "text": "\\cref{fig:ar,fig:pd,fig:cl,fig:ir} and \\cref{tbl:basecoef} suggest that our defined problems dimensions all have an impact on experimental results .\nThe reported $\\hat{R^2}$ of our mixed models in \\cref{sec:basic} suggest that choice of motherset , choice of algorithm and our proposed problem dimensions are capable of predicting experimental results with good accuracy .\nBased on this we are able to recommend using our methodology ( or something appropriately similar ) for controlling and measuring these problem dimensions .\nWe encourage further work that focuses on specific contexts that can be defined by these problem dimensions , especially if it maps these contexts to real - world applications ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_46": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_46",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_46",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_46",
    "text": "\\begin{table}\n\\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.63\\text width } \\tbl{Mean Performance by Algorithm and Metric (Normalized by a Trivial Solution)\\label{tbl:triv} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean            } & \\textbf{Mean            } \\\\ \\hline \\hline abod & 0.0654 & 0.1193 \\\\ egmm & 0.0774 & 0.1265 \\\\ iforest & \\textbf{0.1006} &            \\\\ loda & 0.0578 & 0.1340 \\\\ lof & 0.0723 & 0.1513 \\\\ ocsvm & - 0.1004 & 0.0792 \\\\ rkde & 0.0707 & 0.1316 \\\\ svdd & - 0.1817 & - 0.5010 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_52": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_52",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_52",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_52",
    "text": "We employed the \\texttt{R} package \\texttt{Rlof} available at \\url{http://cran.open-source-solution.org/web/packages/Rlof/} .\nWe chose $k$ to be 3 \\ % of the dataset .\nThis was the smallest value for which LOF would reliably run on all datasets ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_44": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_44",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_44",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_44",
    "text": "By eliminating benchmarks based on hypothesis testing we attempted to account for benchmarks that are too difficult or otherwise unrealistic in composition , but an additional concern of ours is that some benchmarks might be trivially easy .\nIt is often the case in anomaly detection literature that reported results are highly accurate , such as in \\cite{loda,inne,Kriegel:2009,rajasegarar2010centered,Amer:2013} whereas the mean accuracy on our corpus of benchmarks is much lower .\nIn particular , consider the work presented in \\cite{inne} ; the authors share the parameterization of each algorithm on each benchmark and praise the algorithm \\textbf{iNNe} for sometimes performing well with parameter $\\psi=2$ .\nHowever , an understanding of the iNNe algorithm and the implication of that parameter choice will reveal that the algorithm is doing little more than approximating the distance of each point from the mean of the data .\nWhile we acknowledge that this particular work is a smaller workshop publication , it should serve as a warning that benchmarks for which all algorithms perform well might be benchmarks that can be trivially solved and their inclusion in reported results might not be helpful ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_50": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_50",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_50",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_50",
    "text": "To reduce the computational cost of fitting , and to improve the numerical stability of the process , we first transformed each benchmark dataset via principle component analysis .\nWe selected principle components ( in descending eigenvalue order ) to retain 95 \\ % of the variance .\nTo generate the members of the ensemble , we varied the number of clusters $k$ by trying all values in $\\{1,2,3,4, 5, 6\\}$ .\nFor each value of $k$ , we generated 15 GMMs by training on 15 bootstrap replicates of the data and by randomly initializing each replicate .\nWe then computed the average out - of - bag log likelihood for each value of $k$ and discarded $k$ values whose average log likelihood was less than 0.85 times the average log likelihood of the best value of $k$ .\nThe purpose of this was to discard GMMs that do not fit the data very well .\nFinally , an anomaly score is computed for each point $x$ by computing the average `` surprise ' ' , which is the average negative log probability density $\\frac{1}{L} \\sum_{\\ell=1}^L -\\log P_{\\ell}(x)$ , where $L$ is the number of fitted GMMs and $P_{\\ell}(x)$ is the density assigned by GMM $\\ell$ to data point $x$ .\nWe found in preliminary experiments that this worked better than using the mean probability density $\\frac{1}{L} \\sum_{\\ell=1}^L P_{\\ell}(x)$ ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_51": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_51",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_51",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_51",
    "text": "We employed the \\texttt{libsvm} implementation of Chang and Lin \\cite{chang:2011} available at \\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/} .\nFor each benchmark , we employed a Gaussian radial basis function kernel .\nSelection of kernel bandwidth was done using the DFN method proposed in \\cite{xiao2014} , while the parameter $\\nu$ was set to 0.03 ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_45": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_45",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_45",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_45",
    "text": "To investigate this phenomenon further , we ran a trivial algorithm against our corpus of benchmarks .\nThe algorithm simply computes the arithmetic mean of the data and assigns an outlier score to each point based on its euclidean distance from that mean .\nWe use the performance of this algorithm to normalize the performance of the other non-trivial algorithms .\nFor both AUC and AP we compute the ratio of a given algorithm 's performance over the performance of the trivial algorithm .\nAs with logit $(AUC)$ and $\\log(LIFT)$ we then take the natural log of these ratios .\n$$\\log\\left(\\frac{AUC_{\\text{non-trivial}}}{AUC_{\\text{trivial}}}\\right), \\log\\left(\\frac{AP_{\\text{non-trivial}}}{AP_{\\text{trivial}}}\\right) $$\nIn the case of AP , this is very similar to our previous transformed metric , except instead of normalizing against random expectation , we are normalizing against the performance of a trivial solution .\nUnder this metric , achieving a perfect score on a benchmark that is also perfectly or almost - perfectly solved by a trivial solution is not given much merit , while a lower score that is a significant improvement on trivial performance is given much more credit .\n\\cref{tbl:triv} shows the mean performance of each algorithm under these new metrics ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_41": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_41",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_41",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_41",
    "text": "\\begin{table}\n\\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric with Many Irrelevant Features\\label{tbl:hiir} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean            } & \\textbf{Mean            } \\\\ \\hline \\hline abod & 0.4424 & 0.4612 \\\\ eg mm & 0.4561 & 0.5069 \\\\ iforest & \\textbf{0.8117} &            \\\\ loda & 0.5803 & 0.7722 \\\\ lof & 0.6199 & 0.7187 \\\\ ocsvm & 0.5752 & 0.8527 \\\\ rkde & 0.5477 & 0.6716 \\\\ svdd & 0.3366 & 0.3580 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_55": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_55",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_55",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_55",
    "text": "The AUC or AP of a random ranking can be seen as a discrete parametric distribution with parameters $n_{\\text{norm}}$ ( number of normals ) and $n_{\\text{anom}}$ ( number of anomalies ) .\nThe distribution is parametric because there are `` only ' ' $(n_{\\text{norm}}+n_{\\text{anom}})!$ possible rankings , meaning there are a finite number of possible AUC or AP scores for a given set of parameters ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_54": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_54",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_54",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_54",
    "text": "We implemented the algorithm as suggest by the authors of \\cite{loda} .\nEach projection used approximately $\\sqrt{d}$ features and a total of $3d$ projections were used , where $d$ is the number of features in the benchmark ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_40": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_40",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_40",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_40",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric with No Irrelevant Features\\label{tbl:noir} }\n{ \\begin{tabular} { | r | | c | c | }\n\\hline & \\textbf{Mean            } & \\textbf{Mean            } \\\\ \\hline \\hline abod & 1.3706 &            \\\\ egmm & 1.3522 & 1.2120 \\\\ iforest & 1.3145 & 1.1690 \\\\ loda & 1.1020 & 1.0235 \\\\ lof & 1.1800 & 1.0958 \\\\ ocsvm & 0.5654 & 0.9077 \\\\ rkde & \\textbf{1.4256} & 1.1747 \\\\ svdd & - 0.0118 & 0.2290 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_56": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_56",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_56",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_56",
    "text": "In both cases it is possible to enumerate these scores and compute how much probability mass each score carries , and thus quantiles of these distributions can be computed .\nHowever for larger values of $n$ this becomes computationally inefficient ."
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_42": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_42",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_42",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_42",
    "text": "\\begin{table}  \\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric When Clusteredness is Greater Than 0.25\\label{tbl:hicl} }\n{ \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean            } & \\textbf{Mean            } \\\\ \\hline \\hline abod & \\textbf{1.0305} &            \\\\ egmm & 0.7199 & 0.4834 \\\\ iforest & 0.7405 & 0.5075 \\\\ loda & 0.5689 & 0.4225 \\\\ lof & 0.7623 & 0.5336 \\\\ ocsvm & 0.0385 & 0.2316 \\\\ rkde & 0.7179 & 0.5387 \\\\ svdd & 0.0327 & 0.0639 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  },
  "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_43": {
    "id": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_43",
    "phase": "test",
    "topic": "cs.ai",
    "document": "1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem",
    "paragraph": "paragraph_43",
    "prefix": "selected/test/cs.ai-ann3/1503.01158v2.A_Meta_Analysis_of_the_Anomaly_Detection_Problem/paragraph_43",
    "text": "\\begin{table}\n\\resizebox{\\textwidth} {! } { \\begin{minipage} { 0.5\\text width } \\tbl{Mean Performance by Algorithm and Metric with Mothersets Selected for Dubious Reasons\\label{tbl:nefarious} } { \\begin{tabular} { | r | | c | c | } \\hline & \\textbf{Mean            } & \\textbf{Mean            } \\\\ \\hline \\hline abod & 0.6094 & 0.6071 \\\\ eg mm & 0.5826 & 0.6285 \\\\ iforest & 0.7675 & 0.8903\\\\ loda & 0.7981 &            \\\\ lof & 0.5999 & 0.7340\\\\ ocsvm & 0.5121 & 0.8080 \\\\ rkde & \\textbf{0.8554} & 0.9222\\\\ svdd & 0.1764 & 0.3164 \\\\ \\hline \\end{tabular} } \\end{minipage} } \\end{table}"
  }
}