{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Framework-Week1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# workshop folder, e.g. 'acmlab/workshops/project'\n","FOLDERNAME = 'primary_description'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_B2KowBG8Zx_","executionInfo":{"status":"ok","timestamp":1642284323875,"user_tz":480,"elapsed":15361,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"4a63e437-c993-438d-8a48-10ec3462d729"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/primary_description\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import os\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","from textblob import TextBlob\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","\n","from IPython.display import display\n","\n","# import stanza\n","# stanza.install_corenlp()\n","# import spacy\n","# nltk.download('brown')"],"metadata":{"id":"l4pMi_3HCdxb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642284326419,"user_tz":480,"elapsed":2547,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"783f8434-36d9-4f2d-b9a3-9efc7359c547"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVkvU5UR6h6N"},"outputs":[],"source":["data = pd.read_json(\"Practice/cs.ai-ann0.json\")\n","# text_data = data.loc[\"text\"]\n","# text_data.to_dict()"]},{"cell_type":"code","source":["def extract_primary_description(entity): #Entity is a dicitionary.\n","  output = {}\n","  for key in entity.keys():\n","    if entity[key]['label'] == \"PRIMARY\":\n","      output[key] = entity[key]\n","  return output\n"],"metadata":{"id":"fcac7jyqAIe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute(text): #This function will take text and output a data structure of primary descriptions that are predicted\n","  \n","  \n","  \n","  \n","  # is_noun = lambda pos: pos[:2] == 'NN'\n","  # # do the nlp stuff\n","  # tokenized = nltk.word_tokenize(text)\n","  # nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n","  # print(nouns)\n","  \n","  # blob = TextBlob(text)\n","  # print(blob.noun_phrases)\n","  return text"],"metadata":{"id":"0aa0Md7u-LYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = nltk.WordNetLemmatizer()\n","def setup(text):\n","\n","  #word tokenizeing and part-of-speech tagger\n","  document = text\n","  tokens = [nltk.word_tokenize(sent) for sent in [document]]\n","  postag = [nltk.pos_tag(sent) for sent in tokens][0]\n","\n","  # Rule for NP chunk and VB Chunk\n","  grammar = r\"\"\"\n","      NBAR:\n","          {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n","          {<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?} # Verbs and Verb Phrases\n","          \n","      NP:\n","          {<NBAR>}\n","          {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n","          \n","  \"\"\"\n","  #Chunking\n","  cp = nltk.RegexpParser(grammar)\n","\n","  # the result is a tree\n","  tree = cp.parse(postag)\n","  return tree\n","\n","def leaves(tree):\n","    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n","    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):\n","        yield subtree.leaves()\n","        \n","def get_word_postag(word):\n","    if pos_tag([word])[0][1].startswith('J'):\n","        return wordnet.ADJ\n","    if pos_tag([word])[0][1].startswith('V'):\n","        return wordnet.VERB\n","    if pos_tag([word])[0][1].startswith('N'):\n","        return wordnet.NOUN\n","    else:\n","        return wordnet.NOUN\n","    \n","def normalise(word):\n","    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n","    # word = word.lower()\n","    # postag = get_word_postag(word)\n","    # word = lemmatizer.lemmatize(word, postag)\n","    return word\n","\n","def get_terms(tree):    \n","    for leaf in leaves(tree):\n","        terms = [normalise(w) for w,t in leaf]\n","        yield terms\n","\n","# def getNounPhrases(text):\n","#   tree = setup(text)\n","\n","#   terms = get_terms(tree)\n","\n","#   features = []\n","#   for term in terms:\n","#       _term = ''\n","#       for word in term:\n","#           _term += ' ' + word\n","#       if '\\\\' not in _term:\n","#         features.append(_term.strip())\n","#   return features\n","\n","def getNounPhrases(text):\n","  wordsToRemove = ['be', 'is', 'are', 'was', 'were', 'been', 'being']\n","  tree = setup(text)\n","\n","  terms = get_terms(tree)\n","\n","  features = []\n","  for term in terms:\n","      _term = ''\n","      for word in term:\n","        _term += ' ' + word\n","      # if wordsToRemove not in _term:\n","      # if not any(_term for x in wordsToRemove):\n","        # print(_term)\n","      if not any(x in _term.split() for x in wordsToRemove) and '\\\\' not in _term:\n","        features.append(_term.strip())\n","  return features"],"metadata":{"id":"od1I_w4VNa0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(getNounPhrases(\"Mostly, there just is no default way of determining the paragraph boundary and people tend to work with sentences. Still, the unit of a paragraph might be of a higher value than that of a sentence. Examples might be: coreference resolutions that overlap multiple sentences. Questions that find their answer throughout a whole paragraph. A reader that understands a paragraph better than an isolated sentence. Itâ€™s clear that the signal from a writer is best expressed in a paragraph.\"))"],"metadata":{"id":"_5iGVjccQaLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = data\n","for column in df:\n","    wantedOutput = extract_primary_description(df[column][\"entity\"])\n","    paragraph = df[column][\"text\"]\n","    print(\"PARAGRAPH:\\n\", paragraph)\n","    print(\"NOUN PHRASES:\\n\", getNounPhrases(paragraph))\n","    print(\"EXPECTED:\\n\", wantedOutput)\n","    print(\"*\" * 280)\n","    print(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhunPJLjSL0J","executionInfo":{"status":"ok","timestamp":1642284331764,"user_tz":480,"elapsed":4466,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"9476e573-8054-46fc-fc61-c21136665470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PARAGRAPH:\n"," In this model , the input vectors are representations of whether the student answered a particular question correctly or incorrectly at the previous time step , and the output vectors are representations of the probability , over all the questions in the question bank , that a student will get the question correct at the following time step .\n","In \\cite{DKTNIPS} , the authors propose using a one - hot vector $\\Xstvec \\in \\R^{2I}$ to represent the response of a student $s$ ( on item $i$ ) at time $t$ .\n","Here $I$ is the total number of items and the first $I$ slots represent answering correctly and the remaining $I$ slots represent answering incorrectly .\n","Output vectors $\\Ystvec \\in \\R^I$ are vectors of probabilities , where the $i$ th element of $\\Ystvec$ is the model 's predicted probability that student $s$ would answer item $i$ correctly at time $t + 1$ .\n","NOUN PHRASES:\n"," ['model', 'input vectors', 'representations', 'student', 'answered', 'particular question', 'previous time step', 'output vectors', 'representations', 'probability', 'questions', 'question bank', 'student', 'get', 'question correct', 'following time step', 'DKTNIPS', 'authors', 'propose using', 'hot vector', 'represent', 'response', 'student', 'item', 'time', 'total number', 'items', 'slots', 'represent answering', 'remaining', 'slots', 'represent answering', 'Output vectors', 'vectors', 'probabilities', 'th element', 'model', 'predicted probability', 'student', 'answer', 'time', 't +']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 391, 'end': 409, 'text': 'a one - hot vector'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 445, 'end': 502, 'text': 'the response of a student $s$ ( on item $i$ ) at time $t$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 461, 'end': 470, 'text': 'a student'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 480, 'end': 484, 'text': 'item'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 494, 'end': 498, 'text': 'time'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 517, 'end': 542, 'text': 'the total number of items'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 537, 'end': 542, 'text': 'items'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 561, 'end': 566, 'text': 'slots'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 619, 'end': 624, 'text': 'slots'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 659, 'end': 673, 'text': 'Output vectors'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 697, 'end': 721, 'text': 'vectors of probabilities'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 805, 'end': 812, 'text': 'student'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 830, 'end': 834, 'text': 'item'}, 'T25': {'eid': 'T25', 'label': 'PRIMARY', 'start': 852, 'end': 856, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We fit the parameters according to the procedure described in \\cite{EkanadhamKarklin15} .\n","Estimating the entire trajectory $\\thetastraj$ for each student simultaneously with item parameters is very expensive and difficult to do in real - time .\n","To simplify the approach , we learn parameters in two stages : \\begin{enumerate}  \\item We learn the            according to a standard 1PO IRT model (see Section~\\ref{sec:irtlearning} ) on the training student population and freeze these during validation .\n","           .           \n","For the second step , we combine the approximation :            & P ( \\{(s', i, r, t') \\in D: s'=s, t'\\leq t\\}|\\theta_{s,t} ) \\approx \\nonumber \\\\ &\\prod_{(s',i,r,t') \\in D: s'=s, t'\\leq t} P ( ( s' , i , r , t ' ) | \\theta_{s,t} ) \\end{align} with \\eqref{eq:wiener} , integrating out previous proficiencies of the student to get a tractable approximation of the log posterior over the student 's current proficiency given previous responses :\n","\\begin{align}  \\log P(\\theta_{s,t} | D ) &\\ approx            } [ r            _{t'} (            -\\beta_i ) ) + \\ nonumber \\\\ & ( 1 - r )            _{t'} (            -\\beta_i ))) ]\\, , \\end{align} where $\\tilde{\\alpha}_{t'}=\\left(1 + \\gamma^2(t-t')\\right)^{-1/2}$ .\n","The            's are essentially discounting the relative effect of older responses when estimating the current proficiency .\n","See \\cite{EkanadhamKarklin15} for details .\n","NOUN PHRASES:\n"," ['fit', 'parameters', 'according', 'procedure', 'described', 'EkanadhamKarklin15', 'Estimating', 'student', 'item parameters', 'do', 'time', 'simplify', 'approach', 'learn', 'parameters', 'stages', 'enumerate', 'learn', 'according', 'IRT model', 'see', 'sec', 'irtlearning', 'training student population', 'freeze', 'validation', 'second step', 'combine', 'approximation', 'P', 's', 'i', 'r', 't', 'D', \"s'=s\", 's', 't', 's', 'i', 'r', 't', 'D', \"s'=s\", 'P', 's', 'i', 'r', 't', '|', 's', 't', 'align', 'eq', 'wiener', 'integrating', 'previous proficiencies', 'student', 'get', 'tractable approximation', 'log posterior', 'student', 'current proficiency', 'given', 'previous responses', 'align', 's', 't', '| D', '[ r _', 't', '+', 'r', 't', 'align', '_', 't', '-1/2', 'essentially discounting', 'relative effect', 'responses', 'estimating', 'current proficiency', 'See', 'EkanadhamKarklin15', 'details']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 101, 'end': 122, 'text': 'the entire trajectory'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We use a model with one hidden layer , of dimension $H$ , which is fully connected \\footnote{Note that in \\cite{DKTNIPS} , an LSTM network was used in addition to the RNN described here , and the performance of the two networks was comparable .\n","} to both the input and output layers , as well as recurrently to itself .\n","This model is able to capture temporal effects ( via the recurrent component of the network ) and remains flexible enough to describe non-trivial relationships between items .\n","NOUN PHRASES:\n"," ['use', 'model', 'hidden layer', 'dimension', 'H', 'fully connected', 'Note', 'DKTNIPS', 'LSTM network', 'addition', 'RNN', 'described', 'performance', 'networks', 'input', 'output layers', 'model', 'capture', 'temporal effects', 'recurrent component', 'network', 'remains', 'describe', 'non-trivial relationships', 'items']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 42, 'end': 51, 'text': 'dimension'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," 1PO IRT and HIRT assume each student 's knowledge state remains constant over time .\n","However , in a setting where a student may be acquiring ( or forgetting ) knowledge over a period of time ( e.g. , while interacting with a tutoring system ) , we can extend this model by modeling each $\\theta_s$ as a stochastic process varying over time ( see for example \\cite{FAST14} ) .\n","We adopt the approach described in \\cite{EkanadhamKarklin15} , modeling the student 's knowledge as a Wiener process :\n","NOUN PHRASES:\n"," ['IRT', 'HIRT', 'assume', 'student', 'knowledge state', 'remains', 'time', 'setting', 'student', 'forgetting', 'knowledge', 'period', 'time', 'e.g', 'interacting', 'tutoring', 'system', 'extend', 'model', 'modeling', 'stochastic process', 'varying', 'time', 'see', 'FAST14', 'adopt', 'approach', 'described', 'EkanadhamKarklin15', 'modeling', 'student', 'knowledge', 'Wiener process']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{align} & P ( \\theta_{s,t+\\tau} | \\theta_{s,t} ) = e ^ { - \\frac{(\\theta_{s,t+\\tau} - \\theta_{s,t} ) ^ 2 } { 2 \\gamma^2 \\tau}} ~\\forall s,t,\\tau \\, . \\label{eq:wiener}  \\end{align}\n","In other words , the change in student $s$ 's knowledge state between time $t$ and a future time $t+\\tau$ ( expressed as $\\theta_{s,t} - \\theta_{s,t+\\tau}$ ) is normally distributed about 0 with variance $\\gamma^2\\tau$ where $\\gamma$ is a parameter controlling the `` smoothness '' with which the knowledge state varies over time .\n","NOUN PHRASES:\n"," ['align', 'P', 's', 's', 't', 's', 's', 't', 't', 'eq', 'wiener', 'align', 'other words', 'change', 'student', 'knowledge state', 'time', 'future time', 'expressed', 's', 't', 's', 'normally distributed', 'variance', 'parameter', 'controlling', 'smoothness', 'knowledge state varies', 'time']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 218, 'end': 225, 'text': 'student'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 257, 'end': 261, 'text': 'time'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 270, 'end': 283, 'text': 'a future time'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 382, 'end': 390, 'text': 'variance'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 424, 'end': 468, 'text': \"a parameter controlling the `` smoothness ''\"}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In order to make learning tractable , we reduced the dimensionality of the input by projecting the $\\Xstvec \\in \\R^{2I}$ to a lower dimensional space $\\R^C$ using a random projection matrix $c : \\R^{2I} \\to \\R^C$ , as was done in \\cite{DKTNIPS} .\n","We used batch gradient ascent with dropout \\cite{Dropout} , and chose the input dimensionality $C$ and the hidden dimensionality $H$ by sweeping these parameters on a data set that was held out from the data used for training and cross-validation .\n","NOUN PHRASES:\n"," ['order', 'make', 'learning', 'reduced', 'dimensionality', 'input', 'projecting', 'dimensional space', 'using', 'random projection', 'matrix', 'c', 'DKTNIPS', 'used', 'batch gradient ascent', 'Dropout', 'chose', 'input dimensionality', 'C', 'hidden dimensionality', 'H', 'sweeping', 'parameters', 'data set', 'data', 'used', 'training', 'cross-validation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 124, 'end': 149, 'text': 'a lower dimensional space'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 163, 'end': 189, 'text': 'a random projection matrix'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 71, 'end': 80, 'text': 'the input'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 49, 'end': 80, 'text': 'the dimensionality of the input'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 317, 'end': 341, 'text': 'the input dimensionality'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 350, 'end': 375, 'text': 'the hidden dimensionality'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Data was collected from a variety of educational products integrated with Knewton 's adaptive learning platform and used in various classroom settings across the world .\n","These products vary with respect to the educational content used ( disciplines spanned math , science , and English language learning ) as well as the way in which students are guided through the content .\n","For example , students may take an initial assessment and then be remediated on areas needing improvement .\n","In other products , students start from the beginning and work toward a predefined goal set by the teacher .\n","In all of these settings , Knewton receives data about each interaction ( the $(s,i,r,t)$ tuple of Section ~ \\ref{sec:models} ) .\n","We utilized approximately 1 M responses of 6.3 K randomly sampled students on 105.6 K questions spanning roughly 4 months .\n","Students who worked on fewer than 5 questions total were excluded .\n","After pre-processing , student history lengths ranged from 5 to 3.2 K responses .\n","The overall percent correct of these responses is 54.6 \\% .\n","NOUN PHRASES:\n"," ['Data', 'variety', 'educational products', 'integrated', 'Knewton', 'adaptive learning platform', 'used', 'various classroom settings', 'world', 'products', 'vary', 'respect', 'educational content', 'used', 'disciplines', 'spanned', 'math', 'science', 'English language', 'learning', 'way', 'students', 'content', 'example', 'students', 'take', 'initial assessment', 'areas', 'needing', 'improvement', 'other products', 'students', 'start', 'beginning', 'work', 'predefined goal', 'set', 'teacher', 'settings', 'Knewton', 'receives', 'data', 'interaction', 's', 'i', 'r', 't', 'tuple', 'sec', 'models', 'utilized', 'M responses', 'K', 'randomly sampled', 'students', 'K questions', 'spanning', 'months', 'Students', 'worked', 'questions', 'pre-processing', 'student history lengths', 'ranged', 'K responses', 'overall percent correct', 'responses']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 683, 'end': 688, 'text': 'tuple'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," One distinct difference between Carnegie Learning 's product and ASSISTments is that Carnegie Learning provides much finer representations of the concepts assessed by an individual item .\n","In particular , Carnegie Learning is built around scaffolded , formative assessment , where each { \\em step} a student takes to answer a {\\em problem} is counted as a separate interaction , with each step potentially assessing different skills ( called Knowledge Components ( KCs ) in the data set ) .\n","Note that this `` Problem $\\to$ Step ' ' structure provides a hierarchy which HIRT ( Section ~ \\ref{sec:hirt} ) can exploit .\n","NOUN PHRASES:\n"," ['distinct difference', 'Carnegie Learning', 'product', 'ASSISTments', 'Carnegie Learning', 'provides', 'representations', 'concepts', 'assessed', 'individual item', 'Carnegie Learning', 'scaffolded', 'formative assessment', 'student', 'takes', 'answer', 'separate interaction', 'step', 'potentially assessing', 'different skills', 'called', 'Knowledge Components', 'KCs', 'data set', 'Note', 'Problem', 'Step', 'structure', 'provides', 'hierarchy', 'HIRT', 'sec', 'hirt', 'exploit']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The predictions are given by the following equations :\n","\\begin{align}\n","           _{ s , t +1 } & = g ( W_{ hh }            _{ s , t } + W_{ xh } c (            _h ) \\\\\n","           _{ s , t +1 } & = \\phi(W_{hy}            _{ s , t +1 } +            _y )\n","\\end{align}\n","Here , $g$ and $\\phi$ are the logistic and arctangent functions , respectively .\n","The parameters of the model $W_{hh},W_{xh},W_{hy},\\vec{b}_h, \\vec{b}_y$ are fit by optimizing the cross-entropy of the responses with the predicted probabilities ( which is equivalent to the log likelihood if these probabilities were produced via a generative probabilistic model ) :\n","\\begin{equation}\n","\\sum_{(s,i,r,t) \\in D} r\\log y_{s,t,i} + (1-r)\\log (1 - y_{s,t,i})\n","\\end{equation}\n","Stochastic gradient ascent with minibatches of students on the unrolled RNN , coded using Theano \\cite{Theano} , was used to optimize this objective function .\n","NOUN PHRASES:\n"," ['predictions', 'following equations', 'align', '_', 's', 't +1', '= g', 'W_', 'hh', '_', 's', 't', '+ W_', 'xh', 'c', '_h', 's', 't +1', 'W_', 'hy', '_', 's', 't +1', '+ _y', 'align', 'arctangent functions', 'parameters', 'model', 'W_', 'hh', 'W_', 'xh', 'W_', 'hy', 'b', '_h', 'b', 'optimizing', 'cross-entropy', 'responses', 'predicted probabilities', 'log likelihood', 'probabilities', 'generative probabilistic model', 'equation', 'i', 'r', 't', 'D', 's', 't', 'i', '+', 'y_', 's', 't', 'i', 'equation', 'Stochastic gradient ascent', 'minibatches', 'students', 'unrolled RNN', 'coded using', 'Theano', 'optimize', 'objective function']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 289, 'end': 301, 'text': 'the logistic'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 306, 'end': 326, 'text': 'arctangent functions'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 344, 'end': 371, 'text': 'The parameters of the model'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure*} [ t ! ] \\includegraphics[width=\\textwidth]{edm_pred_accuracy_results.pdf}  \\vspace{-20pt}  \\caption{Summary of results across models and metrics. Error bars represent the standard error of measure of the metric across five folds. For TIRT, parameter selection yielded            for ASSISTments,            for KDD (making it identical to IRT), and            for Knewton. For HIRT, parameter selection yielded            and            for ASSISTments,            and            for KDD, and            and            for Knewton. For DKT,            ,            , and the probability of dropout is            for all models.}  \\label{fig:results}  \\end{figure*}\n","NOUN PHRASES:\n"," ['figure*', '[ t', '[', 'edm_pred_accuracy_results.pdf', '-20pt', 'Summary', 'results', 'models', 'metrics', 'Error', 'bars represent', 'standard error', 'measure', 'folds', 'TIRT', 'parameter selection', 'yielded', 'ASSISTments', 'KDD', 'making', 'IRT', 'Knewton', 'HIRT', 'parameter selection', 'yielded', 'ASSISTments', 'KDD', 'Knewton', 'DKT', 'probability', 'dropout', 'models', 'fig', 'results', 'figure*']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table*} [ t ! ] \\ centering \\begin{tabular} {@{} rrrrr@{}}\\ toprule & IRT & HIRT & tIRT & DKT $^\\ast$ \\\\\\ midrule ASSISTments & { \\tt problem\\_id} & {\\tt template\\_id} $\\to$ { \\tt problem\\_id} & {\\tt problem\\_id} & {\\tt template \\_id } \\\\ KDD & { \\tt Step Name} & {\\tt Problem Name} $\\to$ { \\tt Step Name} & {\\tt Step Name} & {\\tt KC } \\\\ Knewton & { \\tt item\\_id} & {\\tt concept\\_id} $\\to$ { \\tt item\\_id} & {\\tt item\\_id} & {\\tt concept \\_id } \\\\ \\ bottomrule \\end{tabular}  \\caption{Item labels yielding best results for each model and data set. For HIRT, the first label specifies the difficulty mean grouping identifier, and the second the item identifier.}  \\label{tab:bestlabels}  \\end{table*}\n","NOUN PHRASES:\n"," ['table*', '[ t', 'rrrrr', 'IRT', 'HIRT', 'tIRT', 'DKT', 'Item', 'labels yielding', 'results', 'model', 'data set', 'HIRT', 'first label', 'specifies', 'difficulty mean', 'grouping', 'identifier', 'item identifier', 'tab', 'bestlabels', 'table*']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The model assumes that many students have completed a test of dichotomous items and assigns each student $s$ a proficiency $\\theta_s \\in \\R$ .\n","A key innovation of IRT is to model variation across different items .\n","In its simplest form , the { \\em one-parameter model}, each item            is assigned a parameter            , representing the {\\em difficulty} of the item .\n","The probability that a student $s$ answers item $i$ correctly is given by $f(\\theta_s - \\beta_i)$ , where $f$ is some sigmoidal function .\n","NOUN PHRASES:\n"," ['model', 'assumes', 'many students', 'have completed', 'test', 'dichotomous items', 'assigns', 'student', 'proficiency', 'A key innovation', 'IRT', 'model', 'variation', 'different items', 'form', 'item', 'parameter', 'representing', 'difficulty', 'item', 'probability', 'student', 'answers', 'item', 'f', 'sigmoidal function']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 97, 'end': 104, 'text': 'student'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 109, 'end': 122, 'text': 'a proficiency'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 274, 'end': 278, 'text': 'item'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 302, 'end': 313, 'text': 'a parameter'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 396, 'end': 405, 'text': 'a student'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 418, 'end': 422, 'text': 'item'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 375, 'end': 436, 'text': 'The probability that a student $s$ answers item $i$ correctly'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 493, 'end': 511, 'text': 'sigmoidal function'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," When $f$ is the logistic function , this corresponds to ( structured ) logistic regression , where the factors for a response to an item are indicators for students and items .\n","We use a variant of this model known as 1PO ( one - parameter ogive ) IRT , where the link function $f(x) = \\Phi(x)$ is the cumulative distribution function of the standard normal distribution \\footnote{The ogive yields nearly identical results to the commonly used logistic link function, but allows closed-form posterior computation in the temporal IRT model described in Sec.~\\ref{sec:tskirt} } .\n","The maximum likelihood solution of $\\{\\theta_s, \\beta_i\\}$ is underdetermined \\footnote{For example, the response predictions are invariant when adding a constant offset to the            's and            's.} ; we take a Bayesian approach and regularize the solution of $\\{\\theta_s, \\beta_i\\}$ by imposing independent standard normal prior distributions over each $\\theta_s$ and $\\beta_i$ .\n","NOUN PHRASES:\n"," ['logistic function', 'corresponds', 'structured', 'logistic regression', 'factors', 'response', 'item', 'indicators', 'students', 'items', 'use', 'variant', 'model', 'known', 'parameter ogive', 'IRT', 'link function', 'f', 'x', '=', 'x', 'cumulative distribution function', 'ogive yields', 'identical results', 'commonly used', 'logistic link function', 'allows', 'closed-form posterior computation', 'temporal IRT model', 'described', 'sec', 'tskirt', 'maximum likelihood solution', 'example', 'response predictions', 'adding', 'constant offset', 'take', 'Bayesian approach', 'regularize', 'solution', 'imposing', 'independent standard normal prior distributions']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 12, 'end': 33, 'text': 'the logistic function'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 259, 'end': 276, 'text': 'the link function'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 297, 'end': 369, 'text': 'the cumulative distribution function of the standard normal distribution'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{itemize} \\item\n","For 1 PO IRT there were no parameters to select .\n","\\item For HIRT , we swept values of the variances $\\tau^2$ and $\\sigma^2$ of the group means and item difficulties respectively , including regimes ( $\\tau^2$ small ) which made the model mathematically equivalent to 1 PO IRT .\n","\\item For TIRT , we swept the temporal smoothness parameter $\\gamma^2$ , including the regime ( $\\gamma^2$ small ) which made the model mathematically equivalent to 1 PO IRT .\n","\\item For DKT , we swept the compression dimension $C$ ( the dimension of the space to which the input was projected using a random matrix ) , the hidden dimension $H$ , the dropout probability $p$ , and the step size of our gradient ascent .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['itemize', 'PO IRT', 'parameters', 'select', 'HIRT', 'swept', 'values', 'variances', 'group', 'means', 'item difficulties', 'including', 'regimes', 'made', 'model', 'PO IRT', 'TIRT', 'swept', 'temporal smoothness', 'including', 'regime', 'made', 'model', 'PO IRT', 'DKT', 'swept', 'compression dimension', 'C', 'dimension', 'space', 'input', 'random matrix', 'hidden dimension', 'H', 'dropout probability', 'step size', 'gradient ascent', 'itemize']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 108, 'end': 121, 'text': 'the variances'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 326, 'end': 359, 'text': 'the temporal smoothness parameter'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 501, 'end': 526, 'text': 'the compression dimension'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 533, 'end': 614, 'text': 'the dimension of the space to which the input was projected using a random matrix'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 619, 'end': 639, 'text': 'the hidden dimension'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 646, 'end': 669, 'text': 'the dropout probability'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section we set notation and describe the models we compare .\n","Throughout , we will represent the student response data $D$ as a set of tuples $(s,i,r,t)$ indicating the student , item , correctness , and time of each response .\n","In this paper , time will be indexed by interaction index ( rather than wall clock time ) .\n","NOUN PHRASES:\n"," ['section', 'set', 'notation', 'describe', 'models', 'compare', 'Throughout', 'represent', 'student response data', 'D', 'set', 'tuples', 's', 'i', 'r', 't', 'indicating', 'student', 'item', 'correctness', 'time', 'response', 'paper', 'time', 'interaction index', 'wall', 'clock time']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 100, 'end': 125, 'text': 'the student response data'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 133, 'end': 148, 'text': 'a set of tuples'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 142, 'end': 148, 'text': 'tuples'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 176, 'end': 183, 'text': 'student'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 186, 'end': 190, 'text': 'item'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 193, 'end': 204, 'text': 'correctness'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 211, 'end': 232, 'text': 'time of each response'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\numberofauthors{1} %\n","in this sample file , there are a * total * \\ author { \\ alignauthor Kevin H. ~ Wilson \\titlenote{Contributed equally to the work.} , Yan Karklin \\raisebox{9pt} { $\\ast$ } , Bojian Han \\titlenote{Performed initial coding and analysis while at Knewton.} , Chaitanya Ekanadham \\raisebox{9pt} { $\\ast$ } \\\\ \\affaddr{            Knewton, Inc. New York, NY \\hspace{2em} $^\\dagger$ Carnegie Mellon University . Pittsburgh , PA } \\\\            @knewton.com \\hspace{3em} bojianh@andrew.cmu.edu } }\n","NOUN PHRASES:\n"," ['%', 'sample file', 'Contributed', 'work', 'Performed initial coding', 'analysis while', 'Knewton', 'Knewton', 'Inc. New York', 'Carnegie Mellon University', 'Pittsburgh', 'PA', 'knewton.com', 'bojianh @ andrew.cmu.edu']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The temporal IRT model yielded higher accuracy on the Knewton dataset , but not on the other two data sets .\n","To understand these effects , we investigated the degree to which temporal structure in the data affects predictive performance by looking at how a naive `` windowed percent correct '' ( predict the student will answer the $t$ th question correctly if they answer at least half of the previous $w$ questions correctly ) model performs as a function of window length $w$ ( Figure ~ \\ref{fig:box_len} ) .\n","The Knewton data set has a clear optimal window length -- integrating over windows too short or too long degraded performance , which is indicative of nontrivial temporal structure .\n","However , for the ASSISTments and KDD data sets , longer window lengths perform equal or better than shorter window lengths , suggesting that static models would do just as well in these cases .\n","Indeed , this would explain why TIRT does more or less the same as baseline 1PO IRT on ASSISTments and KDD but shows significant improvement on the Knewton data set .\n","However , it does not explain why DKT lags regardless of the amount of temporal structure .\n","NOUN PHRASES:\n"," ['temporal IRT model', 'yielded', 'accuracy', 'Knewton dataset', 'data sets', 'understand', 'effects', 'investigated', 'degree', 'temporal structure', 'data affects predictive performance', 'looking', 'windowed', 'percent correct', 'predict', 'student', 'answer', 'th question', 'answer', 'half', 'questions', 'model performs', 'function', 'window length', 'fig', 'box_len', 'Knewton data set', 'has', 'clear optimal window length', 'integrating', 'windows', 'degraded performance', 'nontrivial temporal structure', 'ASSISTments', 'KDD data sets', 'window lengths', 'perform', 'window lengths', 'suggesting', 'static models', 'do', 'cases', 'explain', 'TIRT', 'does', 'baseline', 'IRT', 'ASSISTments', 'KDD', 'shows', 'significant improvement', 'Knewton data set', 'does', 'not explain', 'DKT', 'lags', 'amount', 'temporal structure']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 407, 'end': 416, 'text': 'questions'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 461, 'end': 474, 'text': 'window length'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To train the parameters on student response data , we maximize the log posterior probability of $\\{\\theta_s, \\beta_i\\}$ given the response data ( the set of response correctnesses $\\{r: (s,i,r,t) \\in D\\}$ , each of which is 0 or 1 ) .\n","Assuming independent , standard normal priors on each $\\theta_s$ , $\\beta_i$ , the log posterior is : \\begin{align} & \\log P(\\{\\theta_s\\} ,\\{\\beta_i\\}| D ) = \\nonumber \\\\ & \\sum_{(s,i,r,t) \\in D} r\\ log f (\\theta_s-\\beta_i ) + ( 1 - r )\\ log ( 1 - f (\\theta_s-\\beta_i ) ) \\nonumber \\\\ & -            {2}            {2}\\sum_i\\beta_i^2 + C\\, .           \n","We maximize this objective with respect to the parameters using standard second - order ascent methods to obtain the maximum a posteriori ( MAP ) estimate of each parameter .\n","NOUN PHRASES:\n"," ['train', 'parameters', 'student response data', 'maximize', 'log posterior probability', 'given', 'response data', 'set', 'response correctnesses', 'r', 'r', 't', 'Assuming', 'standard normal priors', 'log posterior', 'align', '| D', '=', 'i', 'r', 't', 'D', 'log f', '+', 'r', 'log', 'f', 'maximize', 'objective', 'respect', 'parameters', 'using', 'order ascent methods', 'obtain', 'maximum', 'posteriori', 'MAP', 'estimate', 'parameter']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 146, 'end': 179, 'text': 'the set of response correctnesses'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 314, 'end': 331, 'text': 'the log posterior'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In many situations , including each of our data sets , the assessment items may have structure that can inform predictions of student responses .\n","For example , groups of items may assess the same topic , resulting in item properties that are more similar within groups than across them\n","Alternatively , items may be derived from common templates .\n","Templates , often found in math courses , look like `` What is $x + y$ ? '' and a particular instantiation is generated by choosing values for $x$ and $y$ .\n","For example , the ASSISTments data set contains several { \\em problems } , many of which are with the same { \\em template}, many of which in turn assess a single {\\em skill} .\n","NOUN PHRASES:\n"," ['many situations', 'including', 'data sets', 'assessment items', 'have', 'structure', 'inform', 'predictions', 'student responses', 'example', 'groups', 'items', 'assess', 'same topic', 'resulting', 'item properties', 'groups', 'items', 'common templates', 'Templates', 'often found', 'math courses', 'look', 'x +', 'particular instantiation', 'choosing', 'values', 'example', 'ASSISTments data set', 'contains', 'turn assess', 'skill']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Table ~ \\ref{tab:bestlabels} enumerates the fields chosen in each data set to identify items and item groups ( for HIRT only ) that yielded the computationally tractable model with the best results .\n","Note that for the IRT - based models , our validation scheme ( Section ~ \\ref{sec:onlinevalidation} ) estimates a single number $\\theta_{st}$ for each student at each point $t > 1$ of the validation .\n","For computational reasons , it was not feasible to evaluate DKT on fine - grained labels in KDD and Knewton ( for ASSISTments , fine - grained labels were tractable but yielded worse results ) , whereas all IRT variants were able to process data at the finest levels .\n","NOUN PHRASES:\n"," ['tab', 'bestlabels', 'enumerates', 'fields', 'chosen', 'data set', 'identify', 'items', 'item groups', 'HIRT', 'yielded', 'tractable model', 'results', 'Note', 'IRT', 'based', 'models', 'validation scheme', 'sec', 'onlinevalidation', 'estimates', 'single number', 'st', 'student', 'point', 't >', 'validation', 'computational reasons', 'evaluate', 'DKT', 'grained', 'labels', 'KDD', 'Knewton', 'ASSISTments', 'grained', 'labels', 'results', 'IRT variants', 'process', 'data', 'levels']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 312, 'end': 327, 'text': 'a single number'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 367, 'end': 372, 'text': 'point'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We maximize this objective with respect to $\\{\\theta_s,\\beta_i,\\mu_j\\}$ .\n","NOUN PHRASES:\n"," ['maximize', 'objective', 'respect']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We can augment the IRT model to incorporate knowledge about item groups , resulting in a hierarchical IRT model ( HIRT ) .\n","Each item $i$ is associated with a group $j(i)$ whose difficulty is distributed normally around a per -group mean $\\mu_{j(i)}$ : $\\beta_i \\sim N(\\mu_{j(i)}, \\sigma^2)$ .\n","Each $\\mu_j$ is in turn distributed according to the hyperprior $\\mu_j \\sim N(0, \\tau^2)$ .\n","This reflects the belief that the difficulty of items in the same group should be similar .\n","The degenerate cases provide some intuition : the limit $\\sigma \\to 0$ is the same model as 1 PO IRT where we consider the items in the group to be the same item , and the limit $\\tau \\to 0$ is equivalent to a 1 PO IRT model with no groupings .\n","NOUN PHRASES:\n"," ['augment', 'IRT model', 'incorporate', 'knowledge', 'item groups', 'resulting', 'hierarchical IRT model', 'HIRT', 'item', 'group', 'j', 'i', 'difficulty', 'per -group', 'mean', 'j', 'i', 'j', 'i', 'turn', 'distributed according', 'reflects', 'belief', 'difficulty', 'items', 'same group', 'degenerate cases', 'provide', 'intuition', 'limit', 'same model', 'PO IRT', 'consider', 'items', 'group', 'same item', 'limit', 'PO IRT model', 'groupings']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 128, 'end': 132, 'text': 'item'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 156, 'end': 163, 'text': 'a group'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 177, 'end': 187, 'text': 'difficulty'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 219, 'end': 236, 'text': 'a per -group mean'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 342, 'end': 356, 'text': 'the hyperprior'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 523, 'end': 532, 'text': 'the limit'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 645, 'end': 654, 'text': 'the limit'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We use an evaluation method we call { \\em online response prediction} which matches that of \\cite{DKTNIPS} .\n","Students are first split into training and testing populations .\n","Each model is first trained on the training population and the model parameters that are not student - level ( item parameters for IRT - based models , weights for neural networks ) are frozen .\n","Then for each time $t > 1$ in each testing student 's history , we train the student - level parameters in the model on the first $t - 1$ interactions of the student history and allow it to compute the probability that the            'th response is correct .\n","This process mirrors the practical task that must be completed by an ITS .\n","NOUN PHRASES:\n"," ['use', 'evaluation method', 'call', 'matches', 'DKTNIPS', 'Students', 'first split', 'training', 'testing', 'populations', 'model', 'first trained', 'training population', 'model parameters', 'level', 'item parameters', 'IRT', 'based', 'models', 'weights', 'neural networks', 'time', 't >', 'testing', 'student', 'history', 'train', 'student', 'level parameters', 'model', 'interactions', 'student history', 'allow', 'compute', 'probability', \"'th response\", 'process', 'mirrors', 'practical task', 'ITS']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 383, 'end': 387, 'text': 'time'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 507, 'end': 519, 'text': 'interactions'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{center}  \\begin{table}  \\begin{tabular} {| l | l | l | l | }\n","\\hline\n","Factor & Performance Bound & Recalcitrance \\\\ \\hline\n","Accuracy & Perfect update & $\\infty$ at limit \\\\\n","Speed & Hardware limit & $\\infty$ at limit \\\\\n","Prior & True prior & $\\infty$ , unalterable \\\\ Data & \\emph{No bound} & Unknown \\\\ \\hline\n","\\end{tabular}  \\caption{Summary of the four factors of prediction performance of an artificially intelligent system, what bounds them, and their effective recalcitrance to self-improvement.}  \\end{table}  \\end{center}  \\begin{itemize}  \\item \\textbf{Computational accuracy} .\n","A system can improve its ability to compute the mathematical function of the Bayesian update .\n","Many widely used statistical inference algorithms use numerical approximation rather and so it is possible for a system to improve its algorithm 's faithfulness to the mathematical formula that defines its goal .\n","\\item \\textbf{Computational speed} .\n","There are faster and slower ways to compute the inference formula .\n","An intelligent system could come up with a way to make itself compute its answer faster .\n","This might be independent of the accuracy of its answer .\n","\\item \\textbf{Prior} .\n","The success of inference depends crucially on the prior probability assigned to hypotheses or models .\n","A prior is better when it assigns higher probability to the true process that generates observable data , or models that are ` close ' to that true process .\n","\\item \\textbf{Data} .\n","Assuming accurate Bayesian computation , performance at prediction will depend on the quality of the data used in the inference .\n","Note that \" better data \" is not necessarily the same as `` more data \" .\n","If the data that the system learns from is from a biased sample of the phenomenon in question , then a successful Bayesian update could make its predictions worse , not better .\n","Better data is data that is informative with respect to the true process that generated the data .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['center', '| l | l | l | l |', 'Performance Bound', 'Perfect update', 'Hardware limit', 'True', 'No bound', 'Summary', 'factors', 'prediction performance', 'intelligent system', 'bounds', 'effective recalcitrance', 'self-improvement', 'center', 'itemize', 'Computational accuracy', 'system', 'improve', 'ability', 'compute', 'mathematical function', 'Bayesian update', 'used statistical inference algorithms use numerical approximation', 'so', 'system', 'improve', 'algorithm', 'faithfulness', 'mathematical formula', 'defines', 'goal', 'Computational speed', 'ways', 'compute', 'inference formula', 'intelligent system', 'come', 'way', 'make', 'compute', 'answer faster', 'accuracy', 'answer', 'success', 'inference', 'depends', 'prior probability', 'assigned', 'hypotheses', 'models', 'assigns', 'probability', 'true process', 'generates', 'observable data', 'models', 'true process', 'Data', 'Assuming', 'accurate Bayesian computation', 'performance', 'prediction', 'depend', 'quality', 'data', 'used', 'inference', 'Note', 'data', 'data', 'data', 'system', 'learns', 'biased sample', 'phenomenon', 'question', 'successful Bayesian update', 'make', 'predictions', 'Better data', 'data', 'respect', 'true process', 'generated', 'data', 'itemize']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Here , $P(H|D)$ is the posterior probability of a hypothesis $H$ given observed data $D$ .\n","If one is following statistically optimal procedure , one can compute this value by taking the prior probability of the hypothesis $P(H)$ , multiplying it by the likelihood of the data given the hypothesis $P(D|H)$ , and then normalizing this result by dividing by the probability of the data over all models , $P(D) = \\sum_{i}P(D|H_i)P(H_i)$ .\n","NOUN PHRASES:\n"," ['P', 'H|D', 'posterior probability', 'hypothesis', 'H', 'given observed', 'data', 'D', 'optimal procedure', 'compute', 'value', 'taking', 'prior probability', 'hypothesis', 'P', 'H', 'multiplying', 'likelihood', 'data', 'given', 'hypothesis', 'P', 'D|H', 'then normalizing', 'result', 'dividing', 'probability', 'data', 'models', 'P', 'D', '=', 'P', 'D|H_i', 'P', 'H_i']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 19, 'end': 88, 'text': 'the posterior probability of a hypothesis $H$ given observed data $D$'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 48, 'end': 60, 'text': 'a hypothesis'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 71, 'end': 84, 'text': 'observed data'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 182, 'end': 221, 'text': 'the prior probability of the hypothesis'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 249, 'end': 296, 'text': 'the likelihood of the data given the hypothesis'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 356, 'end': 399, 'text': 'the probability of the data over all models'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To the extent that the Bayesian formulation is an accurate representation of the general problem of prediction , we can analyze its recalcitrance .\n","We start by enumerating the ways in which an agent might improve its performance on the prediction task , which is validly computing $P(H|D)$ in such a way that best approximates the truth .\n","NOUN PHRASES:\n"," ['extent', 'Bayesian formulation', 'accurate representation', 'general problem', 'prediction', 'analyze', 'recalcitrance', 'start', 'enumerating', 'ways', 'agent', 'improve', 'performance', 'prediction task', 'validly computing', 'P', 'H|D', 'way', 'approximates', 'truth']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Bostrom 's claim is that for instrumental reasons an intelligent system is likely to invest some portion of its intelligence back into improving its intelligence .\n","He introduces a linear model of self - improvement that we will adapt here .\n","By assumption we can model $O(I) = \\alpha I + \\beta$ for some parameters $\\alpha$ and $\\beta$ , where $\\alpha$ and $\\beta$ are positive and represent the contribution of optimization power by the system itself and external forces ( such as a team of researchers ) , respectively .\n","If recalcitrance is constant , e.g $R = k$ , then we can compute :\n","NOUN PHRASES:\n"," ['Bostrom', 'claim', 'instrumental reasons', 'intelligent system', 'invest', 'portion', 'intelligence', 'improving', 'intelligence', 'introduces', 'linear model', 'improvement', 'adapt', 'assumption', 'model', 'O', '=', '+', 'parameters', 'represent', 'contribution', 'optimization power', 'system', 'external forces', 'team', 'researchers', 'recalcitrance', 'R =', 'k', 'compute']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 303, 'end': 313, 'text': 'parameters'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 525, 'end': 538, 'text': 'recalcitrance'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 542, 'end': 550, 'text': 'constant'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $$\\frac{dI}{dt} = \\frac{\\alpha I + \\beta}{k}$$\n","NOUN PHRASES:\n"," ['dI', '+', 'k']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $$\\frac{dI}{dt} = \\frac{O(I)}{R}$$\n","NOUN PHRASES:\n"," ['dI', 'O', 'R']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition}\n","The rate of change in intelligence is equal to optimization power divided by recalcitrance .\n","$$\\frac{dI}{dt} = \\frac{O}{R}$$\n","Optimization power refers to the effort of improving the intelligence of the system .\n","Recalcitrance refers to the resistance of the system to being improved .\n","\\end{proposition}\n","NOUN PHRASES:\n"," ['proposition', 'rate', 'change', 'intelligence', 'optimization', 'power', 'divided', 'recalcitrance', 'dI', 'O', 'R', 'Optimization power refers', 'effort', 'improving', 'intelligence', 'system', 'Recalcitrance refers', 'resistance', 'system', 'proposition']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 20, 'end': 54, 'text': 'The rate of change in intelligence'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 67, 'end': 85, 'text': 'optimization power'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 97, 'end': 110, 'text': 'recalcitrance'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Under these conditions , $I$ will be exponentially increasing in time $t$ .\n","This is the \" intelligence explosion \" that gives Bostrom 's argument so much momentum .\n","The explosion only gets worse if recalcitrance is below a constant .\n","Implicitly , Bostrom appears committed to the following additional proposition :\n","NOUN PHRASES:\n"," ['conditions', 'exponentially increasing', 'time', 'intelligence explosion', 'gives', 'Bostrom', 'argument', 'much momentum', 'explosion', 'only gets', 'recalcitrance', 'Bostrom', 'appears committed', 'following additional proposition']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 65, 'end': 69, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $$\\frac{dI}{dt} = \\frac{\\alpha_o I + \\beta_o}{\\alpha_r I + \\beta_r}$$\n","NOUN PHRASES:\n"," ['dI', '+', '+']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $$P(H|D) = \\frac{P(D|H) P(H)}{P(D)}$$\n","NOUN PHRASES:\n"," ['P', 'H|D', '=', 'P', 'D|H', 'P', 'H', 'P', 'D']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Now there are four parameters instead of three .\n","Note this model is identical to the one above it when $\\alpha_r = 0$ .\n","Assuming all these parameters are positive , as $I$ increases the rate of intelligence growth approaches $\\alpha_o / \\alpha_r$ from below .\n","This is linear , not exponential , growth .\n","In this circumstance , there would be no intelligence explosion and therefore much less catastrophic AI risk .\n","NOUN PHRASES:\n"," ['parameters', 'Note', 'model', 'Assuming', 'parameters', 'increases', 'rate', 'intelligence growth', 'approaches', 'growth', 'circumstance', 'intelligence explosion', 'catastrophic AI risk']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," } \\ Return $newReward$ \\caption{ calcReward for S3}  \\label{calcReward_S3_algo}  \\end{small}  \\end{algorithm*}\n","NOUN PHRASES:\n"," ['Return', 'calcReward', 'S3', 'calcReward_S3_algo', 'algorithm*']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\If {            } { $newReward$ = $newReward$ + 2 * $reward$ } \\ Else { $newReward$ = $newReward$ - 2* $penalty$ }\n","NOUN PHRASES:\n"," ['penalty']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Map: Bridge-26x18}\n","This map size is 26 x 18 ( refer Figure \\ref{Map:Bridge-26x18} ) so total state space for this map is total combination of the $x-y$ co-ordinates of the player and enemy which is $26^{2}$ x $18^{2}$ .\n","This map has a marble wall in between which the tank cannot destroy by firing .\n","So this is an advantage for the tank to hide from opponents and attack when opponents enters their side .\n","NOUN PHRASES:\n"," ['Map', 'map size', 'refer', 'Map', 'total state space', 'map', 'total combination', 'co-ordinates', 'player', 'enemy', 'map', 'has', 'marble wall', 'tank', 'not destroy', 'firing', 'advantage', 'tank', 'hide', 'opponents', 'attack', 'opponents', 'enters', 'side']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 167, 'end': 203, 'text': 'co-ordinates of the player and enemy'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $Player_g$ = 0 , $Player_w$ = 0 , $Enemy_g$ = 0 , $Enemy_w$ = 0\n","$EnemyTroopLength$ = 0 , $PlayerTroopLength$ = 0 , $winner$ = null\n","$newReward$ = 0\n","$Player_g$ = \\texttt{player.getGold()} \\; $Player_w$ = \\texttt{player.getWood()} \\; $Enemy_g$ = \\texttt{enemy.getGold()} \\; $Enemy_w$ = \\texttt{enemy.getWood()} \\; $EnemyTroopLength$ = \\texttt{enemyTroop.size()} \\; $PlayerTroopLength$ = \\texttt{playerTroop.size()} \\; \\If {            == \\texttt{\"end\"} }\n","{ $winner$ = \\texttt{getWinner()}  \\If {            == \\texttt{\"player\"} } { $newReward$ = $newReward$ + $reward$ } \\ Else { $newReward$ = $newReward$ - $penalty$ } } \\ Else { \\If {            } { $newReward$ = $newReward$ + $reward$ } \\ Else { $newReward$ = $newReward$ - $penalty$ }\n","NOUN PHRASES:\n"," ['Player_g', '0', 'Player_w', '0', 'Enemy_g', '0', 'Enemy_w', '=', 'EnemyTroopLength', '0', 'PlayerTroopLength', '0', 'Player_g', 'player.getGold', 'Player_w', 'player.getWood', 'Enemy_g', 'enemy.getGold', 'Enemy_w', 'enemy.getWood', 'EnemyTroopLength', 'enemyTroop.size', 'PlayerTroopLength', 'playerTroop.size', 'end', 'getWinner', '==', 'player', 'penalty', 'penalty']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\If {            } { $newReward$ = $newReward$ + $reward$ } \\ Else { $newReward$ = $newReward$ - $penalty$ }\n","NOUN PHRASES:\n"," ['penalty']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Map: Bridges-34x24}\n","This is the most complex map ( refer Figure \\ref{Map:Bridge-Metal-34x24} ) among all on which we have performed our evaluation because of its size and the structure .\n","It is a 34 x 24 map and it has $34^{2}$ x $24^{2}$ search spaces .\n","It contains many brick wall and water bodies .\n","Brick wall can be destroyed by firing .\n","Its size and water bodies makes it a difficult and complex map .\n","NOUN PHRASES:\n"," ['Map', 'complex map', 'refer', 'Map', 'have performed', 'evaluation', 'size', 'structure', 'x', 'map', 'has', 'search spaces', 'contains', 'many brick wall', 'water bodies', 'Brick wall', 'firing', 'size', 'water bodies', 'makes', 'complex map']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\item \\emph{Temporal Difference~\\cite{RSSuttan} Method } : It is used to estimate the value functions after each step .\n","An estimate of the final reward is calculated at each state and the state - action value updated for every step of the way .\n","This reflects a more realistic assignment of rewards to actions compared to MC , which updates all actions at the end directly .\n","TD Learning is nothing but the combination of dynamic programming with the Monte Carlo method .\n","The formula related to TD learning is given as\n","\\begin{equation}\n","V(s_{t}) \\leftarrow V(s_{t})+\\alpha[r_{t+1}+\\gamma V(s_{t+1})-V(s_{t})]\n","\\end{equation}\n","\\hspace{6mm} where $r_{t+1}$ is the observed reward at time t+1 . \\\\\\\\ \\end{enumerate}\n","NOUN PHRASES:\n"," ['RSSuttan', 'Method', 'estimate', 'value functions', 'step', 'estimate', 'final reward', 'state', 'state', 'action value', 'updated', 'step', 'way', 'reflects', 'realistic assignment', 'rewards', 'actions', 'compared', 'MC', 'updates', 'actions', 'end', 'TD Learning', 'nothing', 'combination', 'dynamic programming', 'Monte Carlo method', 'formula', 'related', 'TD learning', 'equation', 'V', 't', 'V', 't', 't+1', 't+1', 's_', 't', ']', 'equation', 'r_', 't+1', 'observed reward', 'time t+1', 'enumerate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 184, 'end': 208, 'text': 'the state - action value'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 653, 'end': 684, 'text': 'the observed reward at time t+1'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 676, 'end': 680, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We have the following action selections policies which can be used to select desired action according to the behavior of that particular policy\n","\\begin{enumerate}\n","\\item \\textbf{            greedy} :\n","Most of the time the action with the highest estimated reward is chosen , called the greediest action .\n","But , with a small probability $\\epsilon$ , an action is selected at random to ensure optimal actions are discovered .\n","\\item \\textbf{            soft} : Very similar to $\\epsilon-$ greedy .\n","The best action is selected with probability $1 - \\epsilon$ and the rest of the time a random action is chosen uniformly .\n","\\item \\textbf{softmax} : One drawback of the above methods is that they select random actions with some probability .\n","So there is a case when the worst possible action is selected as the second best .\n","Softmax remedies this by assigning a rank or weight to each of the actions , according to their action - value estimate .\n","So the worst actions are unlikely to be chosen .\n","\\end{enumerate}\n","NOUN PHRASES:\n"," ['have', 'following action selections policies', 'select', 'desired action', 'according', 'behavior', 'enumerate', 'greedy', 'time', 'action', 'estimated reward', 'called', 'action', 'small probability', 'action', 'random', 'ensure', 'optimal actions', 'greedy', 'action', 'rest', 'time', 'random action', 'drawback', 'above methods', 'select', 'random actions', 'probability', 'case', 'possible action', 'Softmax', 'remedies', 'assigning', 'rank', 'weight', 'actions', 'according', 'action', 'value estimate', 'actions', 'enumerate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 313, 'end': 332, 'text': 'a small probability'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 525, 'end': 536, 'text': 'probability'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," There are two methods to define these value functions :\n","\\begin{enumerate}  \\item \\emph{Monte Carlo~\\cite{RSSuttan} Method } :\n","In this method the agent would need to wait until the final reward was received before any state - action pair values can be updated .\n","Once the final reward is received , the path taken to reach the final state would need to be traced back and each value updated .\n","\\begin{equation}\n","V(s_{t}) \\leftarrow V(s_{t})+\\alpha[R_{t}-V(s_{t})]\n","\\end{equation}\n","\\hspace{6mm} where $s_{t}$ is the state visited at time t , $R_{t}$ is the reward after time t and $\\alpha$ is a constant parameter .\n","NOUN PHRASES:\n"," ['methods', 'define', 'value functions', 'enumerate', 'RSSuttan', 'Method', 'method', 'agent', 'need', 'wait', 'final reward', 'state', 'action pair values', 'final reward', 'path', 'taken', 'reach', 'final state', 'need', 'value', 'updated', 'equation', 'V', 't', 'V', 't', 'R_', 't', '-V', 't', ']', 'equation', 's_', 't', 'state', 'visited', 'time t', 'R_', 't', 'reward', 'time t', 'constant parameter']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 375, 'end': 380, 'text': 'value'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 505, 'end': 532, 'text': 'the state visited at time t'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 526, 'end': 530, 'text': 'time'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 546, 'end': 569, 'text': 'the reward after time t'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 563, 'end': 567, 'text': 'time'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 586, 'end': 606, 'text': 'a constant parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Value Function}\n","\\textit{Value Functions} are used for mapping from states or from state - action pairs to real numbers , where the value of a state represents the long - term reward achieved starting from that state ( or state - action ) , and executing a particular policy .\n","It estimates how good a particular action will be in a given state , or what the return for that action is expected to be .\n","There are two type of value functions .\n","\\begin{enumerate} \\ item $V^{\\pi}(s)$ is the value of a state ' $s$ ' under policy $\\pi$ .\n","The expected return when starting in s and following $\\pi$ thereafter .\n","\\ item $Q^{\\pi}(s,a)$ is the value of taking action ' $a$ ' in state ' $s$ ' under a policy $\\pi$ .\n","The expected return when starting from s taking the action a and thereafter following policy $\\pi$ .\n","\\end{enumerate}\n","NOUN PHRASES:\n"," ['Value Function', 'Value Functions', 'mapping', 'states', 'state', 'action pairs', 'real numbers', 'value', 'state', 'represents', 'term reward', 'achieved starting', 'state', 'state', 'action', 'executing', 'particular policy', 'estimates', 'particular action', 'given', 'state', 'return', 'action', 'type', 'value functions', 'enumerate', 'V^', 'value', 'state', 'policy', 'expected return', 'starting', 's', 'following', 'Q^', 'value', 'taking', 'action', 'state', 'policy', 'expected return', 'starting', 's', 'taking', 'action', 'thereafter following', 'policy', 'enumerate']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 496, 'end': 543, 'text': \"the value of a state ' $s$ ' under policy $\\\\pi$\"}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 509, 'end': 516, 'text': 'a state'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 531, 'end': 537, 'text': 'policy'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 643, 'end': 715, 'text': \"the value of taking action ' $a$ ' in state ' $s$ ' under a policy $\\\\pi$\"}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 663, 'end': 669, 'text': 'action'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 681, 'end': 686, 'text': 'state'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 703, 'end': 709, 'text': 'policy'}, 'T22': {'eid': 'T22', 'label': 'PRIMARY', 'start': 804, 'end': 810, 'text': 'policy'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{enumerate} \\item\n","The Rlearner observes an input Game state .\n","\\item The Rlearner then creates a new policy based on the dimensions of the world .\n","\\item\n","Set the parameters ( $\\alpha,\\gamma,\\epsilon$ and number of episodes ) for the Rlearner and start learning .\n","\\item\n","Start running epochs .\n","You can optionally run each epoch individually .\n","\\end{enumerate}\n","One epoch contains following steps .\n","\\begin{enumerate} \\item\n","An action is determined by a decision making function ( e.g. $\\epsilon-$ greedy ) .\n","\\item The action is performed .\n","\\item\n","The Rlearner receives a scalar reward or reinforcement from the environment according to reward function .\n","\\item\n","Information about the reward given for that state / action pair is recorded .\n","\\item\n","Update the Q - values in Q - table According to Learning Algorithm ( e.g. Q - learning or SARSA ) .\n","\\end{enumerate}\n","NOUN PHRASES:\n"," ['enumerate', 'Rlearner', 'observes', 'input Game state', 'Rlearner', 'then creates', 'new policy', 'based', 'dimensions', 'world', 'Set', 'parameters', 'number', 'episodes', 'Rlearner', 'start', 'learning', 'running', 'optionally run', 'epoch', 'enumerate', 'epoch', 'contains following', 'steps', 'enumerate', 'action', 'decision making function', 'greedy', 'action', 'Rlearner', 'receives', 'scalar reward', 'reinforcement', 'environment', 'according', 'reward function', 'Information', 'reward', 'given', 'state', '/', 'action pair', 'Q', 'values', 'Q', 'table', 'According', 'Learning Algorithm', 'e.g', 'Q', 'learning', 'SARSA', 'enumerate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 162, 'end': 176, 'text': 'the parameters'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{itemize}\n","\\item \\textbf{Learning Rate            :}\n","The learning rate $0 < \\alpha < 1$ determines what fraction of the old estimate will be updated with the new estimate .\n","$\\alpha = 0$ will stop the RL - agent from learning anything while $\\alpha = 1$ will completely change the previous values with the new one .\n","\\item \\textbf{Discount Factor            :}\n","The discount factor $0 < \\gamma < 1$ determines what fraction of the upcoming reward values will be considered for evaluation .\n","For $\\gamma = 0$ all the upcoming rewards are ignored .\n","For $\\gamma = 1$ means the RL - Agent will consider the current and upcoming rewards as equal weightage .\n","\\item \\textbf{Exploration Rate            :}\n","In action selection policies there is one policy called as $\\epsilon$ greedy method which uses the exploration rate $0 < \\epsilon < 1$ for determining the ratio between the exploration and exploitation .\n","We are using $\\epsilon$ greedy method for selecting the best action and to maintain the balance between exploration and exploitation .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['itemize', 'Learning', 'Rate', 'learning', 'rate', '<', 'determines', 'fraction', 'old estimate', 'new estimate', 'stop', 'RL', 'agent', 'learning', 'anything', '=', 'completely change', 'previous values', 'new one', 'Discount Factor', 'discount factor', '<', 'determines', 'fraction', 'upcoming reward values', 'evaluation', '=', 'upcoming rewards', '=', 'means', 'RL', 'Agent', 'consider', 'upcoming rewards', 'equal weightage', 'Exploration Rate', 'action selection policies', 'policy', 'called', 'greedy method', 'uses', 'exploration rate', '<', 'determining', 'ratio', 'exploration', 'exploitation', 'greedy method', 'selecting', 'action', 'maintain', 'balance', 'exploration', 'exploitation', 'itemize']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 58, 'end': 75, 'text': 'The learning rate'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 364, 'end': 383, 'text': 'The discount factor'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 794, 'end': 814, 'text': 'the exploration rate'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm*} [ ht ]            \\\n","KwIn { $state$ :- contains positions of entities , reward , penalty \\\\\n","\\textbf{Global access to: }\n","$sensorsList$ :- contains sensors of game domain \\\\\n","$gameState$ :- contains state of game is running or not \\\\}\n","           \\\n","Blank Line\n","NOUN PHRASES:\n"," ['algorithm*', 'state', 'contains positions', 'entities', 'reward', 'Global access', 'contains', 'sensors', 'contains state', 'game']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 66, 'end': 106, 'text': 'positions of entities , reward , penalty'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 164, 'end': 186, 'text': 'sensors of game domain'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 214, 'end': 245, 'text': 'state of game is running or not'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ % % % % % % % % %%%% % % % %%\n","Algorithm % % % % %%%%%%%%%%%%%%%\n","\\begin{algorithm*} [ ht ]\n","           \\\n","KwIn\n","{ $state$ :- contains positions of entities , reward , penalty \\\\\n","$sensorsList$ :- contains sensors of game domain .\\\\\n","$gameState$ :- contains state of game is running or not \\\\}\n","           \\Blank\n","Line\n","$Player_x$ = null , $Player_y$ = null , $Enemy_x$ = null , $Enemy_y$ = null \\; $EnemyBase_x$ = null , $EnemyBase_y$ = null , $winner$ = null \\; $newReward$ = 0 , $distance$ = 0 \\; $Player_x$ =            \\; $Player_y$ =            \\;            =             \\;            =            \\;            =            \\;            =            \\; \\If {            == \\texttt{\"end\"} }\n","{ $winner$ = \\texttt{getWinner()} \\; \\If {            == \\texttt{\"player\"} }\n","{            = $newReward$ + $reward$ \\; }\n","\\ Else {            = $newReward$ -            \\; } } \\ Else { \\If {sensorList[EnemyInline]==2} {            = $newReward$ - $penalty$ \\; }\n","\\If {sensorList[EnemyBaseInline]==2}\n","{            = $\\sqrt[2]{(EnemyBase_x - Player_x)^{2} + (EnemyBase_y - Player_y)^{2}}$ \\; $newReward$ = $newReward$ + $2 \\times reward$ - $distance$ \\; }\n","$newReward$ = $newReward$ - $4 \\times distance$ \\; $distance$ =            \\; $newReward$ = $newReward$ + $4 \\times distance$ \\; } \\ Return            \\; \\caption{calcReward for BattleCity }  \\label{calcReward_algo}  \\end{small}  \\end{algorithm*}\n","NOUN PHRASES:\n"," ['% % % % % % % % % % % % % % % % % Algorithm % % % % % % % % % % % % % % % % % % %', 'state', 'contains positions', 'entities', 'reward', 'penalty', 'contains', 'sensors', 'contains state', 'game', 'Line', 'Player_x', '= null', 'Player_y', '= null', 'Enemy_x', '= null', 'Enemy_y', 'EnemyBase_x', '= null', 'EnemyBase_y', '= null', '0', 'Player_x', 'Player_y', 'end', 'getWinner', 'player', 'sensorList [ EnemyInline ] ==2', 'penalty', 'sensorList [ EnemyBaseInline ] ==2', ']', 'EnemyBase_x', 'Player_x', '^', '+', 'EnemyBase_y', 'Player_y', '^', 'distance', 'distance', 'calcReward', 'BattleCity', 'calcReward_algo', 'algorithm*']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 202, 'end': 224, 'text': 'sensors of game domain'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 253, 'end': 284, 'text': 'state of game is running or not'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 132, 'end': 172, 'text': 'positions of entities , reward , penalty'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We set the system to output a chord progression for every diversity parameter $\\alpha$ after every iteration .\n","In this paper , we present four results from each networks ( char - RNNs and word - RNNs ) , part of which are reported in the Table \\ref{table:chord_results} .\n","For simplicity , we added bar symbols $|$ and removed repeating chords in the same bar , e.g. \\textincon{            C:7 C:7 C:7 C:7            } reduced to \\textincon{            C:7            } and \\textincon{            C:7 C:7 E:min E:min            } reduced to \\textincon{            C:7 E:min            } .\n","NOUN PHRASES:\n"," ['set', 'system', 'output', 'chord progression', 'diversity', 'iteration', 'paper', 'present', 'results', 'networks', 'RNNs', 'word', 'RNNs', 'part', 'table', 'chord_results', 'simplicity', 'added', 'bar symbols', 'removed', 'repeating chords', 'same bar', 'e.g', 'C:7 C:7 C:7 C:7', 'reduced', 'C:7', 'C:7 C:7 E', 'min E', 'min', 'reduced', 'C:7 E', 'min']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 58, 'end': 77, 'text': 'diversity parameter'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 298, 'end': 309, 'text': 'bar symbols'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," There were $1,259$ unique chords in the training dataset .\n","In other words , the vocabulary size of word - RNN was $1,259$ .\n","However there were only $39$ characters in total , which significantly reduced the computation of char - RNN .\n","The total numbers of chords ( words ) and characters were $539,609$ and $3,531,261$ , respectively .\n","NOUN PHRASES:\n"," ['unique chords', 'training dataset', 'other words', 'vocabulary size', 'word', 'RNN', 'characters', 'total', 'significantly reduced', 'computation', 'char', 'RNN', 'total numbers', 'chords', 'words', 'characters']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table} [ b ! ] \\begin{center}  \\begin{tabular} { | c | c | c | }\n","\\hline % ROW 0 \\parbox{0.6cm} { \\vspace{.2\\baselineskip} $i$ \\vspace{.2\\baselineskip} } & \\parbox{0.6cm} { \\vspace{.2\\baselineskip} $\\alpha$ \\vspace{.2\\baselineskip} } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textbf{Chord progressions}  \\vspace{.2\\baselineskip} }\n","\\\\ \\hline % ROW 1 \\parbox{0.6cm} { 1 } & \\parbox{0.6cm} { 0.8 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{C:maj            G:7            ...            G:7            C:maj} } \\vspace{.2\\baselineskip} }\n","\\\\ \\hline % ROW 2 \\parbox{0.6cm} { 1 } & \\parbox{0.6cm} { 1.2 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{A\\#:maj            A:7            A:7 D:min7 D:min7 D:min7            D:hdim C:hdim            C:hdim            C:hdim G:9 G:9 D:min7            D:min7 D\\#:dim} } \\vspace{.2\\baselineskip} }\n","\\\\ \\hline % ROW 3 \\parbox{0.6cm} { 23 } & \\parbox{0.6cm} { 0.8 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{ \\small{C:7 F:maj            F:min            C:maj...C:maj            G:7            C:maj} } \\vspace{.1\\baselineskip} }\n","\\\\ \\hline % ROW 4 \\parbox{0.6cm} { 23 } & \\parbox{0.6cm} { 1.2 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{C:7/5 C:7            F:maj6 F\\#:dim            C:6(9)            C:6(9)            C:6(9) C:6(9) C:6(9) C:maj            E:7(b9)            A:min(6,9) A\\#:min(6,9)            A\\#:min(6,9)            ... D:min            G:9 C:maj            ... G:7 \\texttt{\\char`_END\\char`_}  \\texttt{\\char`_START\\char`_} C : maj } } \\vspace{.2\\baselineskip} }\n","\\\\ \\hline % LAST LINE\n","NOUN PHRASES:\n"," ['[ b', 'center', '| c |', 'c', '| c |', '% ROW', 'Chord progressions', '% ROW', 'C', 'maj G:7', 'G:7 C', 'maj', '% ROW', 'maj A:7 A:7 D', 'min7 D', 'min7 D', 'min7 D', 'hdim C', 'hdim C', 'hdim C', 'hdim G:9 G:9 D', 'min7 D', 'dim', '% ROW', 'C:7 F', 'maj F', 'min C', 'maj', 'C', 'maj G:7 C', 'maj', '% ROW', 'C:7/5 C:7 F', 'dim C:6', 'C:6', 'C:6', 'C:6', 'C:6', 'C', 'maj E:7', 'b9', 'A', 'min', 'min', 'min', 'D', 'min G:9 C', 'maj', 'C', 'maj', '% LAST LINE']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We used $2,486$ scores from The Realbooks and The Fakebooks as training data .\n","Every score file was parsed from \\textit{band-in-a-box} format to \\textit{.xlab} format .\n","Then they were transposed to the key of C while every blank quarter note was filled with its preceding chord as in the Table \\ref{table:chord_texts} .\n","Finally , we put \\texttt{\\char`_START\\char`_} and \\texttt{\\char`_END\\char`_} flags ( any distinctive words can be used as flags ) at the beginning and the end of each score .\n","NOUN PHRASES:\n"," ['used', 'scores', 'Realbooks', 'Fakebooks', 'training data', 'score file', 'format', '.xlab', 'format', 'key', 'C', 'blank quarter note', 'preceding', 'chord', 'table', 'chord_texts', 'put', 'flags', 'distinctive words', 'flags', 'beginning', 'end', 'score']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Although the key was transposed to C , only $867$ ( out of $2$ , $846$ ) scores end with \\textincon{C:maj} ( $30\\%$ ) , followed by $489$ \\textincon{G:7} ( 17 \\%),            \\textincon{C:maj6} ( $7\\%$ ) , $52$ \\textincon{F:maj} ( $2\\%$ ) , and $1$ , $252$ scores end with the others -- $237$ chords ( $46\\%$ ) .\n","This is because the The Real book chord progressions usually end with chords for a \\textit{turn-around} to make the progressions natural to repeat the score .\n","NOUN PHRASES:\n"," ['key', 'C', 'scores end', 'C', 'maj', '%', 'followed', 'G:7', 'C', 'maj6', '%', 'F', 'maj', '%', 'scores', 'end', 'others', 'chords', '%', 'Real book chord progressions', 'usually end', 'chords', 'make', 'progressions', 'repeat', 'score']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\hline % ROW 0 \\parbox{0.6cm} { 1 } & \\parbox{0.6cm} { 0.5 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{C:maj            G:7            ...             G:7            C:maj6} } \\vspace{.2\\baselineskip} } \\\\ \\hline % ROW 2 \\parbox{0.6cm} { 1 } & \\parbox{0.6cm} { 1.2 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{...C:maj \\texttt{\\char`_END\\char`_} ... \\texttt{\\char`_START\\char`_}  \\texttt{\\char`_START\\char`_} C \\#:maj A\\#:min A:sus4/5 C:maj/3            F:min7 A:min7 D:min7 D:min7... \\texttt{\\char`_START\\char`_} } } \\vspace{.2\\baselineskip} } \\\\ \\hline % ROW 3 \\parbox{0.6cm} { 8 } & \\parbox{0.6cm} { 0.5 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{ \\small{C:maj A:min            D:min7 G:7(b9)             C:maj            A:min7             D:9            D:9            D:7            D:min7            G:7            C:maj            C:7             F:maj              F:min             C:maj } } \\vspace{.2\\baselineskip} }\n","\\\\ \\hline % ROW 4 \\parbox{0.6cm} { 8 } & \\parbox{0.6cm} { 1.2 } & \\parbox{10.0cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{C:Maj            G:min7            F:maj            D:min7 D:min7 D:min7/4 G:sus4(b7)            G:min9 G:min9 G:min9 F\\#:(1,3,b5,b7,9,13)            C:6(9) G:sus4(b7,9) ... ... C:min \\texttt{\\char`_END\\char`_}  \\texttt{\\char`_START\\char`_} C : maj } } \\vspace{.2\\baselineskip} }\n","\\\\ \\hline % LAST LINE \\end{tabular}\n","NOUN PHRASES:\n"," ['% ROW', 'C', 'maj G:7', 'G:7 C', 'maj6', '% ROW', 'C', 'min', 'sus4/5 C', 'maj/3 F', 'min7', 'min7 D', 'min7 D', 'min7', '% ROW', 'C', 'maj', 'min D', 'min7 G:7', 'b9', 'C', 'maj', 'min7 D:9 D:9 D:7 D', 'min7 G:7 C', 'maj C:7 F', 'maj F', 'min C', 'maj', '% ROW', 'C', 'Maj G', 'min7 F', 'maj D', 'min7 D', 'min7 D', 'min7/4 G', 'sus4', 'b7', 'G', 'min9 G', 'min9 G', 'b5', 'b7,9,13', 'C:6', 'G', 'sus4', 'b7,9', 'C', 'C', 'maj']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," There can be theoretically $2^9=512$ words , but there are supposedly much fewer words because the combinations of drum components that are played simultaneously are limited .\n","The size of the word vocabulary in the training file is $119$ and the file consists of $2$ , $141$ , $692$ words in total .\n","NOUN PHRASES:\n"," ['words', 'words', 'combinations', 'drum components', 'size', 'word vocabulary', 'training file', 'file', 'consists', 'words']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In the experiment , we first loaded $60$ midi files of drum tracks of \\textit{Metallica} and quantised them .\n","Then they were encoded into the above described binary representation .\n","We also added a flag \\texttt{\\char`_BAR\\char`_} as an annotation of the bar segments in order to check if the networks learns the local structure .\n","NOUN PHRASES:\n"," ['experiment', 'first loaded', 'midi files', 'drum tracks', 'Metallica', 'quantised', 'above described binary representation', 'also added', 'annotation', 'bar segments', 'order', 'check', 'networks', 'learns', 'local structure']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," ( b ) \\caption{Chord progressions generated by char-RNN (a) and word-RNN (b). Bar symbols (            ) are inserted for readability and repeated chords in each bar are omitted. }  \\label{table:chord_results}  \\end{center}  \\end{table}\n","NOUN PHRASES:\n"," ['b', 'Chord progressions', 'generated', 'char-RNN', 'b', 'Bar symbols', 'readability', 'repeated', 'chords', 'bar', 'table', 'chord_results', 'center', 'table']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For efficient representation and learning , only nine components were allowed ; kick , snare , open hi - hats , closed hi - hats , three tom - toms , crash cymbal , and ride cymbal .\n","\\footnote{Some of the components in the texts also represent other similar components, e.g. a closed hi-hats in the texts can mean either closed hi-hats or pedalled hi-hats in the original midi file.}\n","We limited the number of events in a bar to $16$ by quantising the drum track by $16$ th - note .\n","NOUN PHRASES:\n"," ['efficient representation', 'learning', 'components', 'kick', 'snare', 'hats', 'closed', 'hats', 'toms', 'crash cymbal', 'ride', 'cymbal', 'components', 'texts', 'also represent', 'other similar components', 'e.g', 'closed hi-hats', 'texts', 'mean', 'closed hi-hats', 'pedalled', 'hi-hats', 'original midi file', 'limited', 'number', 'events', 'bar', 'quantising', 'drum track', 'note']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{center}  \\begin{figure*} [ t ] \\ centering\n","\\includegraphics[width=0.9\\textwidth]{drum_result.jpg}  \\caption{A score of a generated drum track. }  \\label{fig:drum_result}  \\end{figure*}  \\end{center}  \\vspace{-0.7cm}\n","Char - RNNs turned out to fail to learn the drum tracks and output arbitrary            's and            's without any structures ( the results have no spaces or \\texttt{\\char`_BAR\\char`_} flags ) .\n","The length of network may be too short to learn the long - term relationship between characters .\n","In char - RNNs , representing a single bar requires $16$ events $\\times            160$ time steps .\n","Encoding music sequences with only two characters - $0$ and $1$ ( + space to for segmentation ) - is an extreme approach for char - RNNs .\n","In this paper , we therefore only report the result of word - RNNs .\n","NOUN PHRASES:\n"," ['center', 'figure*', 'centering', 'drum_result.jpg', 'A score', 'generated drum track', 'fig', 'drum_result', 'figure*', 'center', '-0.7cm', 'Char', 'RNNs', 'turned', 'fail', 'learn', 'drum tracks', 'output', 'arbitrary', 'structures', 'results', 'have', 'spaces', 'flags', 'length', 'network', 'learn', 'term relationship', 'characters', 'RNNs', 'representing', 'single bar', 'requires', 'events', 'time steps', 'Encoding', 'music sequences', 'characters', '+ space', 'segmentation', 'extreme approach', 'RNNs', 'paper', 'therefore', 'only report', 'result', 'word', 'RNNs']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," { \\em Recurrent Neural Networks} ({\\em RNNs} ) allow for incorporating long term dependency in the model .\n","{ \\em Jordan net} \\cite{jordan1986attractor} , a simple version of RNNs , is used in \\cite{lewis1989algorithms} to generate chord sequences .\n","In \\cite{mozer1994neural} , melodies were generated by a system named \\textit{CONCERT} , which is trained on sets of $10$ Bach pieces to generate melodies by note - wise prediction .\n","One ability \\textit{CONCERT} lacks is to learn the global structure ; this may be due to the difficulty of training an RNNs .\n","Theoretically , it can remember infinitely long sequences , although in practice it is limited by the \\textit{vanishing gradient} problem \\cite{hochreiter2001gradient} .\n","During the training of back - propagation through time , the gradient is extremely diminished by multiplications of sigmoid operations .\n","NOUN PHRASES:\n"," ['Recurrent Neural Networks', 'RNNs', 'allow', 'incorporating', 'long term dependency', 'model', 'jordan1986attractor', 'simple version', 'RNNs', 'lewis1989algorithms', 'generate', 'chord sequences', 'melodies', 'system', 'named', 'CONCERT', 'sets', 'Bach pieces', 'generate', 'melodies', 'note', 'wise prediction', 'CONCERT', 'learn', 'global structure', 'difficulty', 'training', 'RNNs', 'remember', 'long sequences', 'practice', 'vanishing', 'gradient', 'hochreiter2001gradient', 'training', 'propagation', 'time', 'gradient', 'extremely diminished', 'multiplications', 'sigmoid operations']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," { \\em LSTM (Long Short-Term Memory)} units solved this vanishing gradient problem \\cite{hochreiter2001gradient} .\n","LSTM allows the gradient to be flowed by a separate path with not multiplication but \\textit{addition} operations .\n","LSTM is adopted in \\cite{eck2002first} to learn $12$ - bar Blues chords progressions and melodies .\n","\\cite{lambert2015perceiving} focuses on the generation of percussive tracks using LSTM network .\n","The network in \\cite{lambert2015perceiving} directly analyses audio content of drum tracks and learns features using LSTM .\n","NOUN PHRASES:\n"," ['LSTM', 'Long Short-Term Memory', 'units', 'solved', 'vanishing', 'hochreiter2001gradient', 'LSTM', 'allows', 'gradient', 'separate path', 'multiplication', 'addition', 'operations', 'LSTM', 'learn', 'bar Blues chords progressions', 'melodies', 'lambert2015perceiving', 'focuses', 'generation', 'percussive tracks', 'using', 'LSTM network', 'network', 'lambert2015perceiving', 'directly analyses', 'audio content', 'drum tracks', 'learns', 'features', 'using', 'LSTM']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure \\ref{fig:drum_result} shows one example of our results - a part of the generated track with $\\alpha=1.0$ after $25$ iterations .\n","\\footnote{The score uses the percussion clef where            refers to hi-hats, notes on middle and bottom lines refers to snare and kick, respectively.} It consists of reasonable rock drum patterns - $8$ - beat hi - hats , combinations of kick and snare , and occasional crash cymbals and tom - toms .\n","Although there are occasional kick / snare / tom - toms notes on back beats ( of sixteen notes ) , hi - hats remain consistent , playing on $4$ - beat and $8$ - beat pattern , which is very common for instance in drum tracks of Metallica .\n","\\footnote{\\url{https://soundcloud.com/kchoi-research/00-24-100-bonus-for-score} ,\\\\\n","The score in the figure starts from 34 - second . }\n","NOUN PHRASES:\n"," ['Figure', 'fig', 'drum_result', 'shows', 'example', 'results', 'part', 'generated track', 'iterations', 'score', 'uses', 'percussion clef', 'refers', 'hi-hats', 'notes', 'middle', 'bottom lines refers', 'snare', 'kick', 'consists', 'reasonable rock drum', 'patterns', 'beat', 'hats', 'combinations', 'kick', 'snare', 'occasional crash cymbals', 'toms', 'occasional kick / snare / tom', 'toms notes', 'back beats', 'sixteen notes', 'hats', 'remain', 'playing', 'beat', 'beat pattern', 'instance', 'drum tracks', 'Metallica', 'https', '//soundcloud.com/kchoi-research/00-24-100-bonus-for-score', 'score', 'figure', 'starts', 'second']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," { \\em Hidden-Markov models (HMMs)} are one of the most popular methods to model and predict sequences. HMMs are based on the assumption of            (Markov assumption) given the sequence of the hidden states which determine the visible states. Choral harmonisation is generated after learning chorales by Bach using a HMM in \\cite{allan2005harmonising} , where $229$ and $153$ chorales are used for training and testing , respectively .\n","In \\cite{simon2008mysong} , chord progressions are generated to accompany a melody to help non-musicians to create music using a HMM .\n","The training set of the HMM consists of $298$ lead sheets including pop , rock , R \\&B, jazz, and country music. In the prediction, the system generates chords using a            chord transition probability matrix. In practice, HMMs had been the most suitable for time-series modelling given the data, computing power, and feasible optimisation strategies. One of the drawbacks of HMMs, however, is the inefficiency of {\\em 1-of-K} scheme of its hidden states .\n","The memory of HMM is limited to $log_2(N)$ bits when there is $N$ hidden states , which requires to learn $N^2$ parameters for the transition matrix .\n","NOUN PHRASES:\n"," ['Hidden-Markov models', 'HMMs', 'popular methods', 'model', 'predict sequences', 'HMMs', 'assumption', 'Markov assumption', 'given', 'sequence', 'hidden states', 'determine', 'visible states', 'Choral harmonisation', 'learning', 'chorales', 'Bach', 'using', 'HMM', 'allan2005harmonising', 'chorales', 'training', 'testing', 'chord progressions', 'accompany', 'melody', 'help', 'non-musicians', 'create', 'music', 'using', 'HMM', 'training set', 'HMM', 'consists', 'lead sheets', 'including', 'pop', 'rock', 'B', 'jazz', 'country music', 'prediction', 'system', 'generates', 'chords', 'using', 'chord transition probability matrix', 'practice', 'HMMs', 'time-series', 'modelling given', 'data', 'computing', 'power', 'feasible optimisation strategies', 'drawbacks', 'HMMs', 'inefficiency', 'scheme', 'hidden states', 'memory', 'HMM', 'log_2', 'N', 'bits', 'N', 'hidden states', 'requires', 'learn', 'N^2', 'parameters', 'transition matrix']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 1103, 'end': 1116, 'text': 'hidden states'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 1149, 'end': 1159, 'text': 'parameters'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 1080, 'end': 1084, 'text': 'bits'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Music can be represented as a sequence of events and thus it can be modelled as conditional probabilities between musical events .\n","For example , in harmonic tracks , some chords are more likely to occur than others given the previous chords , while the whole chord progressions often depend on the global key of the music .\n","In many automatic composition systems , these relationships are simplified by assuming that the probability of the current state $p(n)$ only depends on the probabilities of the states in the past $p(n-k)...p(n-1)$ .\n","A sequence of musical events - notes , chords , rhythm patterns - is generated by predicting the following event given a seed sequence .\n","NOUN PHRASES:\n"," ['sequence', 'events', 'conditional probabilities', 'musical events', 'example', 'harmonic tracks', 'chords', 'occur', 'others', 'given', 'previous chords', 'whole chord progressions', 'often depend', 'global key', 'music', 'many automatic composition systems', 'relationships', 'assuming', 'probability', 'current state', 'p', 'only depends', 'probabilities', 'states', 'p', 'p', 'sequence', 'musical events', 'notes', 'chords', 'rhythm patterns', 'predicting', 'following event', 'given', 'seed sequence']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 416, 'end': 452, 'text': 'the probability of the current state'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 476, 'end': 519, 'text': 'the probabilities of the states in the past'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Second , fewer number of characters means fewer number of states , which results in reducing the computational cost .\n","From a linguistics point of view , sequence learning methods such as HMMs and RNNs used to model each { \\em word}(e.g. chord) as a {\\em state} as it is natural to find the relationships between words .\n","One drawback of word - based learning is the large number of states ( or the size of vocabulary ) ; in natural language processing tasks , the vocabulary size easily exceeds few thousands to even few millions .\n","In the proposed method the size of the chord vocabulary is $1$ , $259$ .\n","With character - based prediction , this decreases to $39$ .\n","NOUN PHRASES:\n"," ['number', 'characters', 'means', 'number', 'states', 'results', 'reducing', 'computational cost', 'linguistics point', 'view', 'sequence', 'learning', 'methods', 'HMMs', 'RNNs', 'used', 'model', 'chord', 'find', 'relationships', 'words', 'drawback', 'word', 'based', 'learning', 'large number', 'states', 'size', 'natural language processing tasks', 'vocabulary size', 'easily exceeds', 'few thousands', 'few millions', 'proposed', 'method', 'size', 'chord vocabulary', 'character', 'based', 'prediction', 'decreases']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We use two LSTM layers , each of which consists of 512 hidden units .\n","Dropout of $0.2$ is added after every LSTM layers \\cite{zaremba2014recurrent} .\n","NOUN PHRASES:\n"," ['use', 'LSTM layers', 'consists', 'hidden units', 'Dropout', 'zaremba2014recurrent']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table} [ t ] \\begin{center}  \\begin{tabular} { | c | c | } \\ hline \\parbox{3.1cm} { \\vspace{.2\\baselineskip}  \\textincon{F:9 \\hphantom{ MMM M MM} $|$ \\\\ D:min7 \\hphantom{MM} G : 9 \\hphantom{M} $|$ \\\\ C:maj \\hphantom{MaM} F : 9 \\hphantom{al} $|$ \\\\ C:maj \\hphantom{MMMMl M} $|$ } \\vspace{.05\\baselineskip} } & \\parbox{4.5cm} { \\vspace{.2\\baselineskip}  \\textincon{\\small{F:9 F:9 F:9 F:9 D:min7 D:min7 G:9 G:9 C:maj C:maj F:9 F:9 C:maj C:maj C:maj C:maj} } \\vspace{.05\\baselineskip} } \\ \\ \\ hline \\end{tabular}  \\vspace{.05\\baselineskip}\n","NOUN PHRASES:\n"," ['center', '| c |', 'c', 'MMM M MM', 'MM', 'G', 'M', 'MaM', 'F', 'al', 'MMMMl M', 'F:9 F:9 F:9 F:9 D', 'min7 D', 'min7 G:9 G:9 C', 'maj C', 'maj F:9 F:9 C', 'maj C', 'maj C', 'maj C', 'maj']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The prediction is stochastic .\n","In each prediction for time index $n$ , the network outputs the probabilities of every states .\n","To make the system \\textit{tunable} , We employ a diversity parameter $\\alpha$ in the prediction stage ( see Eqn. \\ref{eq:diversity} ) , which suppresses ( $\\alpha<1$ ) or encourages ( $\\alpha>1$ ) the diversity of prediction by re-weighting the probabilities .\n","In detail , the probabilities of $i$ - th state , $p_i$ , are re-weighted as $ \\hat{p_i} = \\exp{(\\log(p_i)/\\alpha)}$ .\n","Then , one of the states is selected by sampling a state according to the re-weighted probabilities .\n","NOUN PHRASES:\n"," ['prediction', 'prediction', 'time index', 'network', 'outputs', 'probabilities', 'states', 'make', 'employ', 'diversity', 'prediction stage', 'see', 'Eqn', 'eq', 'diversity', 'suppresses', '<', 'encourages', '>', 'diversity', 'prediction', 're-weighting', 'probabilities', 'detail', 'probabilities', 'th state', 'p_i', 'p_i', 'states', 'sampling', 'state', 'according', 're-weighted probabilities']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 175, 'end': 196, 'text': 'a diversity parameter'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 401, 'end': 436, 'text': 'the probabilities of $i$ - th state'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Controlling $\\alpha$ provides a way to tune the technical virtuosity of the track .\n","Since large $\\alpha$ increases the probabilities of occasional events , large $\\alpha$ ( = $1.5$ ) results in tracks with many fill - ins with tom - toms and a crash cymbal .\n","On the other hands , when $\\alpha < 1$ , the track almost never contains anything but kick , snare , and hi - hats .\n","As a result , it is possible to use a combination of small and large $\\alpha$ in a drum track generator that is guided by user , who specifies where to add fill - ins .\n","NOUN PHRASES:\n"," ['Controlling', 'provides', 'way', 'tune', 'technical virtuosity', 'track', 'increases', 'probabilities', 'occasional events', '=', 'results', 'tracks', 'ins', 'toms', 'crash cymbal', 'other hands', 'track', 'almost never contains', 'anything', 'kick', 'snare', 'hats', 'result', 'use', 'combination', 'drum track generator', 'user', 'specifies', 'add', 'ins']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\textbf{Third} , DCG provides more coherent playlists compared to the cosine distance and $l_2$ - norm .\n","This phenomenon is found not only in the sequences corresponding to Figure \\ref{fig:transitions} but also in other playlists .\n","This can be explained as follows .\n","In each track , there are consistently strong , consistently weak , and fluctuating features .\n","This pattern , especially the consistencies , can be easily learned by the RNN and the consistent features are maintained in the predictions .\n","Finally , DCG prioritises the features that are large in predictions , resulting in successfully finding a track with those consistently large features .\n","This improves the coherence of the resulting playlists .\n","NOUN PHRASES:\n"," ['DCG', 'provides', 'coherent playlists', 'compared', 'cosine distance', 'norm', 'phenomenon', 'sequences', 'corresponding', 'fig', 'transitions', 'other playlists', 'follows', 'track', 'fluctuating', 'features', 'pattern', 'consistencies', 'easily learned', 'RNN', 'consistent features', 'predictions', 'DCG', 'prioritises', 'features', 'predictions', 'resulting', 'successfully finding', 'track', 'large features', 'improves', 'coherence', 'resulting playlists']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure \\ref{figure:block} illustrates the procedure of dataset construction , as well as the training and prediction stages of the proposed algorithm .\n","First , the training tracks are segmented and $x_i$ , the features for each segments are extracted ( Fig. \\ref{figure:block} a ) .\n","Then an RNN of length $N$ ( $N$ =3 in the figure ) is trained to learn the transitions of the sequence of feature vectors ( Fig. \\ref{figure:block} b ) .\n","When a seed track is provided , the features of the last $N$ segments are extracted and fed into the trained RNN to predict the feature vector $x_{pred}$ ( Fig. \\ref{figure:block} c ) .\n","The algorithm selects a track with a start segment that is most similar to $x_{pred}$ .\n","NOUN PHRASES:\n"," ['Figure', 'figure', 'block', 'illustrates', 'procedure', 'dataset construction', 'training', 'prediction stages', 'proposed', 'algorithm', 'First', 'training tracks', 'features', 'segments', 'Fig', 'figure', 'block', 'RNN', 'N', 'N', '=3', 'figure', 'learn', 'transitions', 'sequence', 'feature vectors', 'Fig', 'figure', 'block', 'b', 'seed track', 'features', 'N', 'segments', 'fed', 'trained RNN', 'predict', 'feature vector', 'pred', 'Fig', 'figure', 'block', 'c', 'algorithm selects', 'track', 'start segment', 'pred']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 206, 'end': 236, 'text': 'the features for each segments'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 298, 'end': 304, 'text': 'length'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 561, 'end': 579, 'text': 'the feature vector'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 498, 'end': 506, 'text': 'segments'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," the feature of the following track ( $x_{pred}$ ) that maintains consistency and fluctuations , i.e. , a certain variation over the features .\n","To this end , a 2 - layer RNN with 512 hidden units is employed .\n","LSTM units \\cite{gers2000learning} are used as they show state - of - the - art performance among RNN variants for several sequence modelling tasks \\cite{greff2015lstm} .\n","NOUN PHRASES:\n"," ['feature', 'following track', 'pred', 'maintains', 'consistency', 'fluctuations', 'i.e', 'certain variation', 'features', 'end', 'layer RNN', 'hidden units', 'gers2000learning', 'show', 'state', 'art performance', 'RNN variants', 'several sequence', 'modelling', 'tasks', 'greff2015lstm']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 34, 'text': 'the feature of the following track'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure} [ t ! ] \\begin{center}  \\centerline{\\includegraphics[width=1.0\\columnwidth]{icml_2016_rnn_playlist_diagram.pdf} } \\caption{A block diagram of the proposed algorithm, (a,b) training of RNN and (c) prediction of a feature vector,            .}  \\label{figure:block}  \\end{center}  \\end{figure}\n","NOUN PHRASES:\n"," ['figure', '[', 'center', '[', 'icml_2016_rnn_playlist_diagram.pdf', 'A block diagram', 'proposed', 'algorithm', 'b', 'training', 'RNN', 'c', 'prediction', 'feature vector', 'figure', 'block', 'center', 'figure']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Recommendation approaches using collaborative filtering are prone to overlook niche or new items , although the popularity bias of known items can be compensated for .\n","This is called the \\textit{cold-start problem}  \\cite{ricci2011introduction} .\n","Content - based approaches which are designed to solve the cold - start problem can suffer from lack of diversity when recommended items are selected simply by similarity .\n","This is often called top - $N$ recommendation .\n","It is well known that \\textit{unexpectedness} , \\textit{surprise} or \\textit{serendipity} play an important role in the music recommendation and discovery \\cite{choi2015understanding} .\n","Compared to other strategies , focusing on transitions can naturally provide these qualities .\n","NOUN PHRASES:\n"," ['Recommendation approaches', 'using', 'collaborative filtering', 'prone', 'overlook', 'niche', 'new items', 'popularity bias', 'known items', 'cold-start problem', 'ricci2011introduction', 'Content', 'based', 'approaches', 'solve', 'start problem', 'suffer', 'lack', 'diversity', 'recommended', 'items', 'similarity', 'often called', 'N', 'recommendation', 'well known', 'surprise', 'serendipity', 'play', 'important role', 'music recommendation', 'discovery', 'choi2015understanding', 'Compared', 'other strategies', 'focusing', 'transitions', 'naturally provide', 'qualities']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," DCG is a weighted version of \\textit{Cumulative Gain} ( CG ) .\n","CG is designed to measure ranking quality of a retrieved list and DCG weights on the top - $N$ relevant items by \\textit{discounting} lower relevant items .\n","Applying this measure was motivated by the type of feature extraction algorithm we use .\n","The extracted feature is a vector of probabilities of each tag and tags with large probabilities should be weighted more than the others to facilitate maintaining the consistency of the generated playlists .\n","Because DCG weights high - ranking elements more , it can theoretically work better .\n","NOUN PHRASES:\n"," ['DCG', 'weighted version', 'Cumulative Gain', 'CG', 'CG', 'measure ranking', 'quality', 'retrieved list', 'DCG weights', 'N', 'relevant items', 'discounting', 'relevant items', 'Applying', 'measure', 'type', 'feature extraction', 'use', 'extracted feature', 'vector', 'probabilities', 'tag', 'tags', 'large probabilities', 'others', 'facilitate maintaining', 'consistency', 'generated playlists', 'DCG weights', 'ranking', 'elements', 'theoretically work']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The segmentation is performed using \\cite{foote2000automatic} , which is implemented in \\cite{nietomsaf} .\n","As mentioned in Section \\ref{sec:prop} , an automatic tagging algorithm was used as a feature extractor \\cite{choi2016automatic} .\n","An RNN with a length of 50 is trained .\n","We compared DCG with the cosine distance and the $l_2$ norm for computing similarity .\n","Audio processing and RNN are implemented using \\textit{librosa}  \\cite{mcfee2015librosa} , \\textit{Keras}  \\cite{chollet2015} , and \\textit{Theano}  \\cite{team2016theano} .\n","NOUN PHRASES:\n"," ['segmentation', 'mentioned', 'sec', 'prop', 'automatic tagging algorithm', 'RNN', 'length', 'compared', 'DCG', 'cosine distance', 'norm', 'computing', 'similarity', 'Audio processing', 'RNN', 'librosa', 'mcfee2015librosa', 'Keras', 'chollet2015', 'Theano', 'team2016theano']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 333, 'end': 337, 'text': 'norm'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This may first be due to the short lengths of segments in the training data .\n","The majority of training tracks have fewer segments ( 90 $\\%$ of the training tracks has less than 17 segments ) , therefore the long - term dependency may not be learnt and the prediction may be dominated by short - term features .\n","NOUN PHRASES:\n"," ['short lengths', 'segments', 'training data', 'majority', 'training', 'tracks', 'have', 'segments', '%', 'training tracks', 'has', 'segments', 'term dependency', 'prediction', 'term features']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure} [ t ! ] \\begin{center}  \\centerline{\\includegraphics[width=\\columnwidth]{fig_transitions_seed_2000.pdf} } \\ caption { Transitions of feature vectors by cosine distance , $l_2$ , and DCG on left , centre , and right , respectively .\n","Y - axis is time ( top to bottom ) and x - axis refers to the 50 feature dimensions .\n","The first track ( topmost feature vectors ) is the seed . }\n","\\label{fig:transitions}  \\end{center}  \\end{figure}\n","NOUN PHRASES:\n"," ['figure', '[', 'center', '[', 'fig_transitions_seed_2000.pdf', 'Transitions', 'feature vectors', 'cosine distance', 'DCG', 'left', 'centre', 'Y', 'axis', 'time', 'bottom', 'axis refers', 'feature dimensions', 'first track', 'topmost feature vectors', 'seed', 'fig', 'transitions', 'center', 'figure']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Since thresholding $L_t$ involves thresholding a tail probability , there is an inherent upper limit on the number of alerts .\n","With $\\epsilon$ very close to $0$ it would be unlikely to get alerts with probability much higher than $\\epsilon$ .\n","This also imposes an upper bound on the number of false positives .\n","Under the assumption that anomalies themselves are also extremely rare , the hope is that the ratio of true positives to false positives is always in a healthy range ( see Results below ) .\n","NOUN PHRASES:\n"," ['thresholding', 'L_t', 'involves thresholding', 'tail probability', 'inherent upper limit', 'number', 'alerts', 'get', 'alerts', 'probability', 'also imposes', 'upper bound', 'number', 'false positives', 'assumption', 'anomalies', 'hope', 'ratio', 'true positives', 'false positives', 'healthy range', 'see', 'Results']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 6, 'end': 18, 'text': 'thresholding'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," It is important to note that this test is applied to the distribution of anomaly scores , not to the distribution of underlying metric values $\\boldsymbol{x}_{t}$ .\n","As such , it is a measure of how well the model is able to predict , relative to the recent history .\n","In clean predictable scenarios $L_t$ behaves similarly to $s_t$ .\n","In these cases the distribution of scores will have very small variance and will be centered near $0$ .\n","Any spike in $s_t$ will similarly lead to a corresponding spike in $L_t$ .\n","However in scenarios with some inherent randomness or noise , the variance will be wider and the mean further from $0$ .\n","A single spike in $s_t$ will not lead to a significant increase in $L_t$ but a series of spikes will .\n","Interestingly , a scenario that goes from wildly random to completely predictable will also trigger an anomaly .\n","NOUN PHRASES:\n"," ['note', 'test', 'distribution', 'anomaly scores', 'distribution', 'underlying', 'metric values', 'x', '_', 't', 'measure', 'model', 'predict', 'relative', 'recent history', 'clean predictable scenarios', 'L_t', 'behaves', 'cases', 'distribution', 'scores', 'have', 'small variance', 'spike', 'similarly lead', 'corresponding spike', 'L_t', 'scenarios', 'inherent randomness', 'noise', 'variance', 'mean', 'single spike', 'not lead', 'significant increase', 'L_t', 'series', 'spikes', 'scenario', 'goes', 'also trigger', 'anomaly']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We assume the inputs representing the system are broken up into $M$ distinct models .\n","Let $\\boldsymbol{x}_t^m$ be the input at time $t$ to the            'th model , and $s_t^m$ be the raw anomaly scores associated with each model .\n","We wish to compute a global metric indicating the overall likelihood of an anomaly in the system ( see Figure ~ \\ref{block-diagram-multi} ) .\n","NOUN PHRASES:\n"," ['assume', 'inputs', 'representing', 'system', 'M', 'distinct models', 'Let', 'x', '_t^m', 'input', 'time', \"'th model\", 'raw anomaly scores', 'associated', 'model', 'wish', 'compute', 'global metric indicating', 'overall likelihood', 'anomaly', 'system', 'see']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 68, 'end': 83, 'text': 'distinct models'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 114, 'end': 163, 'text': \"the input at time $t$ to the 9999999996 'th model\"}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 127, 'end': 131, 'text': 'time'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 181, 'end': 230, 'text': 'the raw anomaly scores associated with each model'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We then compute a recent short term average of anomaly scores , and apply a threshold to the Gaussian tail probability ( Q - function , \\cite{Karagiannidis2007} ) to decide whether or not to declare an anomaly \\\n","footnote\n","{ The Guassian tail probability is the probability that a Gaussian variable will obtain a value larger than $x$ standard deviations above the mean . } .\n","We define the { \\em anomaly likelihood } as the complement of the tail probability :\n","NOUN PHRASES:\n"," ['then compute', 'recent short term average', 'anomaly scores', 'apply', 'threshold', 'Gaussian tail probability', 'Q', 'function', 'Karagiannidis2007', 'decide', 'declare', 'Guassian tail probability', 'probability', 'Gaussian variable', 'obtain', 'value', 'standard deviations', 'mean', 'define', 'complement', 'tail probability']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 333, 'end': 367, 'text': 'standard deviations above the mean'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $W'$\n","here is a window for a short term moving average , where $W' \\ll W$ .\n","We threshold $L_t$ and report an anomaly if it is very close to $1$ :\n","NOUN PHRASES:\n"," ['W', 'window', 'short term', 'moving', 'average', 'W', 'threshold', 'L_t', 'report', 'anomaly']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 13, 'end': 53, 'text': 'a window for a short term moving average'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," One possible approach is to estimate the joint distribution $P(s_t^0,\\ldots,s_t^{M-1})$ and apply a threshold to the tail probability .\n","It can be challenging to model the joint distribution , particularly in a streaming context .\n","If we further assume the models are independent , we could simplify and instead estimate :\n","NOUN PHRASES:\n"," ['possible approach', 'estimate', 'joint distribution', 'P', 's_t^0', 'M-1', 'apply', 'threshold', 'tail probability', 'model', 'joint distribution', 'streaming context', 'further assume', 'models', 'simplify', 'estimate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 37, 'end': 59, 'text': 'the joint distribution'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As before , we detect an anomaly if the combined anomaly likelihood is greater than a threshold $L_t \\geq 1 - \\epsilon$ .\n","Eq. ~ \\ref{eq:multi2} represents a principled yet pragmatic approach for detecting anomalies in complex real time streaming applications \\\n","footnote\n","{ Due to numerical precision issues with products of probabilities , in our implementation we follow common practice and use summation of log probabilities . } .\n","As before , $L_t$ is an indirect measure , computed on top of the raw anomaly scores from each model .\n","It reflects the underlying predictability of the models at a particular point in time and does not directly model the sensor measurements themselves .\n","NOUN PHRASES:\n"," ['detect', 'anomaly', 'combined anomaly likelihood', 'Eq', '~', 'eq', 'multi2', 'represents', 'pragmatic approach', 'detecting', 'anomalies', 'complex real time', 'streaming', 'numerical precision issues', 'products', 'probabilities', 'implementation', 'follow', 'common practice', 'use', 'summation', 'log probabilities', 'L_t', 'indirect measure', 'computed', 'top', 'raw anomaly scores', 'model', 'reflects', 'underlying predictability', 'models', 'particular point', 'time', 'does', 'not directly model', 'sensor', 'measurements']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 36, 'end': 67, 'text': 'the combined anomaly likelihood'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 84, 'end': 95, 'text': 'a threshold'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We apply this convolution to each individual model to obtain a final anomaly likelihood score \\\n","footnote { Since this is a real - time system with no look - ahead , the kernel only applies to time $\\leq t$ .\n","As such we use a half - normal distribution as the kernel , hence the additional factor of 2 . } :\n","NOUN PHRASES:\n"," ['apply', 'convolution', 'individual model', 'obtain', 'time system', 'look', 'kernel', 'only applies', 'time', 'use', 'normal distribution', 'kernel', 'additional factor']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Ideally we would be able to estimate a joint distribution of anomaly scores that go back in time , i.e. $P(s_{t-j}^0,s_{t-j}^1,\\ldots,s_t^{M-2},s_t^{M-1})$ .\n","In theory this would capture all the dependencies , but this is even harder to estimate than the earlier joint probability .\n","Alternatively , in situations where the system 's topology is relatively clear and under your control , it is possible to create an explicit graph of dependencies , monitor expected behavior between pairs of nodes , and detect anomalies with respect to those expectations .\n","This technique has been shown to enable very precise determination of anomalies in websites where specific calls between services are monitored \\cite{Kim2013} .\n","However in most applications this technique is also impractical .\n","It may be difficult to model the various dependencies and it is often infeasible to instrument arbitrary systems to create this graph .\n","NOUN PHRASES:\n"," ['estimate', 'joint distribution', 'anomaly scores', 'go', 'time', 'i.e', 'P', '^0', '^1', 'M-1', 'theory', 'capture', 'dependencies', 'estimate', 'joint probability', 'situations', 'system', 'topology', 'control', 'create', 'explicit graph', 'dependencies', 'monitor', 'expected', 'behavior', 'pairs', 'nodes', 'detect anomalies', 'respect', 'expectations', 'technique', 'enable', 'precise determination', 'anomalies', 'websites', 'specific calls', 'services', 'Kim2013', 'applications', 'technique', 'model', 'various dependencies', 'instrument arbitrary systems', 'create', 'graph']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 37, 'end': 96, 'text': 'a joint distribution of anomaly scores that go back in time'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 61, 'end': 75, 'text': 'anomaly scores'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We would like to have a system that is fast to compute , makes relatively few assumptions , and is adaptive .\n","We propose a simple general purpose mechanism for handling multiple models by modifying Eq. ~ ( \\ref{eq:multi1} ) to incorporate a smooth temporal window .\n","A windowing mechanism allows the system to incorporate spikes in likelihood that are close in time but not exactly coincident .\n","Let $G$ be a Gaussian convolution kernel :\n","NOUN PHRASES:\n"," ['like', 'have', 'system', 'compute', 'makes', 'few assumptions', 'propose', 'simple general purpose mechanism', 'handling', 'multiple models', 'modifying', 'Eq', '~', 'eq', 'multi1', 'incorporate', 'smooth temporal window', 'windowing mechanism', 'allows', 'system', 'incorporate', 'spikes', 'likelihood', 'time', 'Let', 'G', 'Gaussian convolution kernel']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 405, 'end': 434, 'text': 'a Gaussian convolution kernel'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," There are three parameters in the single model scenario : $W$ , $W'$ , and $\\epsilon$ .\n","$W$ is the duration for computing the distribution of anomaly scores .\n","The system performance is not sensitive to $W$ as long as it is large enough to compute a reliable distribution .\n","The number $W'$ controls the short term average of anomaly scores .\n","In all our experiments below , we use a generous value of $W=8000$ and $W'=10$ .\n","NOUN PHRASES:\n"," ['parameters', 'single model scenario', 'W', 'W', 'W', 'duration', 'computing', 'distribution', 'anomaly scores', 'system performance', 'W', 'compute', 'reliable distribution', 'number', 'W', 'controls', 'short term average', 'anomaly scores', 'experiments', 'use', 'generous value', 'W=8000', \"W'=10\"]\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 16, 'end': 26, 'text': 'parameters'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 95, 'end': 156, 'text': 'the duration for computing the distribution of anomaly scores'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," HTM is a learning algorithm that appears to match the above constraints .\n","HTM networks are continuously learning and model the spatiotemporal characteristics of their inputs .\n","HTMs have been shown to work well for prediction tasks \\cite{Cui2015, Padilla2013} but HTM networks do not directly output an anomaly score .\n","In order to perform anomaly detection we utilize two different internal representations available in the HTM .\n","Given an input $\\boldsymbol{x}_t$ , the vector $\\mathbf{a}(\\boldsymbol{x}_t)$ is a sparse binary code representing the current input .\n","We also utilize an internal state vector $\\boldsymbol{\\pi}(\\boldsymbol{x}_t)$ which represents a { \\em prediction } for $\\mathbf{a}(\\boldsymbol{x}_{t+1})$ , i.e. a prediction of the next input $\\boldsymbol{x}_{t+1}$ .\n","The prediction vector incorporates inferred information about current sequences .\n","In particular , a given input will lead to different predictions depending on the current detected sequence and the current inferred position of the input within the sequence .\n","The quality of the prediction is dependent on how well the HTM is modeling the current data stream .\n","See \\cite{Hawkins2016} for a more detailed explanation of these representations .\n","NOUN PHRASES:\n"," ['HTM', 'learning', 'algorithm', 'appears', 'match', 'above constraints', 'HTM networks', 'continuously learning', 'model', 'spatiotemporal characteristics', 'inputs', 'HTMs', 'work', 'Cui2015', 'Padilla2013', 'HTM networks', 'do', 'output', 'anomaly score', 'order', 'perform', 'anomaly detection', 'utilize', 'different internal representations', 'HTM', 'Given', 'x', '_t', 'vector', '_t', 'sparse binary code', 'representing', 'current input', 'also utilize', 'internal state vector', '_t', 'represents', '_', 't+1', 'i.e', 'prediction', 'next input', 'x', '_', 't+1', 'prediction vector', 'incorporates inferred', 'information', 'current sequences', 'given', 'input', 'lead', 'different predictions', 'depending', 'current detected sequence', 'current inferred position', 'input', 'sequence', 'quality', 'prediction', 'HTM', 'current data stream', 'See', 'Hawkins2016', 'detailed explanation', 'representations']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 435, 'end': 443, 'text': 'an input'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 465, 'end': 475, 'text': 'the vector'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 510, 'end': 561, 'text': 'a sparse binary code representing the current input'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 580, 'end': 604, 'text': 'an internal state vector'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 742, 'end': 756, 'text': 'the next input'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $\\mathbf{a}(\\boldsymbol{x}_t)$ and $\\boldsymbol{\\pi}(\\boldsymbol{x}_t)$ are recomputed at every iteration but do not directly represent anomalies .\n","In order to create a robust anomaly detection system we introduce two additional steps .\n","We first compute a { \\em raw anomaly score } from the two sparse vectors .\n","We then compute an {\\em anomaly likelihood } value which is thresholded to determine whether the system is anomolous .\n","Figure ~ \\ref{block-diagram} shows a block diagram of our algorithm .\n","These two steps are detailed below .\n","We then describe how to robustly handle a larger system consisting of multiple distinct models .\n","NOUN PHRASES:\n"," ['_t', '_t', 'iteration', 'do', 'represent anomalies', 'order', 'create', 'robust anomaly detection system', 'introduce', 'additional steps', 'first compute', 'sparse vectors', 'then compute', 'value', 'determine', 'system', 'Figure', '~', 'shows', 'block diagram', 'algorithm', 'steps', 'then describe', 'robustly handle', 'system', 'consisting', 'multiple distinct models']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The parameter $\\epsilon$ is perhaps the most important parameter .\n","It controls how often anomalies are reported , and the balance between false positives and false negatives .\n","In practice we have found that $\\epsilon = 10^{-5}$ works well across a large range of domains .\n","Intuitively , this should represent one false positive about once every 10,000 records .\n","NOUN PHRASES:\n"," ['important parameter', 'controls', 'anomalies', 'balance', 'false positives', 'false negatives', 'practice', 'have found', '-5', 'works', 'large range', 'domains', 'represent', 'records']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 13, 'text': 'The parameter'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 36, 'end': 64, 'text': 'the most important parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Consider for example , the task of monitoring a datacenter .\n","Components of $\\boldsymbol{x}_t$ might include CPU usage for various servers , bandwidth measurements , latencies of servicing requests , etc .\n","At each point in time $t$ we would like to determine whether the behavior of the system up to that point is unusual .\n","One of the key challenges is that the determination must be made in real - time , i.e. before time $t+1$ and without any look ahead .\n","In practical applications , the statistics of the system can change dynamically .\n","For example , in a production datacenter , software upgrades might be installed at any time that alter the behavior of the system ( Figure ~ \\ref{continuous-learning} ) .\n","Any retraining of a model must be done on - line , again before time $t+1$ .\n","Finally , the individual measurements are not independent and contain significant temporal patterns that can be exploited .\n","NOUN PHRASES:\n"," ['Consider', 'example', 'task', 'monitoring', 'datacenter', 'Components', 'x', '_t', 'include', 'CPU usage', 'various servers', 'bandwidth measurements', 'latencies', 'servicing', 'requests', 'point', 'time', 'like', 'determine', 'behavior', 'system', 'point', 'key challenges', 'determination', 'time', 'i.e', 'time', 'look', 'practical applications', 'statistics', 'system', 'change', 'example', 'production datacenter', 'software upgrades', 'time', 'alter', 'behavior', 'system', 'retraining', 'model', 'line', 'time', 'Finally', 'individual measurements', 'contain significant temporal patterns']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 222, 'end': 226, 'text': 'time'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 417, 'end': 421, 'text': 'time'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 774, 'end': 778, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Typical streaming applications involve analyzing a continuous stream of data occurring in real - time .\n","Such applications contain some unique challenges .\n","We formalize this as follows .\n","Let the vector $\\boldsymbol{x}_t$ represent the state of a real - time system at time $t$ .\n","The model receives a continuous stream of inputs :\n","NOUN PHRASES:\n"," ['Typical streaming applications', 'involve analyzing', 'continuous stream', 'data', 'occurring', 'time', 'Such applications', 'contain', 'unique challenges', 'formalize', 'follows', 'Let', 'vector', 'x', '_t', 'represent', 'state', 'time system', 'time', 'model', 'receives', 'continuous stream', 'inputs']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 194, 'end': 200, 'text': 'vector'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 230, 'end': 275, 'text': 'the state of a real - time system at time $t$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 267, 'end': 271, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure ~ \\ref{multi_plot} shows a more detailed example demonstrating the value of combining multiple metrics .\n","The two solid lines indicate the Q - values in Eq ~ ( \\ref{eq:lt} ) for two separate metrics related to Comcast shares .\n","The lower the curve , the more likely it is that the underlying metric is anomalous .\n","When looking at single models independently , an anomaly would only be detected if one of these values drops below $\\epsilon = 10^{-5}$ .\n","In this case no anomaly would be detected since neither one dips below that value .\n","However , as shown by the drops , both metrics are behaving unusually .\n","The red dashed line shows the result of detecting an anomaly based on our combined metric , Eq. ~ ( \\ref{eq:multi2} ) .\n","Because the blue curve is so close to $10^{-5}$ , incorporating both metrics correctly flags an anomaly as soon as the black curve dips below $10^{-1}$ .\n","As can be appreciated from the chart , it is difficult to line up two metrics precisely , hence the value of the smooth temporal window .\n","NOUN PHRASES:\n"," ['Figure', '~', 'multi_plot', 'shows', 'detailed example', 'demonstrating', 'value', 'combining', 'multiple metrics', 'solid lines', 'indicate', 'Q', 'values', 'Eq ~', 'eq', 'lt', 'separate metrics', 'related', 'Comcast shares', 'curve', 'looking', 'single models', 'anomaly', 'values', 'drops', '=', '-5', 'case', 'anomaly', 'dips', 'value', 'shown', 'drops', 'metrics', 'red dashed line', 'shows', 'result', 'detecting', 'anomaly', 'based', 'combined', 'Eq', '~', 'eq', 'multi2', 'blue curve', '-5', 'incorporating', 'metrics', 'correctly flags', 'anomaly', 'black curve dips', '-1', 'chart', 'line', 'metrics', 'value', 'smooth temporal window']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We compute a raw anomaly score that measures the deviation between the model 's predicted input and the actual input .\n","It is computed from the intersection between the predicted and actual sparse vectors .\n","At time $t$ the raw anomaly score , $s_t$ , is given as :\n","NOUN PHRASES:\n"," ['compute', 'raw anomaly score', 'measures', 'deviation', 'model', 'predicted', 'input', 'actual input', 'intersection', 'actual sparse vectors', 'time', 'raw anomaly score']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 209, 'end': 213, 'text': 'time'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 218, 'end': 239, 'text': 'the raw anomaly score'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The raw anomaly score will be $0$ if the current input is perfectly predicted , $1$ if it is completely unpredicted , or somewhere in between depending on the similarity between the input and the prediction .\n","NOUN PHRASES:\n"," ['raw anomaly score', 'current input', 'perfectly predicted', 'depending', 'similarity', 'input', 'prediction']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We have integrated our anomaly detection algorithm into a live real - time product .\n","The application continuously monitors a large set of financial and social media metrics for a range of securities and alerts users in real time when significant anomalies occur .\n","Figure ~ \\ref{htmstocks} shows two screenshots from our application that demonstrate the value of real - time anomaly detection to end users .\n","In this example , an anomaly in the volume of Twitter activity ( related to a decrease in dividends ) preceded a sharp drop in stock price .\n","The Twitter anomaly occurred well before market opening .\n","The underlying data streams are extremely noisy and many of the important anomalies are temporal in nature .\n","Figure ~ \\ref{fb_volume} shows raw data extracted from our application demonstrating one such anomaly .\n","The figure plots stock trading volume for Facebook for a few hours .\n","Each point represents average trading volume for a period of five minutes .\n","It is normal to see a single spike in trading volume but it is extremely unusual to see two consecutive spikes .\n","Two consecutive spikes therefore represents a temporal anomaly in this stream .\n","The red dashed line indicates the point at which our algorithm detected the anomaly , i.e. the point in time where $L_t \\geq 1 - 10^{-5}$ .\n","NOUN PHRASES:\n"," ['have integrated', 'anomaly detection algorithm', 'time product', 'application', 'continuously monitors', 'large set', 'social media metrics', 'range', 'securities', 'alerts', 'users', 'real time', 'significant anomalies', 'occur', 'htmstocks', 'shows', 'screenshots', 'application', 'demonstrate', 'value', 'time anomaly detection', 'end', 'users', 'example', 'anomaly', 'volume', 'Twitter activity', 'related', 'decrease', 'dividends', 'preceded', 'sharp drop', 'stock price', 'Twitter anomaly', 'occurred', 'market opening', 'underlying data streams', 'important anomalies', 'nature', 'Figure', '~', 'fb_volume', 'shows', 'raw data', 'extracted', 'application', 'demonstrating', 'such anomaly', 'figure plots stock trading volume', 'Facebook', 'few hours', 'point', 'represents', 'average trading volume', 'period', 'minutes', 'see', 'single spike', 'trading volume', 'see', 'consecutive spikes', 'consecutive spikes', 'therefore represents', 'temporal anomaly', 'stream', 'red dashed line', 'indicates', 'point', 'algorithm', 'detected', 'anomaly', 'i.e', 'point', 'time', '-5']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To handle this class of scenarios , we introduce a second step .\n","Rather than thresholding the raw score directly , we model the distribution of anomaly scores and use this distribution to check for the likelihood that the current state is anomalous .\n","The anomaly likelihood is thus a metric defining how anomalous the current state is based on the prediction history of the HTM model .\n","To compute the anomaly likelihood we maintain a window of the last $W$ raw anomaly scores .\n","We model the distribution as a rolling normal distribution where the sample mean and variance are continuously updated from previous anomaly scores as follows :\n","NOUN PHRASES:\n"," ['handle', 'class', 'scenarios', 'introduce', 'second step', 'thresholding', 'raw score', 'model', 'distribution', 'anomaly scores', 'use', 'distribution', 'check', 'likelihood', 'current state', 'anomaly likelihood', 'metric defining', 'current state', 'prediction history', 'HTM model', 'compute', 'anomaly likelihood', 'maintain', 'window', 'W', 'raw anomaly scores', 'model', 'distribution', 'rolling', 'normal distribution', 'sample mean', 'variance', 'continuously updated', 'previous anomaly scores', 'follows']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 457, 'end': 475, 'text': 'raw anomaly scores'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An interesting aspect of this score is that branching sequences are handled correctly .\n","In HTMs , multiple predictions are represented in $\\boldsymbol{\\pi}(\\boldsymbol{x}_t)$ as a binary union of each individual prediction .\n","Similar to Bloom filters , as long as the vectors are sufficiently sparse and of sufficient dimensionality , a moderate number of predictions can be represented simultaneously with exponentially small chance of error \\cite{Bloom1970,Ahmad2016} .\n","The anomaly score handles branching sequences gracefully in the following sense .\n","If two completely different inputs are both possible and predicted , receiving either input will lead to a $0$ anomaly score .\n","Any other input will generate a positive anomaly score .\n","NOUN PHRASES:\n"," ['interesting aspect', 'score', 'branching', 'sequences', 'HTMs', 'multiple predictions', '_t', 'binary union', 'individual prediction', 'Bloom filters', 'vectors', 'sufficient dimensionality', 'moderate number', 'predictions', 'small chance', 'Bloom1970', 'Ahmad2016', 'anomaly score handles', 'branching', 'sequences', 'following sense', 'different inputs', 'predicted', 'receiving', 'input', 'lead', 'anomaly score', 'other input', 'generate', 'positive anomaly score']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The muliple model scenario introduces one additional parameter , the window width $\\sigma$ .\n","This is somewhat domain dependent but due to the soft Gaussian convolution , the system is not extremely sensitive to this setting .\n","In all our experiments below , $\\sigma=6$ .\n","NOUN PHRASES:\n"," ['muliple model scenario', 'introduces', 'additional parameter', 'window', 'width', 'soft Gaussian convolution', 'system', 'setting', 'experiments']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 65, 'end': 81, 'text': 'the window width'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\noindent\n","Since Linked Data does not allow negation , it is not possible to prevent an entity from becoming a member of a concept .\n","It is however possible to predict the effect that adding an entity to a dataset would have on the semantic richness of a concept .\n","We define $E$ as the set of patterns that are matched by entity $\\epsilon$ .\n","The degree of typicality of an entity with respect to a concept $\\alpha$ can then be defined as $\\delta^{\\epsilon} = |E \\cap Y |/|Y|$ where $Y$ is the set of frequent patterns of $\\alpha$ .\n","Given two entities $\\epsilon$ and $\\epsilon^{\\prime}$ , we can say that $\\epsilon$ is more typical than $\\epsilon^{\\prime}$ if $\\delta^{\\epsilon} > \\delta^{\\epsilon^{\\prime}}$ .\n","We define an entity to be \\emph{atypical} , meaning that including it into a concept would result in a decrease of its semantic richness , if $\\delta < 0.5$ .\n","If $\\delta \\geq 0.5$ we say that the entity is \\emph{typical} , meaning that its inclusion in the concept would maintain or increase its semantic richness .\n","\\par\n","This typicality measure is scalable and easily verifiable because it can be computed automatically over a subset of the data .\n","In fact , any agent can calculate whether an entity is a typical or an atypical member of a concept .\n","In a closed domain , this measure could be used to determine which entities should be considered as members of a concept , and which should not .\n","In Linked Data , however , it is not possible , nor desirable , to prevent entities from becoming members of a concept .\n","It is however possible to add more knowledge about the typical entities to preserve their semantic richness .\n","A possible approach to do this is to create a sub-concept which groups together the typical entities .\n","\\par For example, the particular way in which the concept \\emph{foaf:Person} is used in DBpedia results in the property that the majority of those entities have an associated Wikipedia page .\n","If we merge these entities with another source about persons which are not related with a Wikipedia page , then this property will no longer hold , thus reducing the semantic richness of the concept .\n","In order to preserve this property , we can create a new sub-concept of \\emph{foaf:Person} that captures the particular interpretation of this concept adopted by DBpedia .\n","NOUN PHRASES:\n"," ['Linked', 'Data', 'does', 'not allow', 'negation', 'prevent', 'entity', 'becoming', 'member', 'concept', 'predict', 'effect', 'adding', 'entity', 'dataset', 'have', 'semantic richness', 'concept', 'define', 'E', 'set', 'patterns', 'entity', 'degree', 'typicality', 'entity', 'respect', 'Y', 'set', 'frequent patterns', 'Given', 'entities', 'say', 'define', 'entity', 'meaning', 'including', 'concept', 'result', 'decrease', 'semantic richness', '<', 'say', 'entity', 'meaning', 'inclusion', 'concept', 'maintain', 'increase', 'semantic richness', 'typicality measure', 'subset', 'data', 'fact', 'agent', 'calculate', 'entity', 'atypical member', 'concept', 'closed domain', 'measure', 'determine', 'entities', 'members', 'concept', 'Linked Data', 'prevent', 'entities', 'becoming', 'members', 'concept', 'add', 'knowledge', 'typical entities', 'preserve', 'semantic richness', 'possible approach', 'do', 'create', 'groups', 'typical entities', 'example', 'particular way', 'foaf', 'Person', 'DBpedia results', 'property', 'majority', 'entities', 'have', 'associated', 'Wikipedia page', 'merge', 'entities', 'source', 'persons', 'not related', 'Wikipedia page', 'property', 'no longer hold', 'thus reducing', 'semantic richness', 'concept', 'order', 'preserve', 'property', 'create', 'new sub-concept', 'foaf', 'Person', 'captures', 'particular interpretation', 'concept', 'adopted', 'DBpedia']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 291, 'end': 348, 'text': 'the set of patterns that are matched by entity $\\\\epsilon$'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 331, 'end': 337, 'text': 'entity'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 405, 'end': 414, 'text': 'a concept'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 351, 'end': 423, 'text': 'The degree of typicality of an entity with respect to a concept $\\\\alpha$'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 498, 'end': 538, 'text': 'the set of frequent patterns of $\\\\alpha$'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 551, 'end': 559, 'text': 'entities'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\label{semantic_measure}\n","\\noindent Given a concept            , such as \\texttt{foaf:Person} , we define a measure of its semantic richness $G$ which quantifies the amount of information conveyed by the concept .\n","Unlike IC - based measures \\cite{harispe2013semanticMeasures} , our measure is not proportional to the probability of an entity being a member of the concept .\n","This probability cannot be estimated reliably for Linked Data due to its open nature and partial observability .\n","When evaluating the semantic richness of a concept we only assume the availability of a representative subset of entities of that concept obtained from a finite number of sources .\n","Taxonomical information about the concept is not required .\n","           } here represented as a SPARQL \\footnote{\\url{http://www.w3.org/TR/rdf-sparql-query/} } graph pattern using the FOAF            } ontology .\n","We define the probability of an entity of type $\\alpha$ from dataset $S$ matching pattern $i$ as $p(i,\\alpha,S)$ .\n","The average number of patterns that we can observe in the entities of type $\\alpha$ is the expected value $\\mu$ of a Poisson binomial distribution of the probabilities $p$ .\n","We only need to compute this value over the finite set of patterns $I$ that have been observed among the entities of the dataset , namely when $p(i,\\alpha,S) > 0$ .\n","The expected value of this distribution can then be computed as follows :\n","\\begin{equation}\n","\\label{expected_value_of_facts}\n","\\mu = \\sum_{i \\in I}p(i,\\alpha,S)\n","\\end{equation}\n","Equation \\ref{expected_value_of_facts} gives us a measure of how many patterns , in average , we can observe from the entities of type $\\alpha$ in dataset $S$ .\n","This measure , however , does not tell us how well we can characterize a concept with a set of common features .\n","In fact , a high value of $\\mu$ could be equally a result of a large number of infrequent patterns or of a small number of frequent patterns .\n","However , frequent patterns are more useful than infrequent ones for the purpose of characterising a concept , for example to learn schema information using Inductive Learning .\n","Taking this into consideration , we define our measure of semantic richness with respect to a set of characteristic patterns $Y \\subseteq I$ that define which patterns we can expect entities of type $\\alpha$ to have .\n","\\par If a pattern            belongs to            , then whenever we observe an entity            of type            we can infer that it would match pattern            . We base our measure of semantic richness on the correctness of such inferences. If an entity            really matches pattern            then we count it as a correct inference, we count it as incorrect otherwise. For example, we might consider including the pattern ``has a dedicated Wikipedia page\" in the set of characteristic patterns of concept \\emph{foaf:Person} .\n","Whenever we observe an entity of type \\emph{foaf:Person} which has a dedicated Wikipedia page we say that inferring this pattern is correct ; we say it is incorrect otherwise .\n","\\\n","par\n","Our measure of semantic richness $G(\\alpha,S)$ of a concept $\\alpha$ with respect to a dataset $S$ and a set of characteristic patterns $Y$ is the difference between the expected number of correct inferences and the expected number of incorrect ones that we can make about an entity :\n","\\begin{equation}\n","\\label{goal_pre}\n","\\begin{array}{l}\n","G(\\alpha,S) = \\sum_{i \\in Y}p(i,\\alpha,S) - \\sum_{i \\in Y}(1-p(i,\\alpha,S)) \\\\[2mm]\n","G(\\alpha,S) = \\sum_{i \\in Y}2p(i,\\alpha,S) - 1\n","\\end{array}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['semantic_measure', 'concept', 'foaf', 'Person', 'define', 'measure', 'G', 'quantifies', 'amount', 'information', 'conveyed', 'concept', 'IC', 'based', 'harispe2013semanticMeasures', 'measure', 'probability', 'entity', 'member', 'concept', 'probability', 'Linked', 'Data', 'open nature', 'partial observability', 'evaluating', 'semantic richness', 'concept', 'only assume', 'availability', 'representative subset', 'entities', 'concept', 'obtained', 'finite number', 'sources', 'Taxonomical information', 'concept', 'not required', 'here represented', 'http', 'graph pattern', 'using', 'FOAF', 'ontology', 'define', 'probability', 'entity', 'dataset', 'S', 'matching', 'p', 'S', 'average number', 'patterns', 'observe', 'entities', 'expected value', 'Poisson binomial distribution', 'probabilities', 'only need', 'compute', 'value', 'finite set', 'entities', 'dataset', 'p', 'S', '>', 'expected value', 'distribution', 'follows', 'equation', 'expected_value_of_facts', 'p', 'S', 'equation', 'expected_value_of_facts', 'gives', 'measure', 'many patterns', 'observe', 'entities', 'S', 'measure', 'does', 'not tell', 'characterize', 'concept', 'set', 'common features', 'fact', 'high value', 'result', 'large number', 'infrequent patterns', 'small number', 'frequent patterns', 'frequent patterns', 'infrequent ones', 'purpose', 'characterising', 'concept', 'example', 'learn', 'schema information', 'using', 'Inductive Learning', 'Taking', 'consideration', 'define', 'measure', 'semantic richness', 'respect', 'set', 'characteristic patterns', 'define', 'patterns', 'expect', 'entities', 'have', 'pattern', 'belongs', 'observe', 'entity', 'type', 'infer', 'match', 'pattern', 'base', 'measure', 'semantic richness', 'correctness', 'such inferences', 'entity', 'really matches', 'count', 'correct inference', 'count', 'incorrect otherwise', 'example', 'consider including', 'pattern', 'has', 'dedicated', 'Wikipedia page', 'set', 'characteristic patterns', 'foaf', 'Person', 'observe', 'entity', 'foaf', 'Person', 'has', 'dedicated', 'Wikipedia page', 'say', 'inferring', 'pattern', 'say', 'measure', 'G', 'S', 'respect', 'S', 'set', 'characteristic patterns', 'Y', 'difference', 'expected number', 'correct inferences', 'expected number', 'incorrect ones', 'make', 'entity', 'equation', 'goal_pre', 'array', 'G', 'S', '=', 'Y', 'p', 'S', 'Y', 'S', '] G', 'S', '=', 'Y', 'i', 'S', 'array', 'equation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 41, 'end': 50, 'text': 'a concept'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 105, 'end': 139, 'text': 'a measure of its semantic richness'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 277, 'end': 288, 'text': 'our measure'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 921, 'end': 925, 'text': 'type'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 940, 'end': 947, 'text': 'dataset'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 961, 'end': 968, 'text': 'pattern'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 889, 'end': 972, 'text': 'the probability of an entity of type $\\\\alpha$ from dataset $S$ matching pattern $i$'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 1064, 'end': 1068, 'text': 'type'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 1081, 'end': 1099, 'text': 'the expected value'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 1144, 'end': 1161, 'text': 'the probabilities'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 1192, 'end': 1202, 'text': 'this value'}, 'T24': {'eid': 'T24', 'label': 'PRIMARY', 'start': 1208, 'end': 1234, 'text': 'the finite set of patterns'}, 'T27': {'eid': 'T27', 'label': 'PRIMARY', 'start': 1333, 'end': 1372, 'text': 'The expected value of this distribution'}, 'T29': {'eid': 'T29', 'label': 'PRIMARY', 'start': 1635, 'end': 1639, 'text': 'type'}, 'T31': {'eid': 'T31', 'label': 'PRIMARY', 'start': 1652, 'end': 1659, 'text': 'dataset'}, 'T34': {'eid': 'T34', 'label': 'PRIMARY', 'start': 2192, 'end': 2224, 'text': 'a set of characteristic patterns'}, 'T36': {'eid': 'T36', 'label': 'PRIMARY', 'start': 2294, 'end': 2298, 'text': 'type'}, 'T38': {'eid': 'T38', 'label': 'PRIMARY', 'start': 2328, 'end': 2335, 'text': 'pattern'}, 'T40': {'eid': 'T40', 'label': 'PRIMARY', 'start': 2396, 'end': 2405, 'text': 'an entity'}, 'T42': {'eid': 'T42', 'label': 'PRIMARY', 'start': 2420, 'end': 2424, 'text': 'type'}, 'T44': {'eid': 'T44', 'label': 'PRIMARY', 'start': 2469, 'end': 2476, 'text': 'pattern'}, 'T46': {'eid': 'T46', 'label': 'PRIMARY', 'start': 2573, 'end': 2582, 'text': 'an entity'}, 'T48': {'eid': 'T48', 'label': 'PRIMARY', 'start': 2609, 'end': 2616, 'text': 'pattern'}, 'T50': {'eid': 'T50', 'label': 'PRIMARY', 'start': 3049, 'end': 3077, 'text': 'measure of semantic richness'}, 'T52': {'eid': 'T52', 'label': 'PRIMARY', 'start': 3095, 'end': 3104, 'text': 'a concept'}, 'T54': {'eid': 'T54', 'label': 'PRIMARY', 'start': 3130, 'end': 3139, 'text': 'a dataset'}, 'T56': {'eid': 'T56', 'label': 'PRIMARY', 'start': 3148, 'end': 3180, 'text': 'a set of characteristic patterns'}, 'T60': {'eid': 'T60', 'label': 'PRIMARY', 'start': 3211, 'end': 3252, 'text': 'the expected number of correct inferences'}, 'T63': {'eid': 'T63', 'label': 'PRIMARY', 'start': 3257, 'end': 3294, 'text': 'the expected number of incorrect ones'}, 'T64': {'eid': 'T64', 'label': 'PRIMARY', 'start': 3188, 'end': 3294, 'text': 'the difference between the expected number of correct inferences and the expected number of incorrect ones'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent From equation \\ref{goal_pre}\n","it follows that , in order to maximize the value of $G(\\alpha,S)$ , $Y$ should be the set of patterns which occur in the majority of the entities ( $\\forall i \\in Y. p(i,\\alpha,S) > 0.5$ ) .\n","Therefore equation \\ref{goal} can be rewritten as follows :\n","\\begin{equation}\n","\\label{goal}\n","\\begin{array}{l}\n","G(\\alpha,S) = \\sum_{i \\in I}\n","\\begin{cases}\n","2p(i,\\alpha,S) - 1 & \\text{if } p(i,\\alpha,S) > 0.5 \\\\\n","0 & \\text{otherwise }\n","\\end{cases}\n","\\end{array}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['goal_pre', 'follows', 'order', 'maximize', 'value', 'G', 'S', 'Y', 'set', 'patterns', 'occur', 'majority', 'entities', 'i', 'Y. p', 'S', '>', 'goal', 'follows', 'equation', 'goal', 'array', 'G', 'S', '=', 'cases', 'i', 'S', 'p', 'S', '>', 'cases', 'array', 'equation']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\label{background}  \\noindent The notion of Information Content (IC) has been proposed to measure the informativeness of concepts within a taxonomy in the field of Information Theory \\cite{Resnik1995InformationContent} .\n","This measure has been shown to be effective not only with respect to concepts , but also to measure the informativeness of resources and relations within a knowledge base \\cite{Meymandpour2013LinkedDataInformativeness} .\n","The IC of a concept $\\alpha$ within a knowledge base is defined as $IC(\\alpha) = -log(p(\\alpha))$ , where $p(\\alpha)$ is the probability of an entity belonging to concept $\\alpha$ .\n","One limitation to the applicability of this measure is that the probability $p(\\alpha)$ might not be meaningful when considering domain specific datasets .\n","For example , several existing datasets contain only instances of type \\emph{foaf:Person} .\n","The IC of this concept computed over these datasets will always be equal to $0$ since $p(\\text{\\emph{foaf:Person}})=1$ .\n","The IC measure would not distinguish between the different datasets although they might have a different semantic interpretation of the concept \\emph{foaf:Person} .\n","A different measure is therefore necessary to compare the amount of information conveyed by the concept in each dataset .\n","\\par In the field of Cognitive Neuroscience several measures of semantic richness have been proposed \\cite{Pexman2008SemanticRichnessNOF} .\n","These measures have been used to explain why certain concepts can be processed better or faster by humans to perform certain tasks .\n","We adopt one such measure , called number - of - features ( NF ) , by measuring the number of facts that characterise a Linked Data concept .\n","The basic intuition behind the NF measure is that semantically rich concepts tend to be characterised with more attributes than less semantically rich ones .\n","\\par Our metric to compute the semantic richness of a concept does not assume the availability of pre-existing inference rules. In order to calculate how many facts we can infer about the entities of a given concept we adopt an Inductive Learning approach. For example, if all the entities of concept            from a graph            share property            , then we can induce the rule that if an entity belongs to            then it will have property            . The applications of Inductive Learning for Linked Data have received considerable attention in the recent years thanks to an increasing availability of Linked Data resources. The main advantages of Inductive Learning approaches are scalability and robustness to errors and uncertainty \\cite{Amato2010InductiveLearning} .\n","NOUN PHRASES:\n"," ['background', 'notion', 'Information Content', 'IC', 'measure', 'informativeness', 'concepts', 'taxonomy', 'field', 'Resnik1995InformationContent', 'measure', 'respect', 'concepts', 'measure', 'informativeness', 'resources', 'relations', 'Meymandpour2013LinkedDataInformativeness', 'IC', 'knowledge base', 'IC', '=', '-log', 'p', 'p', 'probability', 'entity', 'belonging', 'concept', 'limitation', 'applicability', 'measure', 'probability', 'p', 'considering', 'domain specific datasets', 'example', 'several existing', 'datasets', 'contain', 'instances', 'foaf', 'Person', 'IC', 'concept', 'computed', 'datasets', 'p', 'foaf', 'Person', '=1', 'IC measure', 'not distinguish', 'different datasets', 'have', 'different semantic interpretation', 'foaf', 'Person', 'different measure', 'compare', 'amount', 'information', 'conveyed', 'concept', 'dataset', 'field', 'Cognitive Neuroscience several measures', 'semantic richness', 'Pexman2008SemanticRichnessNOF', 'measures', 'explain', 'certain concepts', 'humans', 'perform', 'certain tasks', 'adopt', 'such measure', 'called', 'number', 'features', 'NF', 'measuring', 'number', 'facts', 'characterise', 'Linked', 'Data concept', 'basic intuition', 'NF measure', 'rich concepts', 'tend', 'attributes', 'rich ones', 'compute', 'semantic richness', 'concept', 'does', 'not assume', 'availability', 'pre-existing inference rules', 'order', 'calculate', 'many facts', 'infer', 'entities', 'given', 'concept', 'adopt', 'Inductive Learning approach', 'example', 'entities', 'concept', 'graph share property', 'induce', 'rule', 'entity', 'belongs', 'have', 'property', 'applications', 'Inductive Learning', 'Linked Data', 'have received', 'considerable attention', 'recent years thanks', 'increasing', 'availability', 'Linked Data resources', 'main advantages', 'Inductive Learning approaches', 'scalability', 'robustness', 'errors', 'Amato2010InductiveLearning']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 452, 'end': 461, 'text': 'a concept'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 442, 'end': 470, 'text': 'The IC of a concept $\\\\alpha$'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 44, 'end': 63, 'text': 'Information Content'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 563, 'end': 621, 'text': 'the probability of an entity belonging to concept $\\\\alpha$'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 605, 'end': 612, 'text': 'concept'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 663, 'end': 675, 'text': 'this measure'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 684, 'end': 699, 'text': 'the probability'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 2146, 'end': 2153, 'text': 'concept'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 2170, 'end': 2177, 'text': 'a graph'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 2195, 'end': 2203, 'text': 'property'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 2303, 'end': 2311, 'text': 'property'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\label{problem_definition}  \\noindent We define a measure of semantic richness            as a function that takes a concept            , such as \\emph{foaf:Person}  \\footnote{\\url{http://xmlns.com/foaf/0.1/Person} } and a Linked Data graph $S$ , such as the DBpedia graph \\cite{Auer2007DBpedia} , and returns a positive real number .\n","This number quantifies the amount of information conveyed by the concept $\\alpha$ within the graph $S$ .\n","In other words , it is a measure of how many facts we can expect to infer about the instances of that concept that are found in the graph .\n","If $\\Gamma(\\alpha,S_{1}) > \\Gamma(\\alpha,S_{2})$ then knowing that an entity is of type $\\alpha$ entails more facts in the context of the graph $S_{1}$ rather than $S_{2}$ .\n","\\par\n","When creating such measure it is important to consider scalability and robustness , as there is large variability in the size and quality of Linked Data resources .\n","It should be possible to compute this measure effectively over large amounts of Linked Data .\n","Moreover , this measure should be applicable to any dataset , even when taxonomic information is not available or when all the available entities are of a single type .\n","These requirements prevent the applicability of existing semantic measures such as the Information Content .\n","\\par Being based on the usage of a concept , rather than on its position within a conceptual framework such as a taxonomy , this notion differs from the notion of semantic specificity .\n","For example , while a concept is less semantically specific ( or less informative ) than its sub-concepts , it might be more semantically rich .\n","This situation occurs when the probability of an entity being a member of a sub-concept depends on the other sub-concepts it is a member of , such as in case of mutual exclusivity of sub-concepts .\n","For example , in a dataset $S$ a semantically rich concept $\\alpha$ might contain a small set of entities $s$ of which nothing is known .\n","If we consider those entities as members of concept $\\beta$ , we will have that $\\Gamma(\\beta,S) < \\Gamma(\\alpha,S)$\n","although $\\beta$ is a sub-concept of $\\alpha$ . \\\n","par\n","We define $S^{\\alpha_{1}}$ and $S^{\\alpha_{2}}$ as the sets of entities of type $\\alpha$ contained , respectively , in datasets $S_{1}$ and $S_{2}$ .\n","Our hypothesis is that the reuse of a Linked Data concept results in a decrease of its semantic richness .\n","Assuming that the sets of entities of type $\\alpha$ in two different datasets are disjoint ( $S^{\\alpha_{1}} \\cap S^{\\alpha_{2}} = \\emptyset$ ) , we express this hypothesis as follows :\n","\\begin{equation}\n","\\label{eq:hypothesis}\n","\\Gamma(\\alpha,S_{1} \\cup S_{2}) \\leq \\frac{|S^{\\alpha_{1}}|\\Gamma(\\alpha,S_{1})+ |S^{\\alpha_{2}}|\\Gamma(\\alpha,S_{2})}{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{equation}\n","Intuitively , this inequation states that the semantic richness of a concept with respect to the union of two graphs is at most equal to the average semantic richness of the graphs .\n","The difference between these values represents the amount of information about a concept that is lost when merging two datasets .\n","In Section \\ref{experiments} we will show how , in practice , the union of two graphs typically results in a significant decrease of semantic richness .\n","NOUN PHRASES:\n"," ['problem_definition', 'define', 'measure', 'semantic richness', 'function', 'takes', 'concept', 'foaf', 'Person', 'http', '//xmlns.com/foaf/0.1/Person', 'Linked', 'Data graph', 'S', 'Auer2007DBpedia', 'returns', 'positive real number', 'number', 'quantifies', 'amount', 'information', 'conveyed', 'S', 'other words', 'measure', 'many facts', 'expect', 'infer', 'instances', 'concept', 'graph', 'S_', 'S_', 'then knowing', 'entity', 'entails', 'facts', 'context', 'S_', 'S_', 'creating', 'such measure', 'consider', 'scalability', 'robustness', 'large variability', 'size', 'quality', 'Linked Data resources', 'compute', 'measure', 'large amounts', 'Linked Data', 'measure', 'dataset', 'taxonomic information', 'available entities', 'single type', 'requirements', 'prevent', 'applicability', 'existing', 'semantic measures', 'Information Content', 'based', 'usage', 'concept', 'position', 'conceptual framework', 'taxonomy', 'notion differs', 'notion', 'semantic specificity', 'example', 'concept', 'situation', 'occurs', 'probability', 'entity', 'member', 'sub-concept depends', 'member', 'case', 'mutual exclusivity', 'sub-concepts', 'example', 'S', 'semantically rich concept', 'contain', 'small set', 'entities', 'nothing', 'consider', 'entities', 'members', 'have', 'S', '<', 'S', 'sub-concept', 'define', 'S^', 'S^', 'sets', 'entities', 'contained', 'datasets', 'S_', 'S_', 'hypothesis', 'reuse', 'Linked', 'Data concept results', 'decrease', 'semantic richness', 'Assuming', 'sets', 'entities', 'different datasets', 'disjoint', 'S^', 'express', 'hypothesis', 'follows', 'equation', 'eq', 'hypothesis', 'S_', '|S^', 'S_', '+', 'S_', '|+|S^', 'equation', 'inequation', 'states', 'semantic richness', 'concept', 'respect', 'union', 'graphs', 'average semantic richness', 'graphs', 'difference', 'values', 'represents', 'amount', 'information', 'concept', 'merging', 'datasets', 'experiments', 'show', 'practice', 'union', 'graphs', 'results', 'significant decrease', 'semantic richness']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 48, 'end': 78, 'text': 'a measure of semantic richness'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 115, 'end': 124, 'text': 'a concept'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 221, 'end': 240, 'text': 'a Linked Data graph'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 396, 'end': 407, 'text': 'the concept'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 424, 'end': 433, 'text': 'the graph'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 663, 'end': 667, 'text': 'type'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 718, 'end': 723, 'text': 'graph'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 773, 'end': 785, 'text': 'such measure'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 1029, 'end': 1041, 'text': 'this measure'}, 'T22': {'eid': 'T22', 'label': 'PRIMARY', 'start': 1842, 'end': 1851, 'text': 'a dataset'}, 'T24': {'eid': 'T24', 'label': 'PRIMARY', 'start': 1856, 'end': 1883, 'text': 'a semantically rich concept'}, 'T26': {'eid': 'T26', 'label': 'PRIMARY', 'start': 1907, 'end': 1930, 'text': 'a small set of entities'}, 'T28': {'eid': 'T28', 'label': 'PRIMARY', 'start': 2007, 'end': 2014, 'text': 'concept'}, 'T31': {'eid': 'T31', 'label': 'PRIMARY', 'start': 2100, 'end': 2125, 'text': 'a sub-concept of $\\\\alpha$'}, 'T35': {'eid': 'T35', 'label': 'PRIMARY', 'start': 2185, 'end': 2232, 'text': 'the sets of entities of type $\\\\alpha$ contained'}, 'T36': {'eid': 'T36', 'label': 'PRIMARY', 'start': 2253, 'end': 2261, 'text': 'datasets'}, 'T39': {'eid': 'T39', 'label': 'PRIMARY', 'start': 2429, 'end': 2433, 'text': 'type'}, 'T41': {'eid': 'T41', 'label': 'PRIMARY', 'start': 2405, 'end': 2468, 'text': 'the sets of entities of type $\\\\alpha$ in two different datasets'}, 'T45': {'eid': 'T45', 'label': 'PRIMARY', 'start': 2799, 'end': 2814, 'text': 'this inequation'}, 'T47': {'eid': 'T47', 'label': 'PRIMARY', 'start': 2827, 'end': 2901, 'text': 'the semantic richness of a concept with respect to the union of two graphs'}, 'T49': {'eid': 'T49', 'label': 'PRIMARY', 'start': 2922, 'end': 2965, 'text': 'the average semantic richness of the graphs'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent The particular threshold of            is a result of the fact that in this measure, which aims at being generic, correct and wrong inferences carry the same (absolute) weight. Similarly, all patterns carry equal weight. This formulation of semantic richness is not meant to be unique and variations are possible. For example, equation \\ref{goal} could be modified to penalize wrong inferences more .\n","Also , different patterns could be given different weights to represent how important or informative they are for a specific application .\n","\\\n","par\n","In this work we use use our generic measure of semantic richness to analyse our hypothesis , namely that as we consider more entities as members of a concept , the semantic richness of that concept decreases .\n","In fact , assuming that the two sets of entities are disjoint ( $S^{\\alpha_{1}} \\cap S^{\\alpha_{2}} = \\emptyset$ ) the semantic richness of the union of two sources of data is always inferior or equal to the average individual semantic richness of the sources :\n","\\begin{equation}\n","\\label{diminishing_semantics}\n","G(\\alpha,S_{1} \\cup S_{2}) \\leq \\frac{ |S^{\\alpha_{1}}|G(\\alpha,S_{1}) + |S^{\\alpha_{2}}|G(\\alpha,S_{2}) }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['particular threshold', 'result', 'fact', 'measure', 'aims', 'wrong inferences', 'carry', 'absolute', 'weight', 'patterns', 'carry', 'equal weight', 'formulation', 'semantic richness', 'not meant', 'variations', 'example', 'goal', 'penalize', 'wrong inferences', 'different patterns', 'different weights', 'represent', 'specific application', 'work', 'use', 'generic measure', 'semantic richness', 'analyse', 'hypothesis', 'consider', 'entities', 'members', 'concept', 'semantic richness', 'concept', 'decreases', 'fact', 'assuming', 'sets', 'entities', 'disjoint', 'S^', 'semantic richness', 'union', 'sources', 'data', 'average individual semantic richness', 'sources', 'equation', 'diminishing_semantics', 'G', 'S_', '|S^', '|G', 'S_', '+', '|G', 'S_', '|+|S^', 'equation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 798, 'end': 814, 'text': 'sets of entities'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 881, 'end': 938, 'text': 'the semantic richness of the union of two sources of data'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 970, 'end': 1025, 'text': 'the average individual semantic richness of the sources'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\label{proofs}\n","\\noindent Formula \\ref{diminishing_semantics} can be proved according to our definition of semantic richness defined in Formula \\ref{goal} .\n","It can be observed that the semantic richness of a concept is the sum of the individual semantic richness of each pattern $G^{\\prime}(\\alpha,S,i) = ( 2p(i,\\alpha,S) - 1 ) [p(i,\\alpha,S) > 0.5]$ .\n","Therefore Formula \\ref{diminishing_semantics} can be proved by proving that the inequation holds for every individual pattern :\n","\\begin{equation}\n","\\label{diminishing_semantics_proof_goal}\n","G^{\\prime}(\\alpha,S_{1} \\cup S_{2},i) \\leq \\frac{ |S^{\\alpha_{1}}|G^{\\prime}(\\alpha,S_{1},i) + |S^{\\alpha_{2}}|G^{\\prime}(\\alpha,S_{2},i) }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{equation}\n","Having defined $w_{1} = p(i,\\alpha,S_{1})$ and $w_{2} = p(i,\\alpha,S_{2})$ , then the probability $p(i,\\alpha,S_{1} \\cup S_{2})$ can be defined as ratio between the number of entities of each set which match pattern $i$ and the number of all the entities in the union of the sets :\n","\\begin{equation}\n","\\label{diminishing_semantics_proof_goal_2}\n","p(i,\\alpha,S_{1} \\cup S_{2}) = \\frac{ |S^{\\alpha_{1}}|w_{i} + |S^{\\alpha_{2}}|w_{2} }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{equation}\n","Formula \\ref{diminishing_semantics_proof_goal} can then be written as follows :\n","\\begin{equation}\n","\\label{diminishing_semantics_proof_goal_3}\n","\\begin{array}{l}\n","(2\\frac{ |S^{\\alpha_{1}}|w_{i} + |S^{\\alpha_{2}}|w_{2} }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|} -1) [\\frac{ |S^{\\alpha_{1}}|w_{i} + |S^{\\alpha_{2}}|w_{2} }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|} > 0.5] \\leq \\\\[2mm]\n","\\frac{ |S^{\\alpha_{1}}|(2w_{1} -1)[w_{1}>0.5] + |S^{\\alpha_{2}}|(2w_{2} -1) [w_{2}>0.5]}{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{array}\n","\\end{equation}\n","From Formula \\ref{diminishing_semantics_proof_goal_3} it follows that in case $w_{1} \\leq 0.5$ and $w_{1} \\leq 0.5$ then $p(i,\\alpha,S_{1} \\cup S_{2}) < 0.5$ and therefore Formula \\ref{diminishing_semantics_proof_goal_3} is reduced to the tautology $0 \\leq 0$ .\n","This matches the intuition that if a pattern was not frequent in any of the original datasets , it will not be frequent also in the union of the datasets .\n","\\par In case            and            then            and therefore equation \\ref{diminishing_semantics_proof_goal_3} becomes :\n","\\begin{equation}\n","\\label{diminishing_semantics_proof_goal_4}\n","\\begin{array}{l}\n","2\\frac{ |S^{\\alpha_{1}}|w_{i} + |S^{\\alpha_{2}}|w_{2} }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|} -1 \\leq\n","\\frac{ |S^{\\alpha_{1}}|(2w_{1} -1) + |S^{\\alpha_{2}}|(2w_{2} -1) }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{array}\n","\\end{equation}\n","This equation can be simplified to the tautology $0 \\leq 0$ .\n","This matches the intuition that a pattern which was frequent in both of the original datasets will still be frequent in the union of the datasets .\n","\\par We then consider the last case in which a pattern is frequent only in one of the original datasets: when            and            . This requires us to consider two sub-cases, depending on the value of            . If            , then Formula \\ref{diminishing_semantics_proof_goal_3} can be simplified to :\n","\\begin{equation}\n","\\label{diminishing_semantics_proof_goal_5}\n","\\begin{array}{l}\n","0 \\leq\n","\\frac{ |S^{\\alpha_{1}}|(2w_{1} -1)}{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{array}\n","\\end{equation}\n","This formula can be further simplified to $w_{1} > 0.5$ which is true by assumption .\n","Finally , in case $w_{1} > 0.5$ and $w_{2} \\leq 0.5$ and $p(i,\\alpha,S_{1} \\cup S_{2}) > 0.5$ Formula \\ref{diminishing_semantics_proof_goal_3} can be simplified to :\n","\\begin{equation}\n","\\label{diminishing_semantics_proof_goal_6}\n","\\begin{array}{l}\n","2\\frac{ |S^{\\alpha_{1}}|w_{i} + |S^{\\alpha_{2}}|w_{2} }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|} -1 \\leq\n","\\frac{ |S^{\\alpha_{1}}|(2w_{1} -1) }{|S^{\\alpha_{1}}|+|S^{\\alpha_{2}}|}\n","\\end{array}\n","\\end{equation}\n","This equation can be simplified to $w_{2} \\leq 0.5$ which is true by assumption .\n","Having proved Formula \\ref{diminishing_semantics_proof_goal} , and therefore Formula \\ref{diminishing_semantics} , we have proved our hypothesis ( Formula \\ref{eq:hypothesis} ) with respect to our measure of semantic richness .\n","NOUN PHRASES:\n"," ['proofs', 'diminishing_semantics', 'definition', 'semantic richness', 'defined', 'goal', 'semantic richness', 'concept', 'sum', 'individual semantic richness', 'G^', 'S', 'i', '=', 'i', 'S', '[ p', 'S', '>', 'diminishing_semantics', 'proving', 'inequation', 'holds', 'individual pattern', 'equation', 'diminishing_semantics_proof_goal', 'G^', 'S_', '|S^', '|G^', 'S_', '+', '|G^', 'S_', '|+|S^', 'equation', 'Having', 'defined', 'p', 'S_', 'p', 'S_', 'probability', 'p', 'S_', 'ratio', 'number', 'entities', 'set', 'match', 'number', 'entities', 'union', 'sets', 'equation', 'diminishing_semantics_proof_goal_2', 'p', 'S_', '=', '|S^', '|w_', 'i', '+ |S^', '|w_', '|+|S^', 'equation', 'diminishing_semantics_proof_goal', 'follows', 'equation', 'diminishing_semantics_proof_goal_3', 'array', '|S^', '|w_', 'i', '+ |S^', '|w_', '|+|S^', '-1', '[', '|w_', 'i', '+ |S^', '|w_', '|+|S^', '[', '|S^', '|', '-1', '[', '] + |S^', '|', '-1', '[', ']', '|+|S^', 'array', 'equation', 'diminishing_semantics_proof_goal_3', 'follows', 'case', 'p', 'S_', 'diminishing_semantics_proof_goal_3', 'tautology', 'matches', 'intuition', 'pattern', 'original datasets', 'union', 'datasets', 'case', 'diminishing_semantics_proof_goal_3', 'becomes', 'equation', 'diminishing_semantics_proof_goal_4', 'array', '|S^', '|w_', 'i', '+ |S^', '|w_', '|+|S^', '|S^', '|', '-1', '+', '|', '-1', '|+|S^', 'array', 'equation', 'equation', 'tautology', 'matches', 'intuition', 'pattern', 'frequent', 'original datasets', 'union', 'datasets', 'then consider', 'last case', 'pattern', 'original datasets', 'requires', 'consider', 'sub-cases', 'depending', 'value', 'diminishing_semantics_proof_goal_3', 'equation', 'diminishing_semantics_proof_goal_5', 'array', '|S^', '|', '-1', '|+|S^', 'array', 'equation', 'formula', 'further simplified', 'w_', '>', 'assumption', 'case', '>', 'p', 'S_', 'diminishing_semantics_proof_goal_3', 'equation', 'diminishing_semantics_proof_goal_6', 'array', '|S^', '|w_', 'i', '+ |S^', '|w_', '|+|S^', '|S^', '|', '-1', '|+|S^', 'array', 'equation', 'equation', 'w_', 'assumption', 'Having proved', 'diminishing_semantics_proof_goal', 'diminishing_semantics', 'have proved', 'hypothesis', 'eq', 'hypothesis', 'respect', 'measure', 'semantic richness']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 229, 'end': 277, 'text': 'the individual semantic richness of each pattern'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 810, 'end': 825, 'text': 'the probability'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 875, 'end': 1007, 'text': 'ratio between the number of entities of each set which match pattern $i$ and the number of all the entities in the union of the sets'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 889, 'end': 947, 'text': 'the number of entities of each set which match pattern $i$'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 903, 'end': 947, 'text': 'entities of each set which match pattern $i$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 936, 'end': 943, 'text': 'pattern'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 952, 'end': 1007, 'text': 'the number of all the entities in the union of the sets'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 970, 'end': 1007, 'text': 'the entities in the union of the sets'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent We have run an experiment to quantify the decrease in semantic richness as concepts are reused. In this experiment we have considered the concept \\emph{foaf:Person} as it is used in ten different datasets .\n","The list of the endpoints used to access the datasets is shown in Table \\ref{tab:sources} .\n","From each endpoint we have extracted information about 1000 randomly selected entities .\n","\\par For each dataset, we have calculated its semantic richness at different states. Starting from the semantic richness of the original dataset, we have recalculated this measure after adding progressively more entities from the other datasets. Figure \\ref{fig:decay_of_G} shows the progressive decay of semantic richness as we add new entities of the same type from other sources .\n","The semantic richness of each dataset quickly converges to a value which is significantly inferior to the average measure .\n","While the average semantic richness of all the individual sources is $9.6$ , the semantic richness of the merged set results is $1.3$ .\n","This experiment validates our hypothesis since the semantic richness of the union of a number of sources is inferior to their average individual semantic richness .\n","Another fact that can be observed is that different datasets represent the same concept with significantly different levels of semantic richness .\n","In this case , the values ranged from $0.7$ of \\url{services.data.gov.uk} to $35.2$ of \\url{dbpedia.org} .\n","NOUN PHRASES:\n"," ['have run', 'experiment', 'quantify', 'decrease', 'semantic richness', 'concepts', 'experiment', 'have considered', 'foaf', 'Person', 'ten different datasets', 'list', 'endpoints', 'used', 'access', 'datasets', 'tab', 'sources', 'endpoint', 'have extracted', 'information', 'randomly selected', 'entities', 'dataset', 'have calculated', 'semantic richness', 'different states', 'Starting', 'semantic richness', 'original dataset', 'have recalculated', 'measure', 'adding', 'entities', 'other datasets', 'fig', 'decay_of_G', 'shows', 'progressive decay', 'semantic richness', 'add', 'new entities', 'same type', 'other sources', 'semantic richness', 'dataset', 'quickly converges', 'value', 'average measure', 'average semantic richness', 'individual sources', 'semantic richness', 'merged', 'set results', 'experiment', 'validates', 'hypothesis', 'semantic richness', 'union', 'number', 'sources', 'average individual semantic richness', 'fact', 'different datasets', 'represent', 'same concept', 'different levels', 'semantic richness', 'case', 'values', 'ranged', 'services.data.gov.uk', 'dbpedia.org']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent Our approach uses a numerical measure of the semantic richness of a concept using equation \\ref{goal} .\n","But how well does this measure actually capture the concept of semantic richness ?\n","We evaluate this measure according to the intuition that concepts are , in average , less semantically rich than their sub-concepts .\n","A similar property is also found in the IC measure , as the IC of a concept is , by definition , always inferior or equal to the IC of its sub-concepts .\n","\\par We have evaluated how well our measure matches this intuition on the concepts of the DBpedia ontology\\footnote{\\url{http://dbpedia.org/ontology/} } by calculating the semantic richness of the entities of each concept .\n","\\footnote{The entities were extracted from DBpedia version 3.5.1}\n","To reduce the computational complexity , all concepts with over 1000 entities have been replaced by a randomly sampled set of 1000 entities .\n","Also , we have considered only simple graph patterns which involve a single triple .\n","More specifically , we have considered patterns as pairs $<p,o>$ which match an entity $\\epsilon$ if the triple $<\\epsilon, p, o>$ is asserted in the dataset .\n","Improving the quality of this measure by considering more complex patterns is left for further research .\n","\\par Figure \\ref{fig:increase_of_G} shows the change of semantic richness between each concept and its sub-concepts .\n","This graph shows a significant tendency of concepts to be less semantically rich than their sub-concepts .\n","In fact , 90\\% of the subclass relations resulted in an increase of semantic richness from a concept to its sub-concepts .\n","Also , in all cases concepts resulted in a lower semantic richness than the average semantic richness of their sub-concepts , thus supporting our hypothesis .\n","Only in 10\\% of the cases a subclass relation resulted in a decrease of semantic richness from a concept to a sub-concept .\n","This situation occurred because in the DBpedia ontology , sibling concepts are mutually exclusive .\n","Therefore , the knowledge that an entity belongs to a certain concept affects the probability of it being a member of another , potentially more semantically rich , concept .\n","NOUN PHRASES:\n"," ['approach', 'uses', 'numerical measure', 'semantic richness', 'concept', 'using', 'goal', 'well does', 'measure', 'actually capture', 'concept', 'semantic richness', 'evaluate', 'measure', 'according', 'intuition', 'concepts', 'sub-concepts', 'similar property', 'also found', 'IC measure', 'IC', 'concept', 'definition', 'IC', 'sub-concepts', 'have evaluated', 'measure', 'matches', 'intuition', 'concepts', 'http', '//dbpedia.org/ontology/', 'calculating', 'semantic richness', 'entities', 'concept', 'entities', 'DBpedia version', 'reduce', 'computational complexity', 'concepts', 'entities', 'randomly sampled', 'set', 'entities', 'have considered', 'simple graph patterns', 'involve', 'single triple', 'have considered', 'patterns', '< p', '>', 'match', 'entity', 'p', 'dataset', 'Improving', 'quality', 'measure', 'considering', 'complex patterns', 'further research', 'fig', 'increase_of_G', 'shows', 'change', 'semantic richness', 'concept', 'graph', 'shows', 'significant tendency', 'concepts', 'fact', '%', 'subclass relations', 'resulted', 'increase', 'semantic richness', 'concept', 'sub-concepts', 'cases concepts', 'resulted', 'semantic richness', 'average semantic richness', 'sub-concepts', 'thus supporting', 'hypothesis', '%', 'cases', 'subclass relation', 'resulted', 'decrease', 'semantic richness', 'concept', 'situation', 'occurred', 'DBpedia ontology', 'sibling', 'concepts', 'knowledge', 'entity', 'belongs', 'certain concept', 'affects', 'probability', 'member', 'concept']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 1041, 'end': 1049, 'text': 'patterns'}, 'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 1053, 'end': 1058, 'text': 'pairs'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 1079, 'end': 1088, 'text': 'an entity'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 1103, 'end': 1113, 'text': 'the triple'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A {\\bf results graph } is an RDF graph $G$ where every \\href{https://www.w3.org/TR/rdf11-concepts/#section-rdf-graph} {\\em node } in $G$ that has ASHACL type {\\tt sh : Validation Result } in $G$ meets the following conditions\n","\\begin{itemize}  \\item It has {\\tt sh:ValidationResult} as a value for {\\tt rdf : type } in $G$ .            in $G$ .\n","\\item It has at most one value for {\\tt sh:valueNode} in $G$ .            in $G$ and this value if present is an ASHACL property path in $G$ . \\item It has exactly one value for {\\tt sh:sourceShape} in $G$ . \\item It has exactly one value for {\\tt sh:sourceConstraintComponent} in $G$ . \\item It has exactly one value for {\\tt sh:resultSeverity} in $G$ . \\item Each of its values for {\\tt sh:resultMessage} in $G$ is a \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-language-tagged-string} {\\em language - tagged string } and each has a different \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-language-tag} {\\em language tag } .\n","\\item Each of its values for {\\tt sh:detail} in $G$ has ASHACL type {\\tt sh : Validation Result } in $G$ . \\ item\n","Each of its values for $p$ in $G$ where $p$ is a list - taking parameter is an ASHACL list in $G$ . \\end{itemize}\n","NOUN PHRASES:\n"," ['RDF graph', 'G', 'https', 'G', 'has', 'ASHACL type', 'Validation Result', 'G', 'meets', 'following conditions', 'itemize', 'has', 'ValidationResult', 'value', 'type', 'G', 'G', 'has', 'value', 'valueNode', 'G', 'G', 'value', 'ASHACL property path', 'G', 'has', 'value', 'sourceShape', 'G', 'has', 'value', 'sourceConstraintComponent', 'G', 'has', 'value', 'resultSeverity', 'G', 'values', 'resultMessage', 'G', 'https', 'tagged', 'string', 'has', 'https', 'values', 'detail', 'G', 'has', 'ASHACL type', 'Validation Result', 'G', 'values', 'G', 'list', 'taking', 'parameter', 'ASHACL list', 'G', 'itemize']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 26, 'end': 38, 'text': 'an RDF graph'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 7, 'end': 20, 'text': 'results graph'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If an RDF term is not an ill - formed property path in an RDF graph $G$ then it is an { \\bf ASHACL property path } in $G$ .\n","NOUN PHRASES:\n"," ['RDF term', 'formed', 'property path', 'RDF graph', 'G', 'G']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 55, 'end': 67, 'text': 'an RDF graph'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","A { \\ bf validation result } from RDF term $f$ , optional RDF term $v$ , data graph $D$ , constraint $c$ that has kind $C$ , and shape $s$ in shapes graph $S$ is a node in a results graph $G$ that meets the following conditions\n","\\begin{itemize}\n","\\item It has {\\tt sh:ValidationResult} as a value for {\\tt rdf : type } in $G$ .\n","\\item Its sole value for {\\tt sh:focusNode} in $G$ is $f$ .\n","\\item Its sole value for {\\tt sh:valueNode} in $G$ is $v$\n","if $v$ is present , otherwise it has no value for {\\tt sh : value Node } in $G$ .\n","\\item Its sole value for {\\tt sh:resultPath} in $G$ is an ASHACL property path in $G$ that is path - equivalent to the value of $s$ for {\\tt sh : path } in $S$\n","if there is one , otherwise it has no value for {\\tt sh : resultPath } in $G$ .\n","\\item Its sole value for {\\tt sh:sourceShape} in $G$ is $s$ .\n","\\item Its sole value for {\\tt sh:sourceConstraintComponent} in $G$ is $C$ .\n","\\item Its sole value for {\\tt sh:resultSeverity} in $G$ is the severity of $s$ in $S$ .\n","\\item Its values for {\\tt sh:resultMessage} in $G$ include any values of $s$ for {\\tt sh : message } in $S$ .\n","\\ item\n","Its values , if any , for $p$ in $G$ with $p$ a non-list - taking parameter of $C$ and $X = \\{ x | \\left<p,x\\right> \\mbox{is a parameter value of $ c $}\\}$ are the elements of $X$ .\n","\\ item\n","Its values , if any , for $p$ in $G$ with $p$ a list - taking parameter of $C$ and $X = \\{ x | \\left<p,x\\right> \\mbox{is a parameter value of $ c $}\\}$ contain for each member $x$ of $X$ an ASHACL list in $G$ whose elements in $G$ are the elements of $x$ in $S$ and no other values .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['RDF term', 'optional RDF term', 'D', 'constraint', 'c', 'has', 'kind', 'C', 'shape', 'shapes', 'graph', 'S', 'node', 'results', 'G', 'meets', 'following conditions', 'itemize', 'has', 'ValidationResult', 'value', 'type', 'G', 'sole value', 'focusNode', 'G', 'sole value', 'valueNode', 'G', 'otherwise', 'has', 'value', 'value Node', 'G', 'sole value', 'resultPath', 'G', 'ASHACL property path', 'G', 'equivalent', 'value', 'path', 'S', 'otherwise', 'has', 'value', 'resultPath', 'G', 'sole value', 'sourceShape', 'G', 'sole value', 'sourceConstraintComponent', 'G', 'C', 'sole value', 'resultSeverity', 'G', 'severity', 'S', 'values', 'resultMessage', 'G', 'include', 'values', 'message', 'S', 'values', 'G', 'taking', 'parameter', 'C', 'parameter value', 'elements', 'X', 'values', 'G', 'list', 'taking', 'parameter', 'C', 'parameter value', 'contain', 'member', 'X', 'ASHACL list', 'G', 'elements', 'G', 'elements', 'S', 'other values', 'itemize']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 45, 'end': 53, 'text': 'RDF term'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 60, 'end': 77, 'text': 'optional RDF term'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 84, 'end': 94, 'text': 'data graph'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 101, 'end': 111, 'text': 'constraint'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 125, 'end': 129, 'text': 'kind'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 140, 'end': 145, 'text': 'shape'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 153, 'end': 165, 'text': 'shapes graph'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 183, 'end': 198, 'text': 'a results graph'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 1477, 'end': 1483, 'text': 'member'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A blank node is an { \\bf ill - formed property path } in an RDF graph $G$ if there is a path in $G$ from the node back to itself where the sequence of predicates of the RDF triples in the path matches the regular expression \\begin{align*} ( \\; & ( \\; \\mbox{\\tt rdf:rest} * \\; \\mbox{\\tt rdf:first} \\; ) \\; | \\\\ & \\mbox{\\tt sh:alternativePath}            * \\; \\mbox{\\tt rdf:first} \\;) \\; | \\\\ & \\mbox{\\tt sh:inversePath}  \\; | \\; \\mbox{\\tt sh:zeroOrMorePath} \\; | \\\\ & \\mbox{\\tt sh:oneOrMorePath}  \\; | \\; \\mbox{\\tt sh:zeroOrOnePath} \\; ) \\; \\; + \\end{align*}\n","NOUN PHRASES:\n"," ['blank node', 'ill', 'formed', 'property path', 'RDF graph', 'G', 'path', 'G', 'node', 'sequence', 'predicates', 'RDF triples', 'path', 'matches', 'rest', 'alternativePath', 'inversePath', 'zeroOrMorePath', 'oneOrMorePath', 'zeroOrOnePath', '+', 'align*']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The { \\bf path mapping } in an RDF graph $G$ of a RDF term $p$ that is an ASHACL property path in $G$ , $path(p,S)$ , is a \\href{https://www.w3.org/TR/sparql11-query/#pp-language} { \\em SPARQL property path expression } defined as follows :\n","\\begin{enumerate}            . \\ item\n","If $p$ is a blank node that is an ASHACL list in $G$ that has at least two members in $G$ and none of these members are ill - formed property paths in $G$ then $path(p,S)$ is { \\sl SequencePath (            \\ldots $path(v_n,S)$ ) } where $v_i$ are the members of $p$ in $G$ , in order .\n","\\item If            is a blank node that has exactly one value for {\\tt sh:alternativePath} in G and that value is an ASHACL list in $G$ that has at least two members in $G$ and none of the members are ill - formed property paths in $G$ then $path(p,S)$ is { \\sl AlternativePath (            \\ldots $path(vn,S)$ ) } where $v_i$ are the members of the list in $G$ , in order .\n","\\item If            is a blank node that has exactly one value            for {\\tt sh:inversePath} in $G$ and $v$ is not an ill - formed property path in $G$ then $path(p,S)$ is { \\sl InversePath ( $path(v,S)$ ) } .\n","\\item If            is a blank node that has exactly one value            for {\\tt sh:zeroOrMorePath} in $G$ and $v$ is not an ill - formed property path in $G$ then $path(p,S)$ is { \\sl ZeroOrMorePath ( $path(v,S)$ ) } .\n","\\item If            is a blank node that has exactly one value            for {\\tt sh:oneOrMorePath} in G and $v$ is not an ill - formed property path in $G$ then $path(p,S)$ is { \\sl OneOr MorePath ( $path(v,S)$ ) } .\n","\\item If            is a blank node that has exactly one value            for {\\tt sh:zeroOrOnePath} in G and $v$ is not an ill - formed property path in $G$ then $path(p,S)$ is {\\sl ZeroOrOnePath ( $path(v,S)$ ) } .\n","\\end{enumerate}\n","NOUN PHRASES:\n"," ['RDF graph', 'G', 'RDF term', 'ASHACL property path', 'G', 'path', 'p', 'S', 'https', 'pp-language', 'defined', 'follows', 'enumerate', 'blank node', 'ASHACL list', 'G', 'has', 'members', 'G', 'none', 'members', 'formed', 'property paths', 'G', 'path', 'p', 'S', 'path', 'v_n', 'S', 'members', 'G', 'order', 'blank node', 'has', 'value', 'alternativePath', 'G', 'value', 'ASHACL list', 'G', 'has', 'members', 'G', 'none', 'members', 'formed', 'property paths', 'G', 'path', 'p', 'S', 'path', 'vn', 'S', 'members', 'list', 'G', 'order', 'blank node', 'has', 'value', 'inversePath', 'G', 'formed', 'property path', 'G', 'path', 'p', 'S', 'path', 'v', 'S', 'blank node', 'has', 'value', 'zeroOrMorePath', 'G', 'formed', 'property path', 'G', 'path', 'p', 'S', 'path', 'v', 'S', 'blank node', 'has', 'value', 'oneOrMorePath', 'G', 'formed', 'property path', 'G', 'path', 'p', 'S', 'path', 'v', 'S', 'blank node', 'has', 'value', 'zeroOrOnePath', 'G', 'formed', 'property path', 'G', 'path', 'p', 'S', 'path', 'v', 'S', 'enumerate']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A {\\bf results structure } is a results graph $R$ and a set of nodes from $R$ each of which have ASHACL type {\\tt sh : Validation Result } in $R$ .\n","The set of nodes is called the {\\bf top - level validation results } of the results structure .\n","NOUN PHRASES:\n"," ['results', 'R', 'set', 'nodes', 'R', 'have', 'ASHACL type', 'Validation Result', 'R', 'set', 'nodes', 'level validation results', 'results structure']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 30, 'end': 45, 'text': 'a results graph'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A {\\bf shapes graph } is an RDF graph $G$ containing no ill - formed shapes in $G$ .\n","{\\bf Ill - formed shapes } are defined throughout the remainder of this document .\n","NOUN PHRASES:\n"," ['RDF graph', 'G', 'containing', 'formed', 'shapes', 'G', 'formed', 'shapes', 'remainder', 'document']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 25, 'end': 37, 'text': 'an RDF graph'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The { \\bf explicit shapes } of an RDF graph $G$ , $ES(G)$ , is the set of nodes in $G$ that \\begin{enumerate}  \\item have {\\tt sh:Shape} as an ASHACL type in $G$ , or \\ item are the subject of an RDF triple in $G$ with predicate one of the target predicates , { \\tt sh:targetNode}, {\\tt sh:targetClass} , { \\tt sh:targetSubjectsOf}, and {\\tt sh:targetObjectsOf} .           \n","The { \\bf shapes } of an RDF graph $G$ , $S(G)$ , is the smallest set of RDF terms such that            \\ item $ES(G) \\subseteq S(G)$ , \\ item\n","If $s \\in S(G)$ and $\\left< s, p, o \\right>$ is a triple in $G$ wiith $p$ a non-list - taking , shape - inducing parameter then $o \\in S(G)$ . \\ item If $s \\in S(G)$ , $\\left< s, p, l \\right>$ is a triple in $G$ with $p$ a list - taking , shape - inducing parameter and $l$ an ASHACL list in $G$ , and $o$ is a mamber of $l$ in $G$ then $o \\in S(G)$ .\n","\\end{enumerate}\n","NOUN PHRASES:\n"," ['RDF graph', 'G', 'ES', 'G', 'set', 'nodes', 'G', 'enumerate', 'have', 'Shape', 'ASHACL type', 'G', 'subject', 'RDF triple', 'G', 'target', 'predicates', 'targetNode', 'targetClass', 'targetSubjectsOf', 'targetObjectsOf', 'RDF graph', 'G', 'S', 'G', 'set', 'RDF terms', 'ES', 'G', 'S', 'G', 'G', '< s', 'p', 'triple', 'G', 'taking', 'shape', 'inducing parameter', 'S', 'G', 'item', 'G', '< s', 'p', 'l', '>', 'triple', 'G', 'list', 'taking', 'shape', 'inducing parameter', 'ASHACL list', 'G', 'mamber', 'G', 'S', 'G', 'enumerate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 31, 'end': 43, 'text': 'an RDF graph'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 0, 'end': 43, 'text': 'The { \\\\bf explicit shapes } of an RDF graph'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 397, 'end': 409, 'text': 'an RDF graph'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 375, 'end': 409, 'text': 'The { \\\\bf shapes } of an RDF graph'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 566, 'end': 574, 'text': 'a triple'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 714, 'end': 722, 'text': 'a triple'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 592, 'end': 640, 'text': 'a non-list - taking , shape - inducing parameter'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 739, 'end': 783, 'text': 'a list - taking , shape - inducing parameter'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 827, 'end': 849, 'text': 'a mamber of $l$ in $G$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A results structure { \\ bf contains } a top - level validation result from $f$ , $v$ , $D$ , $c$ , $s$ , $S$ ( or $f$ , $D$ , $c$ , $s$ , $S$ ) if its graph contains a node that is a validation result from $f$ , $v$ , $D$ , $c$ , $s$ , $S$ ( $f$ , $D$ , $c$ , $s$ , $S$ , respectively ) and that node is an element of its top - level validation results .\n","NOUN PHRASES:\n"," ['results structure', 'level validation result', 'D', 'S', 'D', 'S', 'graph', 'contains', 'node', 'validation result', 'D', 'S', 'D', 'S', 'node', 'element', 'level validation results']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A {\\bf validation report } $V$ for a results structure $R$ is any results graph containing at least the RDF triples of $R$ plus a new blank node $n$ with { \\tt sh:ValidationReport} as a value for {\\tt rdf:type} in $V$ ; {\\tt \" false \"\\string ^\\string ^ xsd :boolean } as sole value for {\\tt sh:conforms } in $V$ if $R$ has any top - level validation results in $R$ and {\\tt \" true \"\\string ^\\string ^ xsd :boolean } otherwise ; and whose set of values for {\\tt sh:result } in $V$ is the set of top - level validation results in $R$ .\n","NOUN PHRASES:\n"," ['V', 'results structure', 'R', 'results', 'graph containing', 'RDF triples', 'R', 'ValidationReport', 'value', 'type', 'V', '^ xsd', 'sole value', 'conforms', 'V', 'R', 'has', 'level validation results', 'R', '^ xsd', 'set', 'values', 'result', 'V', 'set', 'level validation results', 'R']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 7, 'end': 24, 'text': 'validation report'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 35, 'end': 54, 'text': 'a results structure'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 128, 'end': 144, 'text': 'a new blank node'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If $s$ is a shape in an RDF graph $G$ with more than one value for { \\tt sh : severity } in $G$ then $s$ is an { \\bf ill - formed shape } in $G$ .\n","The { \\bf severity } of a shape in a shapes graph $S$ is its value for { \\tt sh:severity} in            , if any, otherwise {\\tt sh:Violation}\n","NOUN PHRASES:\n"," ['shape', 'RDF graph', 'G', 'value', 'severity', 'G', 'ill', 'formed', 'shape', 'G', 'shape', 'S', 'value', 'severity', 'Violation']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 10, 'end': 17, 'text': 'a shape'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 21, 'end': 33, 'text': 'an RDF graph'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 108, 'end': 144, 'text': 'an { \\\\bf ill - formed shape } in $G$'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 182, 'end': 196, 'text': 'a shapes graph'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A { \\bf path } in an RDF graph $G$ from RDF term $n$ to RDF term $m$ is a finite sequence of RDF triples in $G$ such that the subject of the first RDF triple is $n$ , the object of the last RDF triple is $m$ , and the object of each RDF triple except the last is the subject of the next .\n","NOUN PHRASES:\n"," ['RDF graph', 'G', 'RDF term', 'RDF term', 'finite sequence', 'RDF triples', 'G', 'subject', 'first RDF triple', 'object', 'last RDF triple', 'object', 'RDF triple', 'subject']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 18, 'end': 30, 'text': 'an RDF graph'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 40, 'end': 48, 'text': 'RDF term'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 56, 'end': 64, 'text': 'RDF term'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An {\\bf ASHACL list } in an RDF graph $G$ is an IRI or a \\href{https://www.w3.org/TR/rdf11-concepts/#section-blank-nodes} {\\em blank node } that is either { \\tt rdf:nil} (provided that {\\tt rdf:nil} has no value for either {\\tt rdf : first } or { \\tt rdf:rest} in            ), or has exactly one value for {\\tt rdf:first} in $G$ and exactly one value for {\\tt rdf : rest } in $G$ that is also an ASHACL list in $G$ and there is no non-empty path in $G$ from the list back to itself where the predicates of the RDF triples in the path are each {\\tt rdf : rest } .\n","NOUN PHRASES:\n"," ['RDF graph', 'G', 'IRI', 'https', 'section-blank-nodes', 'nil', 'provided', 'nil', 'has', 'value', 'rest', 'has', 'value', 'G', 'value', 'rest', 'G', 'ASHACL list', 'G', 'non-empty path', 'G', 'list', 'predicates', 'RDF triples', 'path', 'rest']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If $s$ is a shape in an RDF graph $G$ with a value for { \\tt sh : message } in $G$ that is not either a language - tagged string or a \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-literal} { \\em literal } with \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-datatype-iri} { \\em datatype} {\\tt xsd:string} then $s$ is an {\\bf ill - formed shape } in $G$ .\n","NOUN PHRASES:\n"," ['shape', 'RDF graph', 'G', 'value', 'message', 'G', 'language', 'tagged', 'string', 'https', 'https', 'string', 'ill', 'formed', 'shape', 'G']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 10, 'end': 17, 'text': 'a shape'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 21, 'end': 33, 'text': 'an RDF graph'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 319, 'end': 354, 'text': 'an {\\\\bf ill - formed shape } in $G$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An RDF term $n$ has { \\bf value } $v$ for \\href{https://www.w3.org/TR/sparql11-query/#pp-language} { \\em SPARQL property path expression } $p$ in an RDF graph $G$ if there is a solution mapping in the result of the SPARQL query { \\tt SELECT ?s ?o WHERE \\{ ?s            ?o \\} } on $G$ that binds { \\tt ?s } to $n$ and { \\tt ?o } to $v$ , where $p'$ is SPARQL surface syntax for $p$ .\n","NOUN PHRASES:\n"," ['RDF term', 'has', 'https', 'pp-language', 'RDF graph', 'G', 'solution mapping', 'result', 'SPARQL query', 'SELECT', 's', 's', 'G', 'binds', 's', 'o', 'p', 'SPARQL surface syntax']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 146, 'end': 158, 'text': 'an RDF graph'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 0, 'end': 11, 'text': 'An RDF term'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 26, 'end': 31, 'text': 'value'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 105, 'end': 136, 'text': 'SPARQL property path expression'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 352, 'end': 381, 'text': 'SPARQL surface syntax for $p$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-rdf-term} {\\ em RDF term } $n$ has {\\ bf value } $v$ for property $p$ in an \\href{https://www.w3.org/TR/rdf11-concepts/#section-rdf-graph} {\\ em RDF graph } $G$ if there is an \\href{https://www.w3.org/TR/rdf11-concepts/#section-triples} {\\ em RDF triple } in $G$ with \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-subject} {\\ em subject } $n$ , \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-predicate} {\\ em predicate } $p$ , and \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-object} {\\ em object } $v$ .\n","NOUN PHRASES:\n"," ['https', 'has', 'bf', 'value', 'property', 'https', 'G', 'https', 'section-triples', 'G', 'https', 'https', 'https']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The { \\bf members} of any ASHACL list except {\\tt rdf:nil} in an RDF graph $G$ consist of its value for { \\tt rdf : first } in $G$ followed by the members in $G$ of its value for { \\tt rdf:rest} in            . The ASHACL list {\\tt rdf:nil} has no {\\bf members } in any RDF graph .\n","NOUN PHRASES:\n"," ['ASHACL list', 'nil', 'RDF graph', 'G', 'consist', 'value', 'G', 'followed', 'members', 'G', 'value', 'rest', 'ASHACL list', 'nil', 'has', 'RDF graph']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An RDF term $n$ is an { \\bf ASHACL instance } of an RDF term $m$ in an RDF graph $G$ if there is a path in $G$ from $n$ to $m$ where the predicate of the first RDF triple in the path is { \\tt rdf : type } and the predicates of any other RDF triples in the path are { \\tt rdfs : subClassOf } .\n","NOUN PHRASES:\n"," ['RDF term', 'RDF term', 'RDF graph', 'G', 'path', 'G', 'm', 'predicate', 'first RDF triple', 'path', 'type', 'predicates', 'other RDF triples', 'path', 'subClassOf']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For a constraint component $C$ with mandatory parameters $p_1$ , ... , $p_n$ , a shape s in a shapes graph $S$ has a { \\ bf constraint } that has kind $C$ with { \\ bf mandatory parameter values } $\\left<p_1,v_1\\right>$ , \\ ldots , $\\left<p_n,v_n\\right>$ in $S$ when $s$ has $v_i$ as a value for $p_i$ in $S$ .\n","If $s$ in $S$ has a constraint that has kind $C$ in $S$ then the { \\bf optional parameter values } of the constraint in $S$ are all the $\\left<o_i,v_i\\right>$ where $o_i$ is an optional parameter of $C$ and $s$ has $v_i$ as a value for $o_i$ in $S$ .\n","The { \\ bf parameter values } of a constraint are its mandatory parameter values plus its optional parameter values .\n","NOUN PHRASES:\n"," ['constraint', 'C', 'mandatory parameters', 'shape s', 'S', 'has', 'has', 'kind', 'C', '< p_n', 'S', 'has', 'value', 'S', 'S', 'has', 'constraint', 'has', 'kind', 'C', 'S', 'constraint', 'S', 'optional parameter', 'C', 'has', 'value', 'S', 'constraint', 'mandatory parameter values', 'optional parameter values']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 4, 'end': 26, 'text': 'a constraint component'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 36, 'end': 56, 'text': 'mandatory parameters'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 79, 'end': 86, 'text': 'a shape'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 92, 'end': 106, 'text': 'a shapes graph'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 146, 'end': 150, 'text': 'kind'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 167, 'end': 193, 'text': 'mandatory parameter values'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 283, 'end': 290, 'text': 'a value'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 350, 'end': 354, 'text': 'kind'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A blank node is an { \\ bf ill - formed property path } in an RDF graph $G$ if it has a value for more than one of either { \\tt rdf:first} or {\\tt rdf:rest} , % % CHANGE { \\tt sh:alternativePath}, {\\tt sh:inversePath} , { \\tt sh:zeroOrMorePath}, {\\tt sh:oneOrMorePath} , and { \\tt sh : zeroOrOnePath } in $G$ .\n","NOUN PHRASES:\n"," ['blank node', 'formed', 'property path', 'RDF graph', 'G', 'has', 'value', 'rest', '% % CHANGE', 'alternativePath', 'inversePath', 'zeroOrMorePath', 'oneOrMorePath', 'zeroOrOnePath', 'G']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An RDF term $n$ has an RDF term $m$ as { \\bf ASHACL type } in an RDF graph $G$ if $n$ is an ASHACL instance of $m$ in $G$ .\n","NOUN PHRASES:\n"," ['RDF term', 'has', 'RDF term', 'RDF graph', 'G', 'ASHACL instance', 'G']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If $s$ is a shape in an RDF graph $G$ with a value for { \\tt sh : deactivated } in $G$ that is not a literal with datatype { \\tt xsd : boolean } then $s$ is an { \\bf ill - formed shape } in $G$ .\n","NOUN PHRASES:\n"," ['shape', 'RDF graph', 'G', 'value', 'deactivated', 'G', 'boolean', 'ill', 'formed', 'shape', 'G']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 10, 'end': 17, 'text': 'a shape'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 21, 'end': 33, 'text': 'an RDF graph'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 157, 'end': 193, 'text': 'an { \\\\bf ill - formed shape } in $G$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent\\fbox{\\begin{minipage}\n","{\\ dimexpr\\ line width - 2\\fboxrule - 2\\fboxsep } {\\footnotesize \\tt \\begin{enumerate}\n","[ itemsep= - 1 mm , label= \\textbf{\\arabic*.} ]\n","\\item Haiku corpus $\\rightarrow$ POS tagger $\\rightarrow$ grammatical skeleton fragments . \\item General text corpus $\\rightarrow$ n - gram model . \\ item General text corpus $\\rightarrow$ topic vectors . \\item Combine skeleton fragments to make a haiku template . \\item Assign syllable counts to slots . \\ item Fill in the template , preferring n - grams and close topic matches .\n","\\end{enumerate} } \\end{minipage} }\n","NOUN PHRASES:\n"," ['minipage', 'enumerate', '[', 'mm', 'corpus', 'POS tagger', 'grammatical skeleton fragments', 'General text corpus', 'gram model', 'topic vectors', 'make', 'haiku template', 'Assign syllable counts', 'slots', 'template', 'preferring', 'grams', 'close topic matches', 'enumerate', 'minipage']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table} [ tp ] \\centering \\begin{tabular} { | l | l | l | l | } \\hline & Inputs & Outputs & Total \\\\ \\hline\n","Number of linked entities & 255, 101 & 4,467 & 259,568 \\\\ \\hline\n","Number of different DBpedia types linked & 8,453 & 3,439 & 10,166 \\\\ \\hline Precision & 96\\% ( $P_{I}$ ) & 98.3 \\% ( $P_{O}$ ) & 96\\% ( $P_{I+O}$ ) \\\\ \\hline \\end{tabular}  \\caption{Results of the DBpedia integration experiment}  \\label{tab:statisticsDBpediaIntegration}  \\end{table}\n","NOUN PHRASES:\n"," ['[ tp ]', '| l | l | l | l |', 'Inputs', 'Outputs', 'linked', 'entities', 'different DBpedia types', 'linked', '%', 'P_', '%', 'P_', 'O', '%', 'P_', 'I+O', 'Results', 'DBpedia integration experiment', 'tab', 'statisticsDBpediaIntegration', 'table']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Total precision of the links & 70.7 \\% & 87.3\\% & 88.1\\% \\\\ \\hline\n","Total number of links & 106,056 & 221,351 & 376,795 \\\\ \\hline\n","Total coverage of the links & 45,999 ( 27.5\\% ) & 84,350 ( 50.4\\% ) & 114,166 ( 53.9\\% ) \\\\ \\hline\n","\\end{tabular}  \\caption{Comparison of the wikiHow community integration (\\textbf{WH-C} ) with the results of our integration of wikiHow ( \\textbf{WH} ) and of both wikiHow and Snapguide ( \\textbf{WH+S} ) } \\label{tab:finalcomparison}  \\end{table}\n","The result of this comparison can be seen in Table \\ref{tab:finalcomparison} .\n","For each of the two types of links generated by the wikiHow community ( \\textbf{WH-C} ) , the precision has been evaluated manually on 200 randomly selected links .\n","The precision of the I/O links generated by our system ( \\textbf{WH+S} ) is defined as the probability that both the input and the output links involved are correct : $P_{I/O}=P_{I}*P_{O}$ .\n","As such , it can be derived from the precision values shown in Table \\ref{tab:statisticsDBpediaIntegration} .\n","The precision of the decomposition links generated by our system ( \\textbf{WH+S} ) is determined by the precision of the classifier used to select them .\n","This precision was evaluated using 10 - fold cross validation .\n","\\par It should be noted that the links generated by the wikiHow community only interlink wikiHow resources. On the contrary, our system integrates procedural knowledge both from the wikiHow and the Snapguide repositories. To make a fair comparison, Table \\ref{tab:finalcomparison} also shows the evaluation of the links generated by our system which only connect wikiHow resources ( \\textbf{WH} ) .\n","\\par\n","The result of our evaluation shows how our automatic approach to human know - how integration significantly outperforms manual community - based integration efforts .\n","This is shown for all the metrics considered and for both types of links .\n","This result demonstrates how our framework can be used to significantly increase the value of human know - how by automatic means .\n","We take this result as strong evidence for the effectiveness of our integration framework .\n","NOUN PHRASES:\n"," ['Total precision', 'links', '%', '%', 'links', 'links', '%', '%', '%', 'Comparison', 'wikiHow community integration', 'results', 'integration', 'wikiHow', 'WH', 'wikiHow', 'Snapguide', 'WH+S', 'tab', 'finalcomparison', 'result', 'comparison', 'tab', 'finalcomparison', 'types', 'links', 'generated', 'wikiHow community', 'precision', 'randomly selected', 'links', 'precision', 'I/O links', 'generated', 'system', 'WH+S', 'probability', 'input', 'output links', 'P_', 'I/O', '=P_', 'O', 'precision values', 'shown', 'tab', 'statisticsDBpediaIntegration', 'precision', 'decomposition links', 'generated', 'system', 'WH+S', 'precision', 'classifier', 'used', 'select', 'precision', 'fold cross validation', 'links', 'generated', 'wikiHow community', 'only interlink', 'wikiHow resources', 'system', 'integrates', 'procedural knowledge', 'wikiHow', 'Snapguide repositories', 'make', 'fair comparison', 'tab', 'finalcomparison', 'also shows', 'evaluation', 'links', 'generated', 'system', 'only connect', 'wikiHow resources', 'WH', 'result', 'evaluation', 'shows', 'automatic approach', 'human know', 'integration', 'significantly outperforms', 'manual community', 'based', 'integration efforts', 'metrics', 'considered', 'types', 'links', 'result', 'demonstrates', 'framework', 'significantly increase', 'value', 'human', 'know', 'automatic means', 'take', 'result', 'strong evidence', 'effectiveness', 'integration framework']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 806, 'end': 883, 'text': 'the probability that both the input and the output links involved are correct'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 719, 'end': 749, 'text': 'The precision of the I/O links'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\par After extracting the Linked Data representation of a large number of processes, we applied our Linked Data integration system. Our system followed the method described in section \\ref{sec:DBpediaLinks} to discover the links between DBpedia entities and the inputs and the outputs of the processes .\n","The results of this experiment can be found in Table \\ref{tab:statisticsDBpediaIntegration} .\n","The precision was manually evaluated separately for the inputs ( $P_{I}$ ) and the outputs ( $P_{O}$ ) on 300 randomly selected links for each type .\n","A link was considered wrong ( 1 ) if it linked an entity which was not an input or an output of the process or ( 2 ) if the type of the input or output did not correspond to the linked DBpedia type .\n","NOUN PHRASES:\n"," ['extracting', 'Linked', 'Data representation', 'large number', 'processes', 'applied', 'Linked', 'Data integration system', 'system', 'followed', 'method', 'described', 'sec', 'DBpediaLinks', 'discover', 'links', 'DBpedia entities', 'inputs', 'outputs', 'processes', 'results', 'experiment', 'tab', 'statisticsDBpediaIntegration', 'precision', 'manually evaluated', 'inputs', 'P_', 'outputs', 'P_', 'O', 'randomly selected', 'links', 'type', 'link', 'linked', 'entity', 'input', 'output', 'process', 'type', 'input', 'output', 'did', 'not correspond', 'linked', 'DBpedia type']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 398, 'end': 411, 'text': 'The precision'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In section \\ref{relatedWorkKnowledgeRepresentation} we discussed several issues in reusing existing knowledge representation languages in the human know - how domain .\n","This lead us to the development of a Linked Data vocabulary which is both lightweight and generic \\cite{Pareti2014} .\n","This vocabulary is sufficient to represent the two main concepts that can be reliably extracted from semi-structured human know - how .\n","These concepts , namely dependencies and process decompositions , play a key role in most procedural knowledge representation formalisms .\n","This vocabulary is based on just three properties , as shown in Table \\ref{tab:vocabulary} .\n","\\begin{table}\n","[ tp ] \\centering \\begin{tabular}\n","{ | l | l | } \\hline\n","Prefix & Namespace \\\\ \\hline prohow: & \\url{http://vocab.inf.ed.ac.uk/prohow#} \\\\ \\hline \\hline\n","Term & Definition when $X$ is the subject and $Y$ is the object \\\\ \\hline \\url{prohow:has_step} & $Y$ can help accomplishing / obtaining\n","$X$ \\\\ \\hline\n","\\url{prohow:has_method} & $Y$ can be accomplished / obtained instead of $X$ \\\\ \\hline\n","\\url{prohow:requires} & $Y$ should be accomplished / obtained before doing\n","$X$ \\\\ \\hline\n","\\end{tabular}  \\caption{The vocabulary to represent processes}  \\label{tab:vocabulary}  \\end{table}\n","NOUN PHRASES:\n"," ['relatedWorkKnowledgeRepresentation', 'discussed', 'several issues', 'reusing existing', 'knowledge representation', 'languages', 'human', 'domain', 'development', 'Linked', 'Data vocabulary', 'Pareti2014', 'represent', 'main concepts', 'reliably extracted', 'semi-structured human', 'know', 'concepts', 'dependencies', 'process decompositions', 'play', 'key role', 'procedural knowledge representation formalisms', 'vocabulary', 'properties', 'shown', 'tab', '[ tp ]', '| l | l |', 'http', '//vocab.inf.ed.ac.uk/prohow', 'Definition', 'X', 'subject', 'Y', 'prohow', 'has_step', 'Y', 'help accomplishing / obtaining', 'X', 'prohow', 'has_method', 'Y', '/', 'obtained', 'X', 'prohow', 'requires', 'Y', '/', 'obtained', 'doing', 'X', 'vocabulary', 'represent', 'processes', 'tab']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 849, 'end': 860, 'text': 'the subject'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 872, 'end': 882, 'text': 'the object'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Conduction Cost}\n","A pure double elimination system with $i$ upper rounds has $2^i$ participants , and there will be $(2^i-1)+(2^{i-1}-1)$ matches conducted .\n","Thus , a double elimination tournament for 8 participants will have 10 matches conducted .\n","NOUN PHRASES:\n"," ['Conduction Cost', 'A pure double elimination system', 'upper rounds', 'has', 'participants', '+', '-1', 'matches', 'conducted', 'double elimination tournament', 'participants', 'have', 'matches', 'conducted']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 74, 'end': 86, 'text': 'upper rounds'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 97, 'end': 109, 'text': 'participants'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 152, 'end': 159, 'text': 'matches'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Even with increased number of matches conducted compared to single elimination , it is still very lacking in the ranking precision .\n","However , double elimination has two starting nodes ( see Figure ~ \\ref{fig:DEDT} ) .\n","It can be argued that double elimination is designed for a multi-stage system , or the players are distributed properly by using a rating system \\cite{elo,5446231,trueskilltm-a-bayesian-skill-rating-system,trueskill-through-time-revisiting-the-history-of-chess,5283063,5671406} .\n","Thus , it is expected to have stronger participants and weaker participants distributed ( seeded ) into upper bracket and lower bracket properly .\n","We perform another experiment this way , as a seeded double elimination .\n","There will be $4! * 4! = 576$ cases this time .\n","NOUN PHRASES:\n"," ['increased number', 'matches', 'conducted compared', 'single elimination', 'still very lacking', 'ranking precision', 'double elimination', 'has', 'starting', 'nodes', 'see', 'fig', 'DEDT', 'double elimination', 'multi-stage system', 'players', 'using', 'elo,5446231', 'trueskilltm-a-bayesian-skill-rating-system', 'have', 'participants', 'participants', 'distributed', 'seeded', 'upper bracket', 'bracket', 'perform', 'experiment', 'way', 'seeded double elimination', '*', '=', 'cases', 'time']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Conduction Cost}\n","A pure single elimination system with $i$ rounds has $n = 2^i$ participants , and there will be $n-1$ matches conducted .\n","For 8 players single elimination , there would be 7 matches with 3 rounds .\n","NOUN PHRASES:\n"," ['Conduction Cost', 'A pure single elimination system', 'rounds has', 'n =', 'participants', 'matches', 'conducted', 'players single elimination', 'matches', 'rounds']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 74, 'end': 80, 'text': 'rounds'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 95, 'end': 107, 'text': 'participants'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 134, 'end': 141, 'text': 'matches'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," With the consistent prizes in the same unit , we would be able to evaluate nodes in the progress tree .\n","Since we are only considering the structure of the tournament , we evaluate a node as the average value of its direct child nodes .\n","For example , in Figure ~ \\ref{fig:SEDT} tournament , let $x_1$ , $x_2$ , $x_3$ and $x_4$ be 1st place prize , 2nd prize , 3rd - 4th prize and 5th - 8th prize .\n","Then we have $x1 \\geq x2 \\geq x3 \\geq x4$ .\n","Table ~ \\ref{tab:SEEV} shows the evaluation of other nodes .\n","We call this value \" stability \" value .\n","\\begin{table} [ h ]            }            \\centering            {l c }            } &            }\\\\ \\hline\\\\\n","Final ( $v$ ) &             \\\\\\\\ Semi-final ( $v_1$ ) &             \\\\\\\\\n","Quarter- final ( $v_2$ ) & $\\frac{v_1 + x_4}{2}$\n","\\end{tabular}  \\end{table}\n","NOUN PHRASES:\n"," ['consistent prizes', 'same unit', 'evaluate', 'nodes', 'progress tree', 'only considering', 'structure', 'tournament', 'evaluate', 'node', 'average value', 'direct child nodes', 'example', 'fig', 'SEDT', 'tournament', 'let', 'x_1', 'place prize', 'prize', 'prize', 'prize', 'have', 'tab', 'SEEV', 'shows', 'evaluation', 'other nodes', 'call', 'value', 'stability', 'value', '[ h ]', 'l c', 'v_1 + x_4']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 329, 'end': 344, 'text': '1st place prize'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 347, 'end': 356, 'text': '2nd prize'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 359, 'end': 374, 'text': '3rd - 4th prize'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 379, 'end': 394, 'text': '5th - 8th prize'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Conducting Cost}\n","A pure round - robin system with $n$ participants has $\\frac{n}{2}(n-1)$ matches conducted .\n","Thus for 8 participants , there would be 28 matches .\n","NOUN PHRASES:\n"," ['Conducting', 'Cost', 'A pure round', 'robin system', 'participants', 'has', 'matches', 'conducted', 'Thus', 'participants', 'matches']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 69, 'end': 81, 'text': 'participants'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 105, 'end': 112, 'text': 'matches'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We first show that for any set atom $C$ with arity $n$ ( i.e. , the number of sets involved in the atom ) and set $S$ of atoms , if $C$ is satisfied by $A$ , there is a minimal support for $C$ in $A$ .\n","Let the sets occurring in $C$ be $S_1, \\ldots, S_n$ where $S_i = \\{X_i: condition_i(X_i)\\}$ .\n","For $i \\in 1..n$ , let $T_i = \\{condition_i(t): condition_i(t) \\in A\\}$ .\n","Clearly $\\bar{T}^n$ satisfies the first two conditions in the definition of { \\em minimal support } .\n","By reducing $T_i$ if necessary ( ?? what if $T_i$ is infinite ?? ) , we can always find a minimal support for $C$ in $A$ .\n","NOUN PHRASES:\n"," ['first show', 'set', 'C', 'i.e', 'number', 'sets', 'involved', 'atom', 'set', 'S', 'atoms', 'C', 'minimal support', 'C', 'Let', 'sets', 'occurring', 'C', 'S_1', 'S_n', 'X_i', 'condition_i', 'X_i', 'let', 'condition_i', 't', 'condition_i', 't', 'Clearly', 'T', '^n', 'satisfies', 'conditions', 'definition', 'reducing', 'T_i', 'T_i', 'always find', 'minimal support', 'C']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 27, 'end': 35, 'text': 'set atom'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 45, 'end': 50, 'text': 'arity'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 64, 'end': 103, 'text': 'the number of sets involved in the atom'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 78, 'end': 103, 'text': 'sets involved in the atom'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 110, 'end': 113, 'text': 'set'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 206, 'end': 231, 'text': 'the sets occurring in $C$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Let $R_{SS}(R_I(\\Pi, A), A)$ be a Slog + set reduction ( by replacing any set atom $C$ by a minimal support for $C$ in $A$ ) of the set introduction reduct of $\\Pi$ wrt $A$ .\n","NOUN PHRASES:\n"," ['Let', 'R_', 'SS', 'R_I', 'A', 'A', 'Slog +', 'set', 'reduction', 'replacing', 'set', 'C', 'minimal support', 'C', 'A', 'set introduction reduct']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 32, 'end': 54, 'text': 'a Slog + set reduction'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 74, 'end': 82, 'text': 'set atom'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition}\n","[ Rule Satisfaction and Supportedness ]\n","\\label{p1aa}\n","Let $A$ be an $\\mathcal{A}log$ or $\\mathcal{S}log^+$ answer set of a ground program $\\Pi$ .\n","Then \\begin{itemize} \\ item $A$ satisfies every rule $r$ of            . \\ item\n","If $p(\\bar{t}) \\in A$ then there is a rule $r$ from $\\Pi$ such that the body of $r$ is satisfied by $A$ and            \\ item $p(\\bar{t})$ is the only atom in the head of $r$ which is true in $A$ or \\ item the head of $r$ % contains an is of the form $p \\odot \\{\\bar{X} : q(\\bar{X})\\}$ and $q(\\bar{t}) \\in A$ .\n","( It is often said that rule $r$ supports atom $p$ . )\n","\\end{itemize}  \\end{itemize}  \\end{proposition}\n","NOUN PHRASES:\n"," ['proposition', '[ Rule Satisfaction', 'p1aa', 'Let', 'S', 'log^+', 'set', 'ground program', 'itemize', 'item', 'satisfies', 'rule', 'r', 'p', 't', 'rule', 'body', 'A', 'item', 'p', 't', 'only atom', 'head', 'r', 'item', 'head', '%', 'contains', 'X', 'q', 'X', 'q', 't', 'often said', 'rule', 'r', 'supports', 'atom', 'itemize', 'itemize', 'proposition']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 84, 'end': 162, 'text': 'an $\\\\mathcal{A}log$ or $\\\\mathcal{S}log^+$ answer set of a ground program $\\\\Pi$'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 140, 'end': 156, 'text': 'a ground program'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 213, 'end': 217, 'text': 'rule'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 283, 'end': 287, 'text': 'rule'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 580, 'end': 584, 'text': 'rule'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 598, 'end': 602, 'text': 'atom'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","A \\emph{set atom} of $\\mathcal{A}log$ is an expression of the form\n","\\begin{equation}\\label{aggr-atom1}\n","f_1(S_1) \\odot f_2(S_2)\n","\\end{equation}\n","or\n","\\begin{equation}\\label{aggr-atom2}\n","f(S) \\odot k\n","\\end{equation}\n","where $f$ , $f_1,f_2$ are functions from ${\\cal A}$ , $S$ , $S_1$ , $S_2$ are set names , $k$ is a number , and $\\odot$ is an arithmetic relation $>, \\geq, <, \\leq,=$ or ! = , or of the form\n","\\begin{equation}\\label{set-atom}\n","S_1 \\otimes S_2\n","\\end{equation}\n","where $\\otimes$ is $\\subset, \\subseteq$ , or $=$ .\n","We often write $f(\\{\\bar{X}:p(\\bar{X})\\})$ as $f\\{\\bar{X}:p(\\bar{X})\\}$ and $\\{\\bar{X} : p(\\bar{X})\\} \\otimes S$ and $S \\otimes \\{\\bar{X} : p(\\bar{X})\\}$ as $p \\otimes S$ and $S \\otimes p$ respectively .\n","Regular and set atoms are referred to as \\emph{atoms} .\n","A \\emph{rule} of $\\mathcal{A}log$ is an expression of the form\n","\\begin{equation}\\label{rule}\n","head \\leftarrow body%pos,neg,pos\\_set,neg\\_set\n","\\end{equation}\n","where $head$ is a disjunction of regular literals or a set atom of the form $p \\subseteq S$ , $S \\subseteq q$ , or $p = S$ , and $body$ is a collection of regular literals ( possibly preceded by $not$ ) and set atoms .\n","A rule with set atom in the head is called \\emph{set introduction rule} .\n","Note that both head and body of a rule can be infinite .\n","All parts of $\\mathcal{A}log$ rules , including $head$ , can be empty .\n","A \\emph{program} of $\\mathcal{A}log$ is a collection of $\\mathcal{A}log$ 's rules .\n","NOUN PHRASES:\n"," ['set', 'atom', 'expression', 'form', 'equation', 'f_1', 'S_1', 'f_2', 'S_2', 'equation', 'equation', 'f', 'S', 'equation', 'f_1', 'functions', 'S', 'S_1', 'S_2', 'names', 'number', 'arithmetic relation', '<', '=', '=', 'form', 'equation', 'equation', 'often write', 'f', 'p', 'X', 'p', 'X', 'p', 'X', 'S', 'X', 'p', 'X', 'p', 'set atoms', 'atoms', 'rule', 'expression', 'form', 'equation', 'rule', 'head', 'body % pos', 'equation', 'head', 'disjunction', 'regular literals', 'set atom', 'p = S', 'collection', 'regular literals', 'possibly preceded', 'set', 'atoms', 'rule', 'set atom', 'head', 'set', 'introduction rule', 'Note', 'head', 'body', 'rule', 'parts', 'rules', 'including', 'program', 'collection', 'rules']\n","EXPECTED:\n"," {'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 244, 'end': 269, 'text': 'functions from ${\\\\cal A}$'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 296, 'end': 305, 'text': 'set names'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 315, 'end': 323, 'text': 'a number'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 341, 'end': 363, 'text': 'an arithmetic relation'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 954, 'end': 987, 'text': 'a disjunction of regular literals'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 1077, 'end': 1154, 'text': 'a collection of regular literals ( possibly preceded by $not$ ) and set atoms'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Let $\\Sigma$ be a ( possibly sorted ) signature with a finite collection of predicate and function symbols and ( possibly infinite ) collection of object constants , and let $\\mathcal{A}$ be a finite collection of symbols used to denote functions from sets of terms of $\\Sigma$ into integers .\n","Terms and literals over signature $\\Sigma$ are defined as usual and referred to as \\emph{regular} .\n","Regular terms are called \\emph{ground} if they contain no variables and no occurrences of symbols for arithmetic functions .\n","Similarly for literals .\n","We refer to an expression\n","\\begin{equation}\\label{set-name}\n","\\{\\bar{X}:cond\\}\n","\\end{equation}\n","where $cond$ is a finite collection of regular literals and $\\bar{X}$ is the list of variables occurring in $cond$ , as a \\emph{set name} .\n","It is read as \\ emph { the set of all objects of the program believed to satisfy $cond$ } .\n","Variables from $\\bar{X}$ are often referred to as { \\em set variables } .\n","An occurrence of a set variable in ( \\ref{set-name} ) is called \\emph{bound} within ( \\ref{set-name} ) .\n","Since treatment of variables in extended $\\mathcal{A}log$ is the same as in the original language we limit our attention to programs in which every occurrence of a variable is bound .\n","Rules containing non-bound occurrences of variables are considered as shorthands for their ground instantiations ( for details see \\cite{GelfondZ14} ) .\n","NOUN PHRASES:\n"," ['Let', 'possibly sorted', 'signature', 'finite collection', 'predicate', 'function symbols', 'collection', 'object constants', 'let', 'finite collection', 'symbols', 'used', 'denote', 'functions', 'sets', 'terms', 'integers', 'Terms', 'literals', 'referred', 'Regular terms', 'ground', 'contain', 'variables', 'occurrences', 'symbols', 'arithmetic functions', 'literals', 'refer', 'equation', 'X', 'equation', 'cond', 'finite collection', 'regular literals', 'X', 'list', 'variables', 'occurring', 'set', 'name', 'set', 'objects', 'program', 'believed', 'satisfy', 'Variables', 'X', 'often referred', 'occurrence', 'set variable', 'bound', 'treatment', 'variables', 'original language', 'limit', 'attention', 'programs', 'occurrence', 'Rules', 'containing', 'non-bound occurrences', 'variables', 'shorthands', 'ground instantiations', 'details', 'see', 'GelfondZ14']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 16, 'end': 47, 'text': 'a ( possibly sorted ) signature'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 191, 'end': 221, 'text': 'a finite collection of symbols'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 318, 'end': 327, 'text': 'signature'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 651, 'end': 690, 'text': 'a finite collection of regular literals'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 708, 'end': 749, 'text': 'the list of variables occurring in $cond$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section we give some basic properties of $\\mathcal{A}log$ and $\\mathcal{S}log^+$ programs .\n","Propositions \\ref{p1aa} and \\ref{p2aa} ensure that , as in regular ASP , answer sets of $\\mathcal{A}log$ program are formed using the program rules together with the rationality principle .\n","Proposition \\ref{split} is the $\\mathcal{A}log$ / $\\mathcal{S}log^+$ version of the Splitting Set Theorem -- basic technical tool used in theoretical investigations of ASP and its extensions \\cite{GelfondP92,lt94,Turner96} .\n","NOUN PHRASES:\n"," ['section', 'give', 'basic properties', 'S', 'log^+', 'programs', 'Propositions', 'p1aa', 'p2aa', 'ensure', 'regular ASP', 'sets', 'program', 'program rules', 'rationality principle', 'split', 'S', 'log^+', 'version', 'Splitting Set Theorem', 'basic technical tool', 'used', 'theoretical investigations', 'ASP', 'extensions', 'GelfondP92', 'lt94', 'Turner96']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To define the semantics of $\\mathcal{A}log$ programs we first notice that the \\emph{standard definition of answer set from \\cite{gl91} is applicable to programs with infinite rules } .\n","Hence we already have the definition of answer set for $\\mathcal{A}log$ programs not containing occurrences of set atoms .\n","We also need the satisfiability relation for set atoms .\n","Let $A$ be a set of ground regular literals .\n","If $f(\\{\\bar{t}:cond(\\bar{t}) \\subseteq A\\})$ is defined then $f(\\{\\bar{X}:cond\\}) \\geq k$ is satisfied by $A$ ( is \\emph{true} in $A$ ) iff $f(\\{\\bar{t}:cond(\\bar{t}) \\subseteq A\\}) \\geq k$ .\n","Otherwise , $f(\\{\\bar{X}:cond\\}) \\geq k$ is falsified ( is \\emph{false} in $A$ ) .\n","If $f(\\{\\bar{t}:cond(\\bar{t})\n","\\subseteq A\\})$ is not defined then $f(\\{\\bar{X}:cond\\}) \\geq k$ is \\emph{undefined} in $A$ .\n","( For instance , atom $card\\{X:p(X)\\} \\geq 0$ is undefined in $A$ if $A$ contains an infinite collection of atoms formed by $p$ . )\n","Similarly for other set atoms .\n","Finally a rule is \\emph{satisfied} by $S$ if its head is \\emph{true} in $S$ or its body is \\emph{false} or \\emph{undefined} in $S$ .\n","\\subsubsection{Answer Sets for Programs without Set Introduction Rules.}\n","To simplify the presentation we first give the definition of answer sets for programs whose rules contain no set atoms in their heads .\n","First we need the following definition :\n","\\begin{definition} [ Set Reduct of $\\mathcal{A}log$ ] \\label{reduct1}\n","Let $\\Pi$ be a ground program of $\\mathcal{A}log$ .\n","The \\emph{set reduct} of $\\Pi$ with respect to a set of ground regular literals $A$ is obtained from $\\Pi$ by \\begin{enumerate} \\ item removing rules containing set atoms which are \\emph{false} or \\emph{undefined} in $A$ . \\ item replacing every remaining set atom $SA$ by the union of $cond(\\bar{t})$ such that $\\{\\bar{X} : cond(\\bar{X})\\}$ occurs in $SA$ and $cond(\\bar{t})\n","\\subseteq A$ .\n","\\end{enumerate}  \\end{definition}\n","The first clause of the definition removes rules useless because of the truth values of their aggregates in $A$ .\n","The next clause reflects the principle of avoiding vicious circles .\n","Clearly , set reducts do not contain set atoms .\n","\\begin{definition} [ Answer Set ]\n","\\label{ans-set}\n","A set $A$ of ground regular literals over the signature of a ground $\\mathcal{A}log$ program $\\Pi$ is an \\emph{answer set} of $\\Pi$ if $A$ is an answer set of the set reduct of $\\Pi$ with respect to $A$ .\n","\\end{definition}\n","It is easy to see that for programs of the original $\\mathcal{A}log$ our definition coincides with the old one .\n","Next several examples demonstrate the behavior of our semantics for programs not covered by the original syntax .\n","NOUN PHRASES:\n"," ['define', 'semantics', 'programs', 'first notice', 'standard definition', 'answer', 'set', 'gl91', 'programs', 'infinite rules', 'Hence', 'already have', 'definition', 'answer', 'set', 'programs', 'not containing', 'occurrences', 'set atoms', 'also need', 'satisfiability relation', 'set atoms', 'Let', 'set', 'ground regular literals', 'f', 'cond', 't', 'f', 'A', 'iff', 'f', 'cond', 't', 'Otherwise', 'f', 'A', 'f', 'cond', 't', 'not defined', 'f', 'instance', 'X', 'p', 'X', 'contains', 'infinite collection', 'atoms', 'formed', 'other set atoms', 'rule', 'S', 'head', 'S', 'body', 'S', 'Answer Sets', 'Programs', 'Set Introduction Rules', 'simplify', 'presentation', 'first give', 'definition', 'sets', 'programs', 'rules', 'contain', 'set atoms', 'heads', 'First', 'need', 'following definition', 'definition', '[ Set Reduct', 'reduct1', 'Let', 'ground program', 'set', 'reduct', 'respect', 'set', 'ground regular literals', 'enumerate', 'removing', 'rules', 'containing', 'set atoms', 'replacing', 'remaining', 'set', 'SA', 'union', 'cond', 't', 'cond', 'X', 'occurs', 'SA', 'cond', 't', 'enumerate', 'definition', 'first clause', 'definition', 'removes', 'rules', 'truth values', 'aggregates', 'next clause', 'reflects', 'principle', 'avoiding', 'vicious circles', 'set', 'reducts', 'do', 'not contain', 'set atoms', 'definition', 'A', 'set', 'ground regular literals', 'signature', 'ground', 'program', 'answer', 'set', 'answer set', 'set reduct', 'respect', 'definition', 'see', 'programs', 'definition coincides', 'old one', 'Next several examples', 'demonstrate', 'behavior', 'semantics', 'programs', 'not covered', 'original syntax']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 376, 'end': 408, 'text': 'a set of ground regular literals'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 1441, 'end': 1477, 'text': 'a ground program of $\\\\mathcal{A}log$'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 1736, 'end': 1744, 'text': 'set atom'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st\n","We view this restriction as a possible interpretation of VCP and refer to it as \\emph{Strong VCP} .\n","Let us illustrate the intuition behind $\\mathcal{A}log$ set constructs .\n","\\begin{example}            %\n","[ Example \\ref{e1} revisited ]\n","{ \\rm Let us consider programs from Example \\ref{e1} .\n","$P_0$ clearly has no answer set since $\\emptyset$ does not satisfy its rule and there is no justification for believing in $p(1)$ .\n","$P_1$ is also inconsistent .\n","To see that notice that the first two rules of the program limit our possibilities to $A_1 = \\emptyset$ and $A_2=\\{p(0), p(1)\\}$ .\n","In the first case $\\{X:p(X)\\}$ denotes $\\emptyset$ .\n","But this contradicts the last rule of the program .\n","$A_1$ cannot be an answer set of $P_1$ .\n","In $A_2$ , $\\{X:p(X)\\}$ denotes $S=\\{0, 1\\}$ .\n","But this violates our form of VCP since the reasoner 's beliefs in both , $p(0)$ and $p(1)$ , cannot be established without reference to $S$ .\n","$A_2$ is not an answer set either .\n","Now consider program $P_2$ .\n","There are two candidate answer sets \\\n","footnote\n","{ By a candidate answer set we mean a consistent set of ground regular literals satisfying the rules of the program . } : $A_1 =\n","\\emptyset$ and $A_2 = \\{p(1)\\}$ .\n","In $A_1$ , $S = \\emptyset$ which contradicts the rule .\n","In $A_2$ , $S=\\{1\\}$ but this would contradict the $\\mathcal{A}log$ 's VCP .\n","The program is inconsistent \\\n","footnote\n","{ There is a common argument for the semantics in which $\\{p(1)\\}$ would be the answer set of $P_2$ :\n","`` Since $card\\{X: p(X)\\} \\geq 0$ is always true it can be dropped from the rule without changing the rule 's meaning '' .\n","But the argument assumes existence of the set denoted by $\\{X:p(X)\\}$ which is not always the case in $\\mathcal{A}log$ .}. }\n","\\end{example}\n","We hope that the examples are sufficient to show how the informal semantics of $\\mathcal{A}log$ can give a programmer some guidelines in avoiding formation of sets problematic from the standpoint of VCP .\n","In what follows we \\begin{itemize}\n","\\item\n","Expand $\\mathcal{A}log$ by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching .\n","\\item\n","Propose an alternative formalization of the original VCP and incorporate it into the semantics of new language , $\\mathcal{S}log^+$ , which allows more liberal construction of sets and their use in programming rules .\n","( The name of the new language is explained by its close relationship with language $\\mathcal{S}log$ \\cite{SonP07} -- see Theorem 2 ) .\n","\\item\n","Show that , for programs without disjunction and infinite sets , the formal semantics of aggregates in $\\mathcal{S}log^+$ coincides with that of several other known languages .\n","Their intuitive and formal semantics , however , are based on quite different ideas and seem to be more involved than that of $\\mathcal{S}log^+$ .\n","\\item\n","Prove some basic properties of programs in ( extended ) $\\mathcal{A}log$ and $\\mathcal{S}log^+$ .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['view', 'restriction', 'possible interpretation', 'VCP', 'refer', 'Strong VCP', 'Let', 'illustrate', 'intuition', 'set', 'constructs', 'example', 'e1', 'revisited', 'consider', 'programs', 'e1', 'P_0', 'clearly has', 'answer', 'set', 'does', 'not satisfy', 'rule', 'justification', 'believing', 'p', 'P_1', 'see', 'notice', 'rules', 'program limit', 'possibilities', 'p', 'p', 'first case', 'X', 'p', 'X', 'denotes', 'contradicts', 'last rule', 'program', 'answer set', 'P_1', 'A_2', 'X', 'p', 'X', 'denotes', 'violates', 'form', 'VCP', 'reasoner', 'beliefs', 'p', 'p', 'reference', 'S', 'A_2', 'answer set', 'Now consider', 'program', 'P_2', 'sets', 'candidate answer', 'set', 'mean', 'consistent set', 'ground regular literals', 'satisfying', 'rules', 'program', 'p', 'A_1', 'contradicts', 'rule', 'A_2', 'contradict', 'VCP', 'program', 'common argument', 'semantics', 'p', 'answer set', 'P_2', 'X', 'p', 'X', 'rule', 'changing', 'rule', 'meaning', 'argument', 'assumes', 'existence', 'set', 'denoted', 'X', 'p', 'X', 'case', 'example', 'hope', 'examples', 'show', 'informal semantics', 'give', 'programmer', 'guidelines', 'avoiding', 'formation', 'sets', 'standpoint', 'VCP', 'follows', 'itemize', 'allowing', 'infinite sets', 'several additional set', 'related', 'constructs', 'knowledge representation', 'teaching', 'alternative formalization', 'original VCP', 'incorporate', 'semantics', 'new language', 'S', 'log^+', 'allows', 'liberal construction', 'sets', 'use', 'programming rules', 'name', 'new language', 'close relationship', 'language', 'S', 'log', 'SonP07', 'see', 'Theorem', 'Show', 'programs', 'disjunction', 'infinite sets', 'formal semantics', 'aggregates', 'S', 'log^+', 'coincides', 'several other known', 'languages', 'formal semantics', 'different ideas', 'seem', 'S', 'log^+', 'Prove', 'basic properties', 'programs', 'extended', 'S', 'log^+', 'itemize']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 970, 'end': 977, 'text': 'program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 1000, 'end': 1021, 'text': 'candidate answer sets'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 2424, 'end': 2432, 'text': 'language'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 2229, 'end': 2241, 'text': 'new language'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In what follows we retain the name $\\mathcal{A}log$ for the new language and refer to the earlier version as `` original $\\mathcal{A}log$ '' .\n","NOUN PHRASES:\n"," ['follows', 'retain', 'name', 'new language', 'refer', 'version']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\st{\\bf Infinite Universe}\n","\\begin{example} [ Aggregates on infinite sets ]\n","\\label{new1}\n","Consider a program $E_1$ consisting of the following rules :\n","\\begin{verbatim} even ( 0 ) .\n","even ( I +2 ) :- even ( I ) .\n","q :- min { X : even ( X ) } = 0.            \\noindent\n","It is easy to see that the program has one answer set , $S_{E_1} = \\{q,even(0),even(2),\\dots\\}$ .\n","Indeed , the reduct of $E_1$ with respect to $S_{E_1}$ is the infinite collection of rules\n","\\begin{verbatim} even ( 0 ) .\n","even ( 2 ) :\n","- even ( 0 ) . ...\n","q :- even ( 0 ) , even ( 2 ) , even ( 4 ) ...\n","           \\noindent\n","The last rule has the infinite body constructed in the last step of definition \\ref{reduct1} .\n","Clearly , $S_{E_1}$ is a subset minimal collection of ground literals satisfying the rules of the reduct ( i.e. its answer set ) .\n","Hence $S_{E_1}$ is an answer set of $E_1$ .\n","\\end{example}\n","\\begin{example} [ Programs with undefined aggregates ]\n","\\label{new2}\n","Now consider a program $E_2$ consisting of the rules :\n","\\begin{verbatim} even ( 0 ) .\n","even ( I +2 ) :- even ( I ) .\n","q :- card { X : even ( X ) } > 0.           \n","NOUN PHRASES:\n"," ['example', '[ Aggregates', 'infinite sets', ']', 'new1', 'Consider', 'program', 'E_1', 'consisting', 'following rules', 'verbatim', '+2', 'q', 'min', 'X', 'X', 'see', 'program', 'has', 'answer set', 'S_', 'E_1', 'q', 'reduct', 'E_1', 'respect', 'S_', 'E_1', 'infinite collection', 'rules', 'verbatim', 'q', 'last rule', 'has', 'infinite body', 'constructed', 'last step', 'reduct1', 'Clearly', 'S_', 'E_1', 'subset minimal collection', 'ground literals', 'satisfying', 'rules', 'reduct', 'answer', 'set', 'Hence', 'S_', 'E_1', 'answer set', 'E_1', 'example', 'example', '[', 'Programs', 'undefined aggregates', ']', 'Now consider', 'program', 'E_2', 'consisting', 'rules', 'verbatim', '+2', 'q', 'card', 'X', 'X']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 97, 'end': 106, 'text': 'a program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 306, 'end': 316, 'text': 'answer set'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 699, 'end': 780, 'text': 'a subset minimal collection of ground literals satisfying the rules of the reduct'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 826, 'end': 848, 'text': 'an answer set of $E_1$'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 946, 'end': 955, 'text': 'a program'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st ( Here student is eliminated from the parameters and we are limited to only one required class , $c$ . )\n","Even though in this case the answers are correct , unprincipled use of default negation leads to some potential difficulties .\n","Suppose , for instance , that a student may graduate if given a special permission .\n","This can be naturally added as a rule\n","NOUN PHRASES:\n"," ['student', 'parameters', 'required class', 'case', 'answers', 'unprincipled use', 'default negation', 'leads', 'potential difficulties', 'Suppose', 'instance', 'student', 'graduate', 'given', 'special permission', 'naturally added', 'rule']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 94, 'end': 99, 'text': 'class'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st {\\tt ready \\_to\\_graduate(S) :- \\{C: required(C)\\} $\\subseteq$ \\{ C : taken ( S , C ) \\ } . }\n","NOUN PHRASES:\n"," ['S', 'C', 'required', 'C', 'C', 'taken', 'S', 'C']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","This program has one answer set , $S_{E_2}= \\{even(0),even(2),\\dots\\}$ .\n","Since our aggregates range over natural numbers , the aggregate $card$ is not defined on the set $card\\{t :\n","even(t) \\in S_{E_2}\\}$ .\n","This means that the body of the last rule is undefined .\n","According to clause one of definition \\ref{reduct1} this rule is removed .\n","The reduct of $E_2$ with respect to $S_{E_2}$ is \\begin{verbatim} even ( 0 ) .\n","even ( 2 ) :- even ( 0 ) .\n","even ( 4 ) :- even ( 2 ) . ... \\end{verbatim}\n","Hence $S_{E_2}$ is the answer set of $E_2$ .\n","\\ footnote\n","{ Of course this is true only because of our ( somewhat arbitrary ) decision to limit aggregates of $\\mathcal{A}log$ to those ranging over natural numbers .\n","We could , of course , allow aggregates mapping sets into ordinals .\n","In this case the body of the last rule of $E_2$ will be defined and the only answer set of $E_2$ will be $S_{E_1}$ . }\n","It is easy to check that , since every set $A$ satisfying the rules of $E_2$ must contain all even numbers , $S_{E_1}$ is the only answer set .\n","NOUN PHRASES:\n"," ['program', 'has', 'answer set', 'S_', 'E_2', 'aggregates', 'range', 'natural numbers', 'not defined', 'set', 't', 't', 'S_', 'E_2', 'means', 'body', 'last rule', 'According', 'clause', 'reduct1', 'rule', 'reduct', 'E_2', 'respect', 'S_', 'E_2', 'verbatim', 'verbatim', 'Hence', 'S_', 'E_2', 'answer set', 'E_2', 'course', 'decision', 'limit', 'aggregates', 'ranging', 'natural numbers', 'course', 'allow', 'aggregates', 'mapping', 'sets', 'ordinals', 'case', 'body', 'last rule', 'E_2', 'only answer set', 'E_2', 'S_', 'E_1', 'check', 'set', 'satisfying', 'rules', 'E_2', 'contain', 'numbers', 'S_', 'E_1', 'only answer', 'set']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 32, 'end': 42, 'text': 'answer set'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 520, 'end': 543, 'text': 'the answer set of $E_2$'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 941, 'end': 944, 'text': 'set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To make weak VCP based semantics precise we need the following notation and definitions :\n","By $\\bar{W}^n, \\bar{V}^n$ we denote n - ary vectors of sets of ground regular literals and by $W_i$ , $V_i$ their $i$ - th coordinates .\n","$\\bar{W}^n \\leq \\bar{V}^n$ if for every $i$ , $W_i \\subseteq V_i$ .\n","$\\bar{W}^n < \\bar{V}^n$ if $\\bar{W}^n \\leq \\bar{V}^n$ and $\\bar{W}^n \\not= \\bar{V}^n$ .\n","A set atom $C(\\{X : p_1(X)\\},\\dots,\\{X : p_1(X)\\})$ is \\emph{satisfied} by $\\bar{W}^n$ if $C(\\{t : p_1(t) \\in W_1\\}, \\dots, \\{t : p_n(t) \\in W_n\\})$ is true .\n","NOUN PHRASES:\n"," ['make', 'weak VCP', 'based', 'semantics', 'need', 'following notation', 'definitions', 'W', '^n', 'V', '^n', 'denote', 'ary vectors', 'sets', 'ground regular literals', 'W_i', 'V_i', 'i', 'th coordinates', 'W', 'V', '^n', 'W', 'V', '^n', 'W', 'V', '^n', 'W', 'V', '^n', 'A', 'set', 'C', 'X', 'p_1', 'X', 'X', 'p_1', 'X', 'W', '^n', 'C', 't', 'p_1', 't', 't', 'p_n', 't']\n","EXPECTED:\n"," {'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 126, 'end': 176, 'text': 'n - ary vectors of sets of ground regular literals'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 198, 'end': 224, 'text': 'their $i$ - th coordinates'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 383, 'end': 393, 'text': 'A set atom'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{definition} [ Minimal Support ]\n","\\label{d1}\n","Let $A$ be a set of ground regular literals of $\\Pi$ , and $C$ be a set atom with $n$ parameters .\n","$\\bar{W}^n$ is a \\emph{minimal support} for $C$ in $A$ if            \\ item %\n","For every $i$ , $W^0_i \\subseteq \\{\\bar{t} : p_i(\\bar{t}) \\in A$ For ever $1 \\leq i \\leq n$ ,            . \\ item Every $\\bar{V}^n$ % $W=\\langle W_1,\\dots,W_n \\rangle$ where $W_i =\\{\\bar{t} : p_i(\\bar{t})$ such that for every $1 \\leq i \\leq n$ , $W_i \\subseteq V_i \\subseteq A$ satisfies $C$ .\n","NOUN PHRASES:\n"," ['definition', 'd1', 'Let', 'set', 'ground regular literals', 'C', 'set atom', 'parameters', 'W', '^n', 'minimal support', 'C', 't', 'p_i', 't', 'V', '^n', '%', 't', 'p_i', 't', 'satisfies', 'C']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 61, 'end': 102, 'text': 'a set of ground regular literals of $\\\\Pi$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 116, 'end': 146, 'text': 'a set atom with $n$ parameters'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 136, 'end': 146, 'text': 'parameters'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent { \\bf Programs with Set Atoms in the Bodies of Rules }\n","\\begin{example} [ Set atoms in the rule body ]\n","Consider a knowledge base containing two complete lists of atoms :\n","\\begin{verbatim}\n","taken ( mike , cs1 ) . taken ( mike , cs2 ) . taken ( john , cs2 ) . required ( cs1 ) . required ( cs2 ) .\n","\\end{verbatim}\n","Set atoms allow for a natural definition of the new relation , $ready\\_to\\_graduate(S)$ , which holds if student $S$ has taken all the required classes from the second list :\n","NOUN PHRASES:\n"," ['Set Atoms', 'Bodies', 'Rules', 'example', '[ Set atoms', 'rule body ] Consider', 'knowledge base', 'containing', 'complete lists', 'atoms', 'verbatim', 'taken', 'cs1', 'taken', 'cs2', 'taken', 'john', 'cs2', 'required', 'cs1', 'required', 'cs2', 'verbatim', 'Set atoms', 'allow', 'natural definition', 'new relation', 'S', 'holds', 'S', 'has taken', 'required classes', 'second list']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 424, 'end': 431, 'text': 'student'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\end{example}\n","The next example shows how the semantics deals with vicious circles .\n","\\begin{example}\n","[ Set atoms in the rule body ]\n","Consider a program\n","$P_4$\n","NOUN PHRASES:\n"," ['example', 'next example', 'shows', 'semantics deals', 'vicious circles', 'example', '[ Set atoms', 'rule body ] Consider', 'program', 'P_4']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 140, 'end': 149, 'text': 'a program'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ item No $\\bar{U}^n < \\bar{W}^n$ satisfies the first two conditions .\n","NOUN PHRASES:\n"," ['U', 'W', '^n', 'satisfies', 'conditions']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st\n","Our last example shows how subset introduction rule with equality can be used to represent synonyms :\n","\\begin{example} [ Synonyms ]\n","\\label{e26} %\n","Introducing\n","Suppose we have a set of cars represented by atoms formed by a predicate symbol $car$ , e.g. , $\\{car(a).\\ car(b).\\}$\n","The following rule \\begin{verbatim} carro = { X : car ( X ) } :\n","- spanish .\n","\\end{verbatim} allows to introduce a new name of this set for Spanish speaking people .\n","Clearly , $car$ and $carro$ are synonyms .\n","Hence , program $P_9 \\cup\n","\\{spanish.\\}$ has one answer set :\n","$\\{spanish, car(a), car(b), carro(a),carro(b)\\}$ .           \n","NOUN PHRASES:\n"," ['st', 'last example', 'shows', 'subset introduction rule', 'equality', 'represent', 'synonyms', 'example', 'e26', '%', 'Introducing', 'Suppose', 'have', 'set', 'cars', 'represented', 'atoms', 'formed', 'predicate symbol', 'car', 'e.g', 'car', 'b', 'verbatim', 'carro =', 'X', 'car', 'X', 'spanish', 'verbatim', 'allows', 'introduce', 'new name', 'set', 'Spanish speaking people', 'Clearly', 'car', 'Hence', 'program', 'has', 'answer set', 'car', 'car', 'b', 'carro', 'carro', 'b']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{example}  \\label{e1} %\n","[ Self - reference and the set notation ]\n","$P_0$ consisting of a rule :\n","\\begin{verbatim} p ( 1 ) :\n","- card { X : p ( X ) } ! = 1.           \n","NOUN PHRASES:\n"," ['example', 'e1', '% [ Self', 'reference', 'set notation ]', 'P_0', 'consisting', 'rule', 'verbatim', 'p', 'card', 'X', 'p', 'X']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent $P_1$ consisting of rules :\n","\\begin{verbatim}\n","p ( 1 ) : - p ( 0 ) .\n","p ( 0 ) :- p ( 1 ) .\n","p ( 1 ) :\n","- card { X : p ( X ) } ! = 1.           \n","NOUN PHRASES:\n"," ['P_1', 'consisting', 'rules', 'verbatim', 'p', 'p', 'p', 'p', 'p', 'card', 'X', 'p', 'X']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section we introduce alternative interpretation of VCP ( referred to as \\emph{weak VCP} ) and incorporate it in the semantics of a new logic programming language with set , called $\\mathcal{S}log^+$ .\n","The syntax of $\\mathcal{S}log^+$ coincides with that of $\\mathcal{A}log$ .\n","Its informal semantics is based on weak VCP .\n","By $C(T)$ we denote a set atom containing an occurrence of set term $T$ .\n","The { \\em instantiation } of $C(\\{X:p(X)\\})$ in a set $A$ of regular literals obtained from $C(\\{X:p(X)\\})$ by replacing $\\{X:p(X)\\}$ by $\\{t:p(t) \\in A\\}$ .\n","The weak VCP is : { \\em belief in p ( t ) ( i.e. inclusion of p ( t ) in an answer set $A$ ) must be established without reference to the instantiation of a set atom $C$ in $A$ unless the truth of this instantiation can be demonstrated without reference to $p(t)$ . }\n","NOUN PHRASES:\n"," ['section', 'introduce', 'alternative interpretation', 'VCP', 'referred', 'weak VCP', 'incorporate', 'semantics', 'new logic programming language', 'set', 'called', 'S', 'log^+', 'syntax', 'S', 'log^+', 'coincides', 'informal semantics', 'weak VCP', 'C', 'T', 'denote', 'set', 'containing', 'occurrence', 'set term', 'T', 'C', 'X', 'p', 'X', 'set', 'regular literals', 'obtained', 'C', 'X', 'p', 'X', 'replacing', 'X', 'p', 'X', 't', 'p', 't', 'weak VCP', 'p', 't', 'i.e', 'inclusion', 'p', 't', 'answer', 'set', 'A', 'reference', 'instantiation', 'set', 'C', 'truth', 'instantiation', 'reference', 'p', 't']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 137, 'end': 178, 'text': 'a new logic programming language with set'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 350, 'end': 401, 'text': 'a set atom containing an occurrence of set term $T$'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 389, 'end': 397, 'text': 'set term'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 635, 'end': 648, 'text': 'an answer set'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 717, 'end': 727, 'text': 'a set atom'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\end{definition}\n","Intuitively , the weak VCP says that set atom $C$ can be safely used to support the reasoner 's beliefs iff the existence of a minimal support of $C$ can be established without reference to those beliefs .\n","Precise definition of answer sets of $\\mathcal{S}log^+$ is obtained by replacing definition \\ref{reduct1} of set reduct of $\\mathcal{A}log$ by definition \\ref{vcp2} below and combining it with definition \\ref{reduct2} .\n","\\begin{definition}\n","[ Set - reduct of $\\mathcal{S}log^+$ ]\n","\\label{vcp2}\n","A \\emph{set reduct} of $\\mathcal{S}log^+$ program $\\Pi$ with respect to a set $A$ of ground regular literals is obtained from $\\Pi$ by \\begin{enumerate} \\ item\n","Removing rules containing set atoms which are \\emph{false} or \\emph{undefined} in            . \\ item\n","Replacing every remaining set atom $C$ in the body of the rule by the union of coordinates of one of its minimal supports .\n","\\end{enumerate}\n","Clearly such a reduct is a regular ASP program without sets .\n","$A$ is an \\emph{answer set} of a $\\mathcal{S}log^+$ program\n","$\\Pi$ if $A$ is an answer set of a weak set reduct of $\\Pi$ with respect to $A$ .\n","\\end{definition}\n","NOUN PHRASES:\n"," ['definition', 'weak VCP', 'says', 'set', 'C', 'safely used', 'support', 'reasoner', 'beliefs', 'iff', 'existence', 'minimal support', 'C', 'reference', 'beliefs', 'Precise definition', 'sets', 'S', 'replacing', 'reduct1', 'set reduct', 'vcp2', 'combining', 'reduct2', 'definition', '[ Set', 'reduct', 'S', 'log^+', 'vcp2', 'set', 'reduct', 'S', 'program', 'respect', 'set', 'ground regular literals', 'enumerate', 'Removing', 'rules', 'containing', 'set atoms', 'Replacing', 'remaining', 'set', 'C', 'body', 'rule', 'union', 'coordinates', 'minimal supports', 'enumerate', 'Clearly', 'reduct', 'regular ASP program', 'sets', 'answer', 'set', 'S', 'program', 'answer set', 'weak set reduct', 'respect', 'definition']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 54, 'end': 62, 'text': 'set atom'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 537, 'end': 563, 'text': '$\\\\mathcal{S}log^+$ program'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 586, 'end': 591, 'text': 'a set'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 802, 'end': 810, 'text': 'set atom'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 1009, 'end': 1037, 'text': 'a $\\\\mathcal{S}log^+$ program'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\st {\\tt p ( a ) :- p            \\{X : q ( X ) \\} . \\\\ q ( a ) . }\n","NOUN PHRASES:\n"," ['p', 'X', 'q', 'X']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ hide\n","{ \\begin{example}            %\n","[ Example \\ref{e1} revisited ]\n","{\\rm\n","To better understand the weak VCP let us consider programs in Example \\ref{e1} from the standpoint of $\\mathcal{S}log^+$ .\n","Inconsistency of $P_0$ is not caused by self - reference and the program remains inconsistent under the new semantics .\n","As before , there are two candidate answer sets of $P_1$ : $A_1=\\emptyset$ and $A_2=\\{p(0),p(1)\\}$ .\n","$A_1$ is ruled out by the satisfiability requirement .\n","$A_2$ was shown to violate the original version of VCP but one can see that it does not satisfy weak VCP either .\n","Beliefs in $p(0)$ and $p(1)$ depend on the existence of the set $S =\n","\\{t:p(t) \\in A_2\\} = \\{0,1\\}$ satisfying condition $card(S) \\not= 1$ .\n","The truth of this condition cannot be however established without reference to these beliefs .\n","So $P_1$ is still inconsistent .\n","For $P_2$ the situation changes .\n","Consider a candidate answer set $A=\\{p(1)\\}$ .\n","In this case $S = \\{t:p(t) \\in A\\} = \\{1\\}$ .\n","Truth of condition $card(S) \\geq 0$ does not depend on $p(1)$ .\n","Hence the set can be formed without violating the weak VCP and $\\{p(1)\\}$ is the only answer set of $P_2$ . } \\end{example} }\n","NOUN PHRASES:\n"," ['example', 'e1', 'revisited', 'understand', 'weak VCP', 'let', 'consider', 'programs', 'e1', 'standpoint', 'S', 'log^+', 'Inconsistency', 'P_0', 'not caused', 'reference', 'program', 'remains', 'new semantics', 'sets', 'P_1', 'p', 'p', 'A_1', 'satisfiability requirement', 'A_2', 'violate', 'original version', 'VCP', 'see', 'does', 'not satisfy', 'weak VCP', 'Beliefs', 'p', 'p', 'depend', 'existence', 'set', 't', 'p', 't', 'satisfying', 'condition', 'card', 'S', 'truth', 'condition', 'however established', 'reference', 'beliefs', 'P_1', 'P_2', 'situation changes', 'Consider', 'candidate answer', 'set', 'p', 'case', 't', 'p', 't', 'Truth', 'condition', 'card', 'S', 'does', 'not depend', 'p', 'Hence', 'set', 'violating', 'weak VCP', 'p', 'only answer set', 'P_2', 'example']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 343, 'end': 364, 'text': 'candidate answer sets'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 898, 'end': 920, 'text': 'a candidate answer set'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 1123, 'end': 1151, 'text': 'the only answer set of $P_2$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\hide{For program            , consider candidate answer set            .            is a minimal support for {\\tt card\\{X:p(X)\\} ! = 1 } in $A$ .\n","The set - redutt of $P_0$ wrt $A$ is            \\} for which $A$ is not the answer set .\n","So , $A$ is not an answer set of $P_0$ .\n","Similarly , there is no answer set for $P_1$ .\n","For $P_2$ , consider the candidate answer set $A = \\{p(1)\\}$ .\n","$\\{\\}$ is a minimal support for { \\tt card\\{X:p(X)\\} $\\ge$ 0 } in $A$ .\n","The set - reduct of $P_2$ wrt $A$ is \\{\\noindent {\\tt p(1)} \\} .\n","Hence , $A$ is an answer set of $P_2$ . }\n","NOUN PHRASES:\n"," ['program', 'consider', 'candidate answer set', 'minimal support', 'X', 'p', 'X', 'set', 'redutt', 'P_0', 'wrt', 'answer set', 'So', 'answer set', 'P_0', 'answer', 'set', 'P_1', 'P_2', 'consider', 'candidate answer', 'set', 'p', 'minimal support', 'X', 'p', 'X', 'set', 'reduct', 'P_2', 'wrt', 'Hence', 'answer set', 'P_2']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 345, 'end': 369, 'text': 'the candidate answer set'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 539, 'end': 561, 'text': 'an answer set of $P_2$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This paper is the continuation of work started in \\cite{GelfondZ14} with introduction of $\\mathcal{A}log$ -- a version of Answer Set Prolog ( ASP ) with aggregates .\n","The semantics of $\\mathcal{A}log$ combines the Rationality Principle of ASP \\cite{GelK14} with the adaptation of the Vicious Circle Principle ( VCP ) introduced by Poincare and Russel \\cite{poin1906,Russell} in their attempt to resolve paradoxes of set theory .\n","In $\\mathcal{A}log$ , the latter is used to deal with formation of sets and their legitimate use in program rules .\n","To understand the difficulty addressed by $\\mathcal{A}log$ consider the following programs :\n","NOUN PHRASES:\n"," ['paper', 'continuation', 'work', 'started', 'GelfondZ14', 'introduction', 'version', 'Answer Set Prolog', 'ASP', 'aggregates', 'semantics', 'combines', 'Rationality Principle', 'GelK14', 'adaptation', 'Vicious Circle Principle', 'VCP', 'introduced', 'Poincare', 'poin1906', 'Russell', 'attempt', 'resolve', 'paradoxes', 'set theory', 'latter', 'deal', 'formation', 'sets', 'legitimate use', 'program rules', 'understand', 'difficulty', 'addressed', 'consider', 'following programs']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 109, 'end': 163, 'text': 'a version of Answer Set Prolog ( ASP ) with aggregates'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ make title \\begin{abstract}\n","The paper continues the investigation of Poincare and Russel 's Vicious Circle Principle ( VCP ) in the context of the design of logic programming languages with sets .\n","We expand previously introduced language $\\mathcal{A}log$ with aggregates by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching .\n","In addition , we propose an alternative formalization of the original VCP and incorporate it into the semantics of new language , $\\mathcal{S}log^+$ , which allows more liberal construction of sets and their use in programming rules .\n","We show that , for programs without disjunction and infinite sets , the formal semantics of aggregates in $\\mathcal{S}log^+$ coincides with that of several other known languages .\n","Their intuitive and formal semantics , however , are based on quite different ideas and seem to be more involved than that of $\\mathcal{S}log^+$ .\n","NOUN PHRASES:\n"," ['make', 'abstract', 'paper', 'continues', 'investigation', 'Poincare', 'Russel', 'Vicious Circle Principle', 'VCP', 'context', 'design', 'logic programming languages', 'sets', 'expand', 'previously introduced', 'language', 'aggregates', 'allowing', 'infinite sets', 'several additional set', 'related', 'constructs', 'knowledge representation', 'teaching', 'addition', 'propose', 'alternative formalization', 'original VCP', 'incorporate', 'semantics', 'new language', 'S', 'log^+', 'allows', 'liberal construction', 'sets', 'use', 'programming rules', 'show', 'programs', 'disjunction', 'infinite sets', 'formal semantics', 'aggregates', 'S', 'log^+', 'coincides', 'several other known', 'languages', 'formal semantics', 'different ideas', 'seem', 'S', 'log^+']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 231, 'end': 239, 'text': 'language'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 511, 'end': 523, 'text': 'new language'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{example}\n","\\label{e4} {\\rm\n","Consider now an $\\mathcal{S}log^+$ program $P_3$ \\begin{verbatim} p ( 3 ) :\n","- card { X : p ( X ) } >= 2. p ( 2 ) :\n","- card { X : p ( X ) } >= 2. p ( 1 ) .\n","\\end{verbatim}\n","It has two candidate answer sets : $A_1=\\{p(1)\\}$ and $A_2=\\{p(1),p(2),p(3)\\}$ .\n","In $A_1$ the corresponding condition is not satisfied and , hence , the weak set reduct of the program with respect to $A_1$ is $p(1).$\n","Consequently , $A_1$ is an answer set of $P_3$ .\n","In $A_2$ the condition has three minimal supports : $M_1 = \\{p(1),p(2)\\}$ , $M_2\n","= \\{p(1),p(3)\\}$ , and $M_3 = \\{p(2),p(3)\\}$ .\n","Hence , the program has nine weak set reducts of $P_3$ with respect to $A_2$ .\n","Each reduct is of the form \\begin{verbatim} p ( 3 ) :\n","- Mi. p ( 2 ) :\n","- Mj. p ( 1 ) .\n","\\end{verbatim} where $M_i$ and $M_j$ are minimal supports of the condition .\n","Clearly , the first two rules of such a reduct are useless and hence $A_2$ is not an answer set of this reduct .\n","Consequently $A_2$ is not an answer set of $P_3$ . }\n","\\end{example}\n","NOUN PHRASES:\n"," ['example', 'e4', 'S', 'program', 'P_3', 'verbatim', 'p', 'card', 'X', 'p', 'X', '>', 'p', 'card', 'X', 'p', 'X', '>', 'p', 'verbatim', 'has', 'candidate answer sets', 'p', 'p', 'p', 'p', 'A_1', 'corresponding condition', 'weak set reduct', 'program', 'respect', 'A_1', 'p', 'A_1', 'answer set', 'P_3', 'A_2', 'condition', 'has', 'minimal supports', 'p', 'p', 'p', 'p', 'p', 'p', 'Hence', 'program', 'has', 'weak set', 'reducts', 'P_3', 'respect', 'A_2', 'reduct', 'form', 'verbatim', 'p', 'Mi', 'p', 'Mj', 'p', 'verbatim', 'M_i', 'M_j', 'minimal supports', 'condition', 'rules', 'reduct', 'A_2', 'answer set', 'reduct', 'A_2', 'answer set', 'P_3', 'example']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 45, 'end': 74, 'text': 'an $\\\\mathcal{S}log^+$ program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 212, 'end': 233, 'text': 'candidate answer sets'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 350, 'end': 406, 'text': 'the weak set reduct of the program with respect to $A_1$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 442, 'end': 464, 'text': 'an answer set of $P_3$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","First we assume $C$ be $card\\{X:p(X)\\} > 0$ .\n","There is only one candidate answer set $A = \\{p(0)\\}$ for this program .\n","Belief in $p(0)$ ( i.e. its membership in answer set $A$ ) can only be established by checking if instantiation $card\\{t : p(t) \\in A\\} > 0$ of $C$ in $A$ holds .\n","This is prohibited by weak VCP unless the truth of this instantiation can be demonstrated without reference to $p(0)$ .\n","But this cannot be so demonstrated because $card\\{t : p(t) \\in A\\} > 0$ holds only when $p(0)$ is in $A$ .\n","Hence , $A$ is not an answer set .\n","Now let $C$ be $card\\{X:p(X)\\} \\ge 0$ .\n","This time the truth of instantiation $card\\{t : p(t) \\in A\\} \\ge 0$ of $C$ can be demonstrated without reference to $p(0)$ -- the instantiation would be true even if $A$ were empty .\n","Hence $p(0)$ must be believed and thus the program has one answer set , $\\{p(0)\\}$ .\n","\\end{example}\n","NOUN PHRASES:\n"," ['noindent First', 'assume', 'C', 'X', 'p', 'X', 'candidate answer', 'set', 'p', 'program', 'Belief', 'p', 'i.e', 'membership', 'answer', 'set', 'A', 'checking', 't', 'p', 't', '>', 'C', 'holds', 'weak VCP', 'truth', 'instantiation', 'reference', 'p', 't', 'p', 't', '>', 'holds', 'p', 'Hence', 'answer set', 'Now let', 'C', 'X', 'p', 'X', 'time', 'truth', 'instantiation', 't', 'p', 't', 'C', 'reference', 'p', 'instantiation', 'Hence', 'p', 'program', 'has', 'answer set', 'p', 'example']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 75, 'end': 95, 'text': 'candidate answer set'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 837, 'end': 847, 'text': 'answer set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st has answer sets $A_1 = \\{q(a)\\}$ where the set $p$ is empty and $A_2 = \\{q(a),p(a)\\}$ where $p = \\{a\\}$ .           \n","The formal definition of answer sets of programs with set introduction rules is given via a notion of \\emph{set introduction reduct} .\n","( The definition is similar to that presented in \\cite{gel02} ) .\n","NOUN PHRASES:\n"," ['has', 'sets', 'q', 'set', 'q', 'p', 'formal definition', 'sets', 'programs', 'set', 'introduction rules', 'notion', 'set', 'introduction reduct', 'definition', 'presented', 'gel02']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 9, 'end': 20, 'text': 'answer sets'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 44, 'end': 51, 'text': 'the set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As shown in \\cite{SonPT07} $\\mathcal{S}log$ has sufficient expressive power to formalize complex forms of recursion , including that used in the Company Control Problem \\cite{FaberPL11} .\n","Theorem \\ref{th2} guarantees that the same representations will work in $\\mathcal{S}log^+$ .\n","Of course , in many respects $\\mathcal{S}log^+$ substantially increases the expressive power of $\\mathcal{S}log$ .\n","Most importantly it expands the $\\mathcal{S}log$ semantics to programs with epistemic disjunction -- something which does not seem to be easy to do using the original definition of $\\mathcal{S}log$ answer sets .\n","Of course , new set constructs and rules with infinite number of literals are available in $\\mathcal{S}log^+$ but not in $\\mathcal{S}log$ .\n","On another hand , $\\mathcal{S}log$ allows multisets -- a feature we were not trying to include in our language .\n","The usefulness of multisets and the analysis of its cost in terms of growing complexity of the language due to its introduction is still under investigation .\n","NOUN PHRASES:\n"," ['shown', 'SonPT07', 'S', 'log', 'has', 'sufficient expressive power', 'formalize', 'complex forms', 'recursion', 'including', 'used', 'FaberPL11', 'th2', 'guarantees', 'same representations', 'work', 'S', 'log^+', 'course', 'many respects', 'S', 'log^+', 'substantially increases', 'expressive power', 'S', 'log', 'expands', 'S', 'log', 'semantics', 'programs', 'epistemic disjunction', 'something', 'does', 'not seem', 'do using', 'original definition', 'S', 'log', 'sets', 'course', 'new set', 'constructs', 'rules', 'infinite number', 'literals', 'S', 'log^+', 'S', 'log', 'hand', 'S', 'log', 'allows', 'multisets', 'feature', 'not trying', 'include', 'language', 'usefulness', 'multisets', 'analysis', 'cost', 'terms', 'growing', 'complexity', 'language', 'introduction', 'investigation']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent $P_2$ consisting of rules :\n","NOUN PHRASES:\n"," ['P_2', 'consisting', 'rules']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{verbatim} p ( 1 ) :- card { X : p ( X ) } >= 0.           \n","Even for these seemingly simple programs , there are different opinions about their meaning .\n","To the best of our knowledge all ASP based semantics , including that of \\cite{FaberPL11,SonP07,GelfondZ14} ) view $P_0$ as a bad specification .\n","It is inconsistent , i.e. , has no answer sets .\n","Opinions differ , however , about the meaning of the other two programs .\n","\\cite{FaberPL11} views $P_1$ as a reasonable specification having one answer set -- $\\{p(0),p(1) \\}$ .\n","According to \\cite{SonP07,GelfondZ14} $P_1$ is inconsistent .\n","According to most semantics $P_2$ has one answer set , $\\{p(1)\\}$ .\n","$\\mathcal{A}log$ , however , views it as inconsistent .\n","\\end{example}\n","NOUN PHRASES:\n"," ['verbatim', 'p', 'card', 'X', 'p', 'X', '>', 'simple programs', 'different opinions', 'meaning', 'knowledge', 'ASP', 'based', 'semantics', 'including', 'FaberPL11', 'SonP07', 'GelfondZ14', 'view', 'P_0', 'bad specification', 'i.e', 'has', 'answer', 'sets', 'Opinions', 'differ', 'meaning', 'programs', 'FaberPL11', 'views', 'P_1', 'reasonable specification', 'having', 'answer set', 'p', 'p', 'According', 'SonP07', 'GelfondZ14', 'P_1', 'According', 'semantics', 'P_2', 'has', 'answer set', 'p', 'views', 'inconsistent', 'example']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 499, 'end': 509, 'text': 'answer set'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 636, 'end': 646, 'text': 'answer set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ medskip\n","Unfortunately , the additional power of $\\mathcal{S}log^+$ as compared with $\\mathcal{A}log$ comes at a price .\n","Part of it is a comparative complexity of the definition of $\\mathcal{S}log^+$ set reduct .\n","But , more importantly , the formalization of the weak VCP does not eliminate all the known paradoxes of reasoning with sets .\n","Consider , for instance the following example :\n","\\begin{example}\n","\\label{e5} { \\rm\n","Recall program $P_2$ :            p ( 1 ) :\n","- card { X : p ( X ) } >= 0.            from Example \\ref{e1} and assume , for simplicity , that parameters of $p$ are restricted to $\\{0,1\\}$ .\n","Viewed as a program of $\\mathcal{A}log$ , $P_2$ is inconsistent .\n","In $\\mathcal{S}log^+$ ( and hence in $\\mathcal{S}log$ and $\\mathcal{F}log$ ( the language defined in \\cite{FaberPL11} ) ) it has an answer set $\\{p(1)\\}$ .\n","The latter languages therefore admit existence of set $\\{X:p(X)\\}$ .\n","Now let us look at program $P_5$ : \\begin{verbatim} p ( 1 ) :\n","- card { X : p ( X ) } = Y , Y >=0.            and its grounding\n","$P_6$ : \\begin{verbatim} p ( 1 ) :\n","- card { X : p ( X ) } = 1, 1 >=0. p ( 1 ) :\n","- card { X : p ( X ) } = 0, 0 >=0.           \n","They seem to express the same thought as $P_2$ , and it is natural to expect all these programs to be equivalent .\n","It is indeed true in $\\mathcal{A}log$ -- none of the programs is consistent .\n","According to the semantics of $\\mathcal{S}log^+$ ( and $\\mathcal{S}log$ and $\\mathcal{F}log$ ) , however , $P_5$ and $P_6$ are inconsistent .\n","To see that notice that there are two candidate answer sets for $P_6$ : $A_1=\\emptyset$ and $A_2 = \\{p(1)\\}$ .\n","The minimal support of $card\\{X:p(X)\\}\n","= 0$ in $A_1$ is $\\emptyset$ and hence the only weak set reduct of $P_6$ with respect to $A_1$ is \\{{\\tt p(1) :- 0>=0} \\} .\n","$A_1$ is not an answer set of $P_6$ .\n","The minimal support of $card\\{X:p(X)\\}=1$ in $A_2$ is $\\{p(1)\\}$ .\n","The only weak set reduct is \\{ {\\tt p ( 1 ) : - p ( 1 ) , 1 >=0 } \\} .\n","$A_2$ is not an answer set of $P_6$ either .\n","It could be that this paradoxical behavior will be in the future explained from some basic principles but currently authors are not aware of such an explanation .\n","NOUN PHRASES:\n"," ['additional power', 'S', 'compared', 'comes', 'price', 'Part', 'comparative complexity', 'definition', 'S', 'log^+', 'set', 'reduct', 'formalization', 'weak VCP', 'does', 'not eliminate', 'known', 'paradoxes', 'reasoning', 'sets', 'Consider', 'instance', 'following example', 'example', 'e5', 'P_2', 'p', 'card', 'X', 'p', 'X', '>', 'e1', 'assume', 'simplicity', 'parameters', 'Viewed', 'program', 'P_2', 'S', 'log^+', 'S', 'log', 'F', 'log', 'language', 'defined', 'FaberPL11', 'has', 'answer', 'set', 'p', 'latter languages', 'therefore', 'admit existence', 'X', 'p', 'X', 'Now let', 'look', 'program', 'P_5', 'verbatim', 'p', 'card', 'X', 'p', 'X', '= Y', 'Y > =0', 'grounding', 'P_6', 'verbatim', 'p', 'card', 'X', 'p', 'X', '> =0', 'p', 'card', 'X', 'p', 'X', '> =0', 'seem', 'express', 'same thought', 'P_2', 'expect', 'programs', 'none', 'programs', 'According', 'semantics', 'S', 'log^+', 'S', 'log', 'F', 'log', 'P_5', 'P_6', 'see', 'notice', 'sets', 'P_6', 'p', 'minimal support', 'X', 'p', 'X', 'A_1', 'hence', 'only weak set', 'reduct', 'P_6', 'respect', 'A_1', 'p', '> =0', 'A_1', 'answer set', 'P_6', 'minimal support', 'X', 'p', 'X', 'A_2', 'p', 'only weak set', 'reduct', 'p', 'p', '> =0', 'A_2', 'answer set', 'P_6', 'paradoxical behavior', 'future', 'explained', 'basic principles', 'authors', 'explanation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 429, 'end': 436, 'text': 'program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 806, 'end': 819, 'text': 'an answer set'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 883, 'end': 886, 'text': 'set'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 921, 'end': 928, 'text': 'program'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 1528, 'end': 1559, 'text': 'candidate answer sets for $P_6$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{definition}\n","[ Set Introduction Reduct ] \\label{reduct2}\n","The \\emph{set introduction reduct} of a ground $\\mathcal{A}log$ program $\\Pi$ with respect to a set of ground regular literals $A$ is obtained from $\\Pi$ by            \\ item replacing every set introduction rule of $\\Pi$ whose head is not true in $A$ by            \\ item replacing every set introduction rule of $\\Pi$ whose head $p\n","\\subseteq \\{\\bar{X}:q(\\bar{X})\\}$ ( or $p = \\{\\bar{X}:q(\\bar{X})\\}$ or $\\{\\bar{X}:q(\\bar{X})\\} \\subseteq p$ ) is true in $A$ by\n","$$p(\\bar{t}) \\leftarrow body$$\n","for each $p(\\bar{t}) \\in A$ .\n","\\end{enumerate}\n","Set $A$ is an \\emph{answer set} of $\\Pi$ if it is an answer set of the set introduction reduct of $\\Pi$ with respect to $A$ .\n","\\end{definition}\n","\\begin{example}\n","[ Set introduction rule ] \\label{e25}\n","Consider a program $P_9$ from Example \\ref{e20} .\n","The reduct of this program with respect to $A_1 = \\{q(a)\\}$ is $\\{q(a).\\}$ and hence $A_1$ is an answer set of $P_9$ .\n","The reduct of $P_9$ with respect to $A_2 = \\{q(a),p(a)\\}$ is $\\{q(a). \\ p(a).\\}$ and hence $A_2$ is also an answer set of $P_9$ .\n","There are no other answer sets .\n","\\end{example}\n","The use of a set introduction rule $p \\subseteq S \\leftarrow body$ is very similar to that of choice rule $\\{p(\\bar{X}) : q(\\bar{X})\\} \\leftarrow body$ of \\cite{nss02} implemented in Clingo and other similar systems .\n","In fact , if $p$ from the set introduction rule does not occur in the head of any other rule of the program , the two rules have the same meaning .\n","However if this condition does not hold the meaning is different .\n","An $\\mathcal{A}log$ program consisting of rules $p \\subseteq \\{X:q_1(X)\\}$ and $p \\subseteq \\{X:q_2(X)\\}$ defines an arbitrary set $p$ from the intersection of $q_1$ and $q_2$ .\n","With choice rules it is not the case .\n","We prefer the set introduction rule because of its more intuitive reading ( after all everyone is familiar with the statement `` $p$ is an arbitrary subset of $q$ ' ' ) and relative simplicity of the definition of its formal semantics as compared with that of the choice rule .\n","NOUN PHRASES:\n"," ['definition', 'reduct2', 'set', 'introduction reduct', 'ground', 'program', 'respect', 'set', 'ground regular literals', 'replacing', 'set introduction rule', 'head', 'replacing', 'set introduction rule', 'X', 'q', 'X', 'X', 'q', 'X', 'q', 'X', 'p', 't', 'p', 't', 'enumerate', 'Set', 'answer', 'set', 'answer set', 'set introduction reduct', 'respect', 'definition', 'example', 'e25', 'Consider', 'program', 'P_9', 'e20', 'reduct', 'program', 'respect', 'q', 'q', 'A_1', 'answer set', 'P_9', 'reduct', 'P_9', 'respect', 'q', 'q', 'A_2', 'answer set', 'P_9', 'sets', 'example', 'use', 'set introduction rule', 'choice rule', 'p', 'X', 'q', 'X', 'implemented', 'Clingo', 'other similar systems', 'fact', 'p', 'set introduction rule', 'does', 'not occur', 'head', 'other rule', 'program', 'rules', 'have', 'same meaning', 'condition', 'does', 'not hold', 'meaning', 'program consisting', 'rules', 'p', 'X', 'q_1', 'X', 'X', 'q_2', 'X', 'defines', 'arbitrary set', 'intersection', 'choice rules', 'case', 'prefer', 'set introduction rule', 'intuitive reading', 'everyone', 'statement', 'arbitrary subset', 'relative simplicity', 'definition', 'formal semantics', 'compared', 'choice rule']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 101, 'end': 134, 'text': 'a ground $\\\\mathcal{A}log$ program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 157, 'end': 189, 'text': 'a set of ground regular literals'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 602, 'end': 605, 'text': 'Set'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 808, 'end': 817, 'text': 'a program'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 849, 'end': 908, 'text': 'The reduct of this program with respect to $A_1 = \\\\{q(a)\\\\}$'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 943, 'end': 965, 'text': 'an answer set of $P_9$'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 968, 'end': 1025, 'text': 'The reduct of $P_9$ with respect to $A_2 = \\\\{q(a),p(a)\\\\}$'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 1073, 'end': 1095, 'text': 'an answer set of $P_9$'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 1156, 'end': 1179, 'text': 'a set introduction rule'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{theorem}  \\label{th2}\n","Let $\\Pi$\n","be a program which , syntactically , belongs to both $\\mathcal{S}log$ and $\\mathcal{S}log^+$ .\n","A set $A$ is an $\\mathcal{S}log$ answer set of $\\Pi$ iff it is an $\\mathcal{S}log^+$ answer set of            .\n","\\end{theorem}\n","NOUN PHRASES:\n"," ['theorem', 'th2', 'Let', 'program', 'belongs', 'S', 'log', 'S', 'log^+', 'A', 'set', 'S', 'log', 'set', 'iff', 'S', 'log^+', 'set', 'theorem']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 42, 'end': 51, 'text': 'a program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 134, 'end': 139, 'text': 'A set'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 147, 'end': 186, 'text': 'an $\\\\mathcal{S}log$ answer set of $\\\\Pi$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\st {\\tt q ( a ) . \\\\ p            \\{X:q ( X ) \\}.}\n","NOUN PHRASES:\n"," ['X', 'q', 'X']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st \\emph { An expression $\\{X:p(X)\\}$ denotes a set $S$ only if for every $t$ rational belief in $p(t)$ can be established without a reference to $S$ } , or equivalently , \\ emph { the reasoner 's belief in $p(t)$ can not depend on existence of a set denoted by $\\{X:p(X)\\}$ } .\n","NOUN PHRASES:\n"," ['expression', 'X', 'p', 'X', 'denotes', 'S', 'rational belief', 'p', 't', 'reference', 'S', 'emph', 'reasoner', 'belief', 'p', 't', 'not depend', 'existence', 'set', 'denoted', 'X', 'p', 'X']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 13, 'end': 26, 'text': 'An expression'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 48, 'end': 53, 'text': 'a set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent\n","As in the naive set theory , the difficulty in interpretations seems to be caused by self - reference .\n","In both $P_1$ and $P_2$ , the definition of $p(1)$ references the set described in terms of $p$ .\n","It is , of course , not entirely clear how this type of differences can be resolved .\n","Sometimes , further analysis can find convincing arguments in favor of one of the proposals .\n","Sometimes , the analysis discovers that different approaches really model different language or world phenomena and are , hence , all useful in different contexts .\n","We believe that the difficulty can be greatly alleviated if the designers of the language provide its users with as clear intuitive meaning of the new constructs as possible .\n","Accordingly , the \\emph{set name} construct $\\{X:p(X)\\}$ of $\\mathcal{A}log$ denotes \\ emph { the set of all objects believed by the rational agent associated with the program to satisfy property $p$ } .\n","( This reading is in line with the epistemic view of ASP connectives shared by the authors . )\n","The difficulties with self - reference in $\\mathcal{A}log$ are resolved by putting the following intuitive restriction on the formation of sets\n","\\ footnote\n","{ It is again similar to set theory where the difficulty is normally avoided by restricting comprehension axioms guaranteeing existence of sets denoted by expressions of the form $\\{X:p(X)\\}$ .\n","In ASP such restrictions are encoded in the definition of answer sets . } :\n","NOUN PHRASES:\n"," ['naive set theory', 'difficulty', 'interpretations', 'seems', 'reference', 'P_1', 'P_2', 'definition', 'p', 'references', 'set', 'described', 'terms', 'course', 'type', 'differences', 'further analysis', 'find', 'convincing arguments', 'favor', 'proposals', 'analysis', 'discovers', 'different approaches', 'really model', 'different language', 'world phenomena', 'different contexts', 'believe', 'difficulty', 'greatly alleviated', 'designers', 'language', 'provide', 'users', 'clear intuitive meaning', 'new constructs', 'set', 'name', 'construct', 'X', 'p', 'X', 'set', 'objects', 'believed', 'rational agent', 'associated', 'program', 'satisfy', 'property', 'reading', 'line', 'epistemic view', 'ASP connectives', 'shared', 'authors', 'difficulties', 'reference', 'putting', 'following intuitive restriction', 'formation', 'footnote', 'set', 'theory', 'difficulty', 'normally avoided', 'restricting', 'comprehension axioms', 'guaranteeing', 'existence', 'sets', 'denoted', 'expressions', 'X', 'p', 'X', 'ASP such restrictions', 'definition', 'sets']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 920, 'end': 928, 'text': 'property'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st in which definition of $p(a)$ depends on the existence of the set denoted by $\\{X: p(X)\\}$ .\n","In accordance with the vicious circle principle no answer set of this program can contain $p(a)$ .\n","There are only two candidates for answer sets of $P_4$ : $S_1 = \\{q(a)\\}$ and $S_2 =\n","\\{q(a),p(a)\\}$ .\n","The set atom reduct of $P_4$ with respect to $S_1$ is \\begin{verbatim} p ( a ) : - q ( a ) .\n","q ( a ) .\n","\\end{verbatim} while set atom reduct of $P_4$ with respect to $S_2$ is \\begin{verbatim} p ( a ) : - p ( a ) , q ( a ) . q ( a ) .\n","\\end{verbatim}\n","Clearly , neither $S_1$ nor $S_2$ is an answer set of $P_4$ .\n","As expected , the program is inconsistent .\n","\\end{example}\n","\\subsubsection{Programs with Set Introduction Rules.}\n","A set introduction rule with head $p \\subseteq S$ ( where $p$ is a predicate symbol and $S$ is a set name ) defines set $p$ as an arbitrary subset of $S$ ; rule with head $p = S$ simply gives $S$ a different name ; $S \\subseteq p$ defines $p$ as an arbitrary superset of $S$ .\n","\\begin{example}\n","[ Set introduction rule ]\n","\\label{e20}\n","According to this intuitive reading the program $P_9$ :\n","NOUN PHRASES:\n"," ['st', 'definition', 'p', 'depends', 'existence', 'set', 'denoted', 'X', 'p', 'X', 'accordance', 'vicious circle principle', 'answer set', 'program', 'contain', 'p', 'candidates', 'sets', 'P_4', 'q', 'q', 'set atom reduct', 'P_4', 'respect', 'S_1', 'verbatim', 'p', 'q', 'q', 'verbatim', 'set', 'atom reduct', 'P_4', 'respect', 'S_2', 'verbatim', 'p', 'p', 'q', 'verbatim', 'Clearly', 'S_1', 'S_2', 'answer set', 'P_4', 'expected', 'program', 'example', 'Programs', 'Set Introduction Rules', 'set introduction rule', 'head', 'predicate symbol', 'S', 'set name', 'defines set', 'arbitrary subset', 'S', 'rule', 'head', 'p = S', 'simply gives', 'S', 'different name', 'p', 'defines', 'arbitrary superset', 'S', 'example', 'e20', 'According', 'intuitive reading', 'program', 'P_9']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 786, 'end': 804, 'text': 'a predicate symbol'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 816, 'end': 826, 'text': 'a set name'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 1088, 'end': 1099, 'text': 'the program'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st\n","The following two results help to better understand the semantics of $\\mathcal{S}log^+$ .\n","\\begin{theorem}\n","\\label{th1}\n","If a set $A$ is an $\\mathcal{A}log$ answer set of $\\Pi$ then $A$ is an $\\mathcal{S}log^+$ answer set of $\\Pi$ .\n","\\end{theorem}\n","As an $\\mathcal{S}log^+$ program , $P_2$ has an answer set of $\\{p(1)\\}$ , but it has no answer set as an $\\mathcal{A}log$ program .\n","The following result shows that there are many such programs and justifies our name for the new language .\n","NOUN PHRASES:\n"," ['st', 'results', 'help', 'better understand', 'semantics', 'S', 'log^+', 'theorem', 'th1', 'set', 'set', 'S', 'log^+', 'set', 'theorem', 'S', 'program', 'P_2', 'has', 'answer set', 'p', 'has', 'answer', 'set', 'program', 'following result', 'shows', 'many such programs', 'justifies', 'name', 'new language']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 126, 'end': 131, 'text': 'a set'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 139, 'end': 178, 'text': 'an $\\\\mathcal{A}log$ answer set of $\\\\Pi$'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 191, 'end': 232, 'text': 'an $\\\\mathcal{S}log^+$ answer set of $\\\\Pi$'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 252, 'end': 281, 'text': 'an $\\\\mathcal{S}log^+$ program'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 294, 'end': 307, 'text': 'an answer set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Let $R_{I}(\\Pi, A)$ , $R_{AS}(\\Pi, A)$ and $R_{SS}(\\Pi, A)$ to denote the set introduction reduct , set reduct and Slog + set reduct of $\\Pi$ wrt $A$ respectively .\n","NOUN PHRASES:\n"," ['Let', 'R_', 'A', 'R_', 'A', 'R_', 'SS', 'A', 'denote', 'set introduction reduct', 'set', 'reduct', 'Slog +', 'set', 'reduct']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 70, 'end': 97, 'text': 'the set introduction reduct'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 100, 'end': 110, 'text': 'set reduct'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 115, 'end': 149, 'text': 'Slog + set reduct of $\\\\Pi$ wrt $A$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ item Proved a number of basic properties of programs of $\\mathcal{A}log$ and $\\mathcal{S}log^+$ .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['Proved', 'number', 'basic properties', 'programs', 'S', 'log^+', 'itemize']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Therefore $A$ is an answer set of $R_{SS}(R_I(\\Pi, A), A)^A$ , i.e. , $A$ is an Slog + answer set of            . \\ hfill $\\Box$\n","NOUN PHRASES:\n"," ['answer set', 'R_', 'SS', 'R_I', 'A', 'A', '^A', 'i.e', 'Slog + answer set']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 17, 'end': 60, 'text': 'an answer set of $R_{SS}(R_I(\\\\Pi, A), A)^A$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 77, 'end': 111, 'text': 'an Slog + answer set of 9999999996'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The paper belongs to the series of works aimed at the development of an answer set based knowledge representation language .\n","Even though we want to have a language suitable for serious applications our main emphasis is on teaching .\n","This puts additional premium on clarity and simplicity of the language design .\n","In particular we believe that the constructs of the language should have a simple syntax and a clear intuitive semantics based on understandable informal principles .\n","In our earlier paper \\cite{GelfondZ14} we concentrated on a language $\\mathcal{A}log$ expanding standard Answer Set Prolog by aggregates .\n","We argued that the syntax of the language is simpler than that of the most popular aggregate language $\\mathcal{F}log$ implemented in Clingo and other similar systems .\n","In particular , $\\mathcal{A}log$ 's notion of grounding \\ emph { allows to define the intuitive ( and formal ) meaning of a set name independently from its occurrence in a rule } .\n","As the result , set name $\\{X : p(X)\\}$ can be always equivalently replaced by $\\{Y : p(Y)\\}$ .\n","In $\\mathcal{F}log$ , it is not the case .\n","A semantics of aggregates in $\\mathcal{A}log$ was based on a particularly simple and restrictive formalization of VCP .\n","In this paper we :            \\item\n","Expanded syntax and semantics of the original $\\mathcal{A}log$ by allowing            \\ item rules with an infinite number of literals -- a feature of theoretical interest also useful for defining aggregates on infinite sets ; \\ item subset relation between sets in the bodies of rules concisely expressing a specific form of universal quantification ; \\ item set introduction -- a feature with functionality somewhat similar to that of the choice rule of clingo but with different intuitive semantics . \\end{itemize}\n","Our additional set constructs are aimed at showing that our original languages can be expanded in a natural and technically simple ways .\n","Other constructs such as set operations and rules with variables ranging over sets ( in the style of \\cite{DovierPR03} ) , etc. are not discussed .\n","Partly this is due to space limitations -- we do not want to introduce any new constructs without convincing examples of their use .\n","The future will show if such extensions are justified .\n","NOUN PHRASES:\n"," ['paper', 'belongs', 'series', 'works', 'aimed', 'development', 'answer', 'set based', 'knowledge representation language', 'want', 'have', 'language', 'serious applications', 'main emphasis', 'teaching', 'puts', 'additional premium', 'clarity', 'simplicity', 'language design', 'believe', 'constructs', 'language', 'have', 'simple syntax', 'clear intuitive semantics', 'based', 'understandable informal principles', 'GelfondZ14', 'concentrated', 'language', 'expanding', 'standard Answer Set Prolog', 'aggregates', 'argued', 'syntax', 'language', 'popular aggregate language', 'F', 'log', 'implemented', 'Clingo', 'other similar systems', 'notion', 'grounding', 'allows', 'define', 'intuitive', 'meaning', 'set name', 'occurrence', 'rule', 'result', 'set', 'X', 'p', 'X', 'always equivalently replaced', 'Y', 'p', 'Y', 'F', 'log', 'case', 'semantics', 'aggregates', 'restrictive formalization', 'VCP', 'paper', 'Expanded', 'syntax', 'semantics', 'allowing', 'infinite number', 'literals', 'feature', 'theoretical interest', 'defining', 'aggregates', 'infinite sets', 'subset', 'relation', 'sets', 'bodies', 'rules', 'concisely expressing', 'specific form', 'universal quantification', 'set', 'introduction', 'feature', 'functionality', 'choice rule', 'clingo', 'different intuitive semantics', 'itemize', 'additional set constructs', 'showing', 'original languages', 'simple ways', 'Other constructs', 'set', 'operations', 'rules', 'variables', 'ranging', 'sets', 'style', 'DovierPR03', 'not discussed', 'space limitations', 'do', 'not want', 'introduce', 'new constructs', 'convincing', 'examples', 'use', 'future', 'show', 'such extensions']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 985, 'end': 993, 'text': 'set name'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Since $A$ is an answer set of $R_{AS}(R_I(\\Pi, A), A)$ , $A$ is a minimal model of $R_{AS}(R_I(\\Pi, A), A)^A$ .\n","We will show $A$ is a model of $R_{SS}(R_I(\\Pi, A), A)^A$ .\n","Consider any $r$ of $R_{SS}(R_I(\\Pi, A), A)^A$ such that $body(r)$ is satisfied by $A$ .\n","There is $r' \\in R_{AS}(R_I(\\Pi, A), A)$ such that both $r'$ and $r$ are obtained from the same rule in $R_I(\\Pi, A)$ during ( Slog + ) set reduct .\n","By definition of set reduct , $body(r) \\subseteq body(r')$ and all atoms of $body(r') - body(r)$ are from $A$ .\n","Hence , $A$ satisfies $body(r)$ implies that $A$ satisfies $body(r')$ .\n","Therefore $A$ satisfies the head of $r'$ because $A$ satisfies $r'$ .\n","Since both $r$ and $r'$ have the same head , $A$ satisfies the head of $r$ .\n","Hence $A$ is a model of $R_{SS}(R_I(\\Pi, A), A)^A$ .\n","NOUN PHRASES:\n"," ['answer set', 'R_', 'R_I', 'A', 'A', 'minimal model', 'R_', 'R_I', 'A', 'A', '^A', 'show', 'model', 'R_', 'SS', 'R_I', 'A', 'A', '^A', 'Consider', 'R_', 'SS', 'R_I', 'A', 'A', 'body', 'r', 'r', 'R_I', 'A', 'A', 'r', 'r', 'same rule', 'R_I', 'A', 'Slog +', 'set', 'reduct', 'definition', 'set reduct', 'body', 'r', 'body', 'r', 'atoms', 'body', 'r', 'body', 'r', 'Hence', 'satisfies', 'body', 'r', 'implies', 'satisfies', 'body', 'r', 'Therefore', 'A', 'satisfies', 'head', 'r', 'satisfies', 'r', 'r', 'r', 'have', 'same head', 'satisfies', 'head', 'Hence', 'model', 'R_', 'SS', 'R_I', 'A', 'A', '^A']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 13, 'end': 54, 'text': 'an answer set of $R_{AS}(R_I(\\\\Pi, A), A)$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 64, 'end': 109, 'text': 'a minimal model of $R_{AS}(R_I(\\\\Pi, A), A)^A$'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 132, 'end': 169, 'text': 'a model of $R_{SS}(R_I(\\\\Pi, A), A)^A$'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 754, 'end': 791, 'text': 'a model of $R_{SS}(R_I(\\\\Pi, A), A)^A$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We next show $A$ is minimal by contradiction .\n","Assume there exists $B \\subset A$ such that $B$ is a model of $R_{SS}(R_I(\\Pi, A), A)^A$ .\n","By set reduct definition , for every $r' \\in R_{AS}(R_I(\\Pi, A), A)^A$ , there is a rule $r \\in R_{SS}(R_I(\\Pi, A), A)^A$ such that they are obtained from the same rule of $R_I(\\Pi, A)$ .\n","Hence they have the same head and $body(r') \\supseteq body(r)$ .\n","Therefore , $B$ is also a model of $R_{AS}(R_I(\\Pi, A), A)^A$ contradicting that $B \\subset A$ and $A$ is a minimal model of $R_{AS}(R_I(\\Pi, A), A)^A$ .\n","NOUN PHRASES:\n"," ['next show', 'contradiction', 'Assume', 'exists', 'B', 'model', 'R_', 'SS', 'R_I', 'A', 'A', '^A', 'set reduct definition', 'r', 'R_I', 'A', 'A', '^A', 'rule', 'SS', 'R_I', 'A', 'A', 'same rule', 'R_I', 'A', 'Hence', 'have', 'same head', 'body', 'r', 'body', 'r', 'Therefore', 'B', 'model', 'R_', 'R_I', 'A', 'A', '^A', 'contradicting', 'minimal model', 'R_', 'R_I', 'A', 'A', '^A']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 98, 'end': 135, 'text': 'a model of $R_{SS}(R_I(\\\\Pi, A), A)^A$'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 222, 'end': 226, 'text': 'rule'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 415, 'end': 452, 'text': 'a model of $R_{AS}(R_I(\\\\Pi, A), A)^A$'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 497, 'end': 542, 'text': 'a minimal model of $R_{AS}(R_I(\\\\Pi, A), A)^A$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ item\n","Introduced a new KR language , $\\mathcal{S}log^+$ , with the same syntax as $\\mathcal{A}log$ but different semantics for the set related constructs .\n","The new language is less restrictive and allows formation of substantially larger collection of sets .\n","Its semantics is based on the alternative , weaker formalization of VCP .\n","\\item\n","Proved that ( with the exception of multisets ) $\\mathcal{S}log^+$ is an extension of a well known aggregate language $\\mathcal{S}log$ .\n","The semantics of the new language is based on the intuitive idea quite different from that of $\\mathcal{S}log$ and the definition of its semantics is simpler .\n","We point out some paradoxes of $\\mathcal{S}log^+$ ( and $\\mathcal{F}log$ ) which prevent us from advocating them as standard ASP language with aggregates .\n","NOUN PHRASES:\n"," ['Introduced', 'new KR language', 'S', 'log^+', 'same syntax', 'different semantics', 'set related constructs', 'new language', 'allows', 'formation', 'collection', 'sets', 'semantics', 'alternative', 'formalization', 'VCP', 'Proved', 'exception', 'multisets', 'S', 'extension', 'well', 'known', 'aggregate language', 'S', 'log', 'semantics', 'new language', 'intuitive idea', 'S', 'log', 'definition', 'semantics', 'point', 'paradoxes', 'S', 'log^+', 'F', 'log', 'prevent', 'advocating', 'standard ASP language', 'aggregates']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 439, 'end': 457, 'text': 'aggregate language'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 410, 'end': 474, 'text': 'an extension of a well known aggregate language $\\\\mathcal{S}log$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{definition}\n","[ Occurrences of Regular Literals in Aggregate Atoms ]\n","\\label{occur}\n","We say that a ground literal $l$ \\emph{occurs in a set atom} $C$ if there is a set name $\\{X:cond(X)\\}$ occurring in $C$ and $l$ is a ground instance of some literal in $cond$ .\n","If $B$ is a set of ground literals possibly preceded by default negation $not$ then $l$ occurs in $B$ if $l \\in B$ , or $not\\ l \\in B$ , or $l$ occurs in some set atom from $B$ .\n","\\end{definition}  \\begin{definition}\n","[ Splitting Set ]\n","\\label{split-set}\n","Let $\\Pi$ be a program with signature $\\Sigma$ .\n","A set $S$ of ground regular literals of $\\Sigma$ is called a \\emph{splitting set} of $\\Pi$ if , for every rule $r$ of $\\Pi$ , if $l$ occurs in the head of $r$ then every literal occurring in the body of $r$ belongs to $S$ .\n","The set of rules of $\\Pi$ constructed from literals of $S$ is called \\emph{the bottom} of $\\Pi$ relative to $S$ ; the remaining rules are referred to as \\emph{the top} of $\\Pi$ relative to $S$ .\n","\\end{definition}\n","Note that the definition implies that no literal occurring in the bottom of $\\Pi$ relative to $S$ can occur in the heads of rules from the top of $\\Pi$ relative to $S$ .\n","NOUN PHRASES:\n"," ['definition', '[ Occurrences', 'Regular Literals', 'say', 'ground', 'occurs', 'set atom', 'C', 'set name', 'X', 'cond', 'X', 'occurring', 'C', 'ground instance', 'B', 'set', 'ground literals', 'possibly preceded', 'default negation', 'occurs', 'B', 'occurs', 'set atom', 'B', 'definition', 'definition', 'Let', 'program', 'A', 'set', 'S', 'ground regular literals', 'splitting set', 'rule', 'r', 'l', 'occurs', 'head', 'literal occurring', 'body', 'belongs', 'S', 'set', 'rules', 'constructed', 'literals', 'S', 'bottom', 'S', 'remaining', 'rules', 'S', 'definition', 'Note', 'definition', 'implies', 'literal occurring', 'bottom', 'S', 'occur', 'heads', 'rules', 'top', 'S']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 100, 'end': 116, 'text': 'a ground literal'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 137, 'end': 147, 'text': 'a set atom'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 165, 'end': 175, 'text': 'a set name'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 220, 'end': 263, 'text': 'a ground instance of some literal in $cond$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 276, 'end': 344, 'text': 'a set of ground literals possibly preceded by default negation $not$'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 322, 'end': 338, 'text': 'default negation'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 531, 'end': 564, 'text': 'a program with signature $\\\\Sigma$'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 546, 'end': 555, 'text': 'signature'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 567, 'end': 572, 'text': 'A set'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 673, 'end': 677, 'text': 'rule'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition} [ Anti-chain Property ]\n","\\label{p2aa}\n","If $\\Pi$ is a program without set atoms in the heads of its rules then there are no $\\mathcal{A}log$ answer sets $A_1$ , $A_2$ of $\\Pi$ such that $A_1 \\subset A_2$ .\n","Similarly for its $\\mathcal{S}log^+$ answer sets .\n","NOUN PHRASES:\n"," ['proposition', '[', 'p2aa', 'program', 'set atoms', 'heads', 'rules', 'sets', 'A_1', 'A_2', 'Similarly', 'S', 'log^+', 'sets']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 69, 'end': 122, 'text': 'a program without set atoms in the heads of its rules'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 141, 'end': 169, 'text': '$\\\\mathcal{A}log$ answer sets'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition}\n","[ Splitting Set Theorem ]\n","\\label{split}\n","Let $\\Pi$ be a ground program , $S$ be its splitting set , and $\\Pi_1$ and $\\Pi_2$ be the bottom and the top of $\\Pi$ relative to $S$ respectively .\n","Then a set $A$ is an answer set of $\\Pi$ iff $A \\cap S$ is an answer set of $\\Pi_1$ and $A$ is an answer set of $(A \\cap S) \\cup \\Pi_2$ .\n","\\end{proposition}\n","Note that this formulation differs from the original one in two respects .\n","First , rules of the program can be infinite .\n","Second , the definition of occurrence of a regular literal in a rule changes to accommodate the presence of set atoms .\n","NOUN PHRASES:\n"," ['proposition', 'split', 'Let', 'ground program', 'S', 'splitting', 'set', 'bottom', 'top', 'S', 'set', 'answer set', 'answer set', 'answer set', 'proposition', 'Note', 'formulation differs', 'respects', 'First', 'rules', 'program', 'Second', 'definition', 'occurrence', 'regular literal', 'rule changes', 'accommodate', 'presence', 'set atoms']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 73, 'end': 89, 'text': 'a ground program'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 103, 'end': 116, 'text': 'splitting set'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 146, 'end': 156, 'text': 'the bottom'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 161, 'end': 193, 'text': 'the top of $\\\\Pi$ relative to $S$'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 214, 'end': 219, 'text': 'a set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," There are multiple approaches to introducing aggregates in logic programming languages under the answer sets semantics \\cite{KempS91,gel02,nss02,Marek04,Marek2004set,Pelov04,PelovDB04,PelovT04,Ferraris05,FerrarisL05,pdb07,SonP07,LeeLP08,ShenYY09,LiuPST10,FaberPL11,PontelliST11,liu2011strong,WangLZY12,HarrisonLY14,shen2014flp,GelfondZ14,gebser2015abstract,alviano2015complexity} .\n","In addition to this work our paper was significantly influenced by the original work on VCP in set theory and principles of language design advocated by Dijkstra , Hoare , Wirth and others .\n","Harrison et al 's work \\cite{HarrisonLY14} explaining the semantics of some constructs of gringo in terms of infinitary formulas of Truszczynski \\cite{truszczynski2012connecting} led to their inclusion in $\\mathcal{A}log$ and $\\mathcal{S}log^+$ .\n","The notion of set reduct of $\\mathcal{A}log$ was influenced by the reduct introduced for defining the semantics of Epistemic Specification in \\cite{Gelfond2011new} .\n","Recent work by Alviano and Faber \\cite{alviano2015stable} helped us to realize the close relationship between $\\mathcal{A}log$ and $\\mathcal{S}log$ and Argumentation theory \\cite{dung1995acceptability,brewka2013abstract,strass2013approximating} which certainly deserves further investigation , as well as provided us with additional knowledge about $\\mathcal{A}log$ .\n","More information about $\\mathcal{S}log$ and $\\mathcal{S}log^+$ can be found in Section 3 .\n","\\\n","hide\n","{\n","We start with a short discussion of our semantics for ASP with infinite rules .\n","The idea of expending the syntax of ASP to allow infinitary formulas is not new .\n","Stable model semantics for such formulas was first introduced in \\cite{Truszczynski12} .\n","It is based on the notion of stable models for finite propositional formulas from \\cite{Ferraris05,Ferraris11} .\n","Proposition 28 from \\cite{fl05} shows that , for finite ASP rules , the latter definition coincides with the original definition of answer sets .\n","The proof of this proposition can be easily adopted to the infinite case to show that a semantics of ASP program with infinite rules coincides with that from \\cite{Truszczynski12} . }\n","Shen et al. \\cite{ShenYY09} and Liu et al. \\cite{liu2011strong} propose equivalent semantics for disjunctive constraint programs ( i.e. , programs with rules whose bodies are built from constraint atoms and whose heads are epistemic disjunctions of such atoms ) .\n","This generalizes the standard ASP semantics for disjunctive programs .\n","We conjecture that when we adapt our definition of $\\mathcal{S}log^+$ semantics to disjunctive constraint programs , it will coincide with that of \\cite{ShenYY09,liu2011strong} .\n","However , our definition seems to be simpler and is based on clear , VCP related intuition .\n","NOUN PHRASES:\n"," ['multiple approaches', 'introducing', 'aggregates', 'logic programming languages', 'answer', 'sets', 'semantics', 'KempS91', 'gel02', 'Marek04', 'Marek2004set', 'Pelov04', 'PelovDB04', 'PelovT04', 'Ferraris05', 'FerrarisL05', 'pdb07', 'SonP07', 'LeeLP08', 'ShenYY09', 'LiuPST10', 'FaberPL11', 'PontelliST11', 'WangLZY12', 'HarrisonLY14', 'shen2014flp', 'GelfondZ14', 'gebser2015abstract', 'alviano2015complexity', 'addition', 'work', 'paper', 'significantly influenced', 'original work', 'VCP', 'set theory', 'principles', 'language design', 'advocated', 'Dijkstra', 'Hoare', 'Wirth', 'others', 'Harrison', 'et', 'al', 'work', 'HarrisonLY14', 'explaining', 'semantics', 'constructs', 'gringo', 'terms', 'infinitary formulas', 'truszczynski2012connecting', 'led', 'inclusion', 'S', 'log^+', 'notion', 'set reduct', 'reduct', 'introduced', 'defining', 'semantics', 'Epistemic Specification', 'Gelfond2011new', 'Recent work', 'Alviano', 'helped', 'realize', 'close relationship', 'S', 'log', 'dung1995acceptability', 'brewka2013abstract', 'strass2013approximating', 'certainly deserves', 'further investigation', 'provided', 'additional knowledge', 'information', 'S', 'log', 'S', 'log^+', 'Section', 'start', 'short discussion', 'semantics', 'ASP', 'infinite rules', 'idea', 'expending', 'syntax', 'ASP', 'allow', 'infinitary formulas', 'Stable model semantics', 'such formulas', 'first introduced', 'Truszczynski12', 'notion', 'stable models', 'finite propositional formulas', 'Ferraris05', 'Ferraris11', 'Proposition', 'fl05', 'shows', 'finite ASP rules', 'latter definition coincides', 'original definition', 'sets', 'proof', 'proposition', 'easily adopted', 'infinite case', 'show', 'semantics', 'ASP program', 'infinite rules coincides', 'Truszczynski12', 'Shen', 'al', 'ShenYY09', 'Liu', 'et', 'al', 'propose equivalent semantics', 'disjunctive constraint programs', 'i.e', 'programs', 'rules', 'bodies', 'constraint atoms', 'heads', 'epistemic disjunctions', 'such atoms', 'generalizes', 'standard ASP semantics', 'disjunctive programs', 'conjecture', 'adapt', 'definition', 'S', 'log^+', 'semantics', 'disjunctive', 'constraint programs', 'coincide', 'ShenYY09', 'definition', 'seems', 'VCP', 'related', 'intuition']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{Hierarchical Q-value iteration (HQI)}  \\begin{algorithmic}  \\label{alg:hqi} \\ REQUIRE $O$ , $D$ \\\n","STATE $Train \\leftarrow O_i \\in O$ with only primitive children \\ STATE $Done \\leftarrow \\{A\\}$ \\WHILE{            }  \\FOR{            }  \\STATE{SQI            }  \\STATE{            } \\ ENDFOR \\\n","STATE $Train \\leftarrow O_i \\in (O-Done)$ AND $U_i \\in Done$ \\ ENDWHILE \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'Hierarchical Q-value iteration', 'HQI', 'alg', 'hqi', 'O', 'D', 'primitive children', 'STATE', 'SQI', 'O-Done', 'algorithm']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The HQI algorithm is summarized in Algorithm ~ \\ref{alg:hqi} and SQI is summarized in Algorithm ~ \\ref{alg:sqi} .\n","Any dataset $D$ can be used at every iteration of SQI .\n","If the initial data is sufficient to cover important state - action space , the same dataset is able to train all subtasks of the \\textit{DAG} .\n","NOUN PHRASES:\n"," ['HQI algorithm', 'alg', 'hqi', 'SQI', 'alg', 'sqi', 'D', 'iteration', 'SQI', 'initial data', 'cover', 'important state', 'action space', 'same dataset', 'train', 'subtasks', 'DAG']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 118, 'end': 125, 'text': 'dataset'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{Subtask Q-value Iteration (SQI)}  \\begin{algorithmic}            \\\n","REQUIRE $O_i,D$ \\WHILE{            }  \\FOR{            }  \\FOR{            }            \\\n","STATE            \\\n","STATE            \\ELSE            \\\n","STATE            \\\n","STATE            \\\n","STATE            \\ ENDIF \\ ENDI F \\ END FOR \\ END FOR \\ END WHILE \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'Subtask Q-value Iteration', 'SQI', 'O_i', 'D', 'algorithm']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Therefore , instead of using the above Bellman equation that updates the Q table of the parent when a child exits , we use the intra-option Bellman equation proposed in the $option$ framework ~ \\cite{sutton1999between} : \\begin{align}\n","\\label{eq:optionq}\n","Q_i ( s , u )\n","&= \\sum_{a\\in A}\n","{ \\ pi_u ( a | s ) E [ r ( s , a ) + \\gamma V_i ( s' , u ) ] } \\\\ &= \\sum_{a\\in A}\n","{ \\pi_u(a|s)\\bigg [r(s,a) + \\gamma \\sum_{s'} P ( s' | s , a ) V_ i ( s' , u ) \\ bigg ] }\n","\\end{align} Where\n","\\begin{equation}\n","V_i(s,u) = (1-\\beta_i(s))Q_i(s, u) + \\beta_i(s)\\max_{u'\\in U_i} Q_i(s, u')\n","\\end{equation}\n","NOUN PHRASES:\n"," ['using', 'above Bellman equation', 'updates', 'Q table', 'parent', 'child exits', 'use', 'intra-option Bellman equation', 'proposed', 'option', 'align', 'eq', 'optionq', 'Q_i', '| s', 'E [ r', 'V_i', 's', ']', 'a|s', '[ r', 's', 'P', '| s', 'V_ i', 's', 'bigg ]', 'align', 'equation', 'V_i', '=', 'Q_i', 'Q_i', 'equation']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 182, 'end': 191, 'text': 'framework'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Equation \\eqref{eq:optionq} also yeilds a contraction in the max norm and is able to learn the Q table after observing every new reward , which eliminates the need to estimate $P_i^{\\pi}(s', N | s, u)$ .\n","Another key benefit is that we can use flat samples to estimate the one step transition probability and rewards in equation \\eqref{eq:optionq} , which makes the algorithm independent of the hierarchical decomposition and is able to learn optimal polices for different structures from the same dataset .\n","Specifically , we can estimate the above two terms by $\\sum_{s'}P(s'|s,a)V(i, s',u) \\approx \\frac{1}{c}\\sum_{m=1}^{c}V(i, s'_{m}=s', u)$ and $r(s,a) \\approx \\frac{1}{c}\\sum_{m=1}^{c}r(s_m=s, a_m=a)$ , where $c$ is the number of experiences that has $s'$ and ( $s$ , $a$ ) , respectively .\n","At last , since we assume converged subtasks follow deterministic greedy policy , $\\pi_u(a|s) = 1$ if $a$ is the greedy primitive action that subtask $u$ would take at state $s$ , and $\\pi_u(a|s) = 0$ otherwise .\n","This step is in fact crucial for HQI to learn the optimal policy because it allows a subtask to discard those samples that are not following the optimal behavior of its children .\n","NOUN PHRASES:\n"," ['eq', 'optionq', 'also yeilds', 'contraction', 'max norm', 'learn', 'Q table', 'observing', 'new reward', 'eliminates', 'need', 'estimate', 'P_i^', 'N | s', 'u', 'key benefit', 'use', 'flat samples', 'estimate', 'step transition probability', 'rewards', 'eq', 'optionq', 'makes', 'hierarchical decomposition', 'learn', 'optimal polices', 'different structures', 'same dataset', 'estimate', 'terms', 's', 'P', \"s'|s\", 'V', 'i', 's', 'm=1', '^', 'c', 'V', 'i', 'm', '=s', 'u', 'r', 'm=1', '^', 'c', 'r', 's_m=s', 'a_m=a', 'c', 'number', 'experiences', 'has', 's', 'assume', 'converged subtasks', 'follow', 'deterministic greedy policy', 'a|s', '=', 'greedy primitive action', 'subtask', 'take', 'state', 'a|s', '=', 'step', 'fact', 'HQI', 'learn', 'optimal policy', 'allows', 'subtask', 'discard', 'samples', 'not following', 'optimal behavior', 'children']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 721, 'end': 778, 'text': \"the number of experiences that has $s'$ and ( $s$ , $a$ )\"}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 735, 'end': 778, 'text': \"experiences that has $s'$ and ( $s$ , $a$ )\"}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 938, 'end': 945, 'text': 'subtask'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 964, 'end': 969, 'text': 'state'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 905, 'end': 932, 'text': 'the greedy primitive action'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{Fitted Subtask Q-value Iteration (Fitted SQI)}  \\begin{algorithmic}  \\label{alg:fsqi} \\\n","REQUIRE $O_i,D$ \\WHILE{            }  \\STATE{            ,            }  \\FOR{            }  \\FOR{            }  \\IF{            } \\\n","STATE $y \\leftarrow r$ \\ELSE \\IF{GreedyPolicy(            ,            )            } \\\n","STATE $y \\leftarrow r + \\gamma((1-\\beta_u(s')) Q_i^{k-1}(s', u)$ \\\n","STATE $\\quad \\quad \\quad + \\beta_u(s')max_{u'\\in U_i}Q_i^{k-1}(s', u'))$ \\ ENDIF \\ ENDIF \\STATE{            ,            } \\ ENDFOR \\ ENDFOR \\STATE{            } \\ ENDWHILE \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'Fitted Subtask Q-value Iteration', 'Fitted SQI', 'alg', 'fsqi', 'O_i', 'D', 'STATE', 'r', 'GreedyPolicy', 'STATE', 's', 'Q_i^', 'Q_i^', 'algorithm']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{enumerate} \\ item\n","We first prove that : For a subtask $O_i$ , with all of its children converged to their recursive optimal policies and infinity amount of batch data , algorithm SQI converge to the optimal Q - value function after infinity number of iterations , $\\lim_{k->\\infty}Q^k_i=Q^*_i$\n","NOUN PHRASES:\n"," ['enumerate', 'first prove', 'O_i', 'children', 'converged', 'recursive optimal policies', 'infinity amount', 'batch data', 'algorithm SQI converge', 'optimal Q', 'value function', 'infinity number', 'iterations', 'Q^k_i=Q^*_i']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 51, 'end': 60, 'text': 'a subtask'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 202, 'end': 232, 'text': 'the optimal Q - value function'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We want to prove that for an MDP $M=(S,A,P,R,P_0, \\gamma)$ with hierarchical decomposition $O=\\{O_0,..O_n\\}$ , HQI converges to recursive optimal policy for the hierarchical policy of $M$ , $\\pi^*_r$ .\n","NOUN PHRASES:\n"," ['want', 'prove', 'MDP', 'M=', 'S', 'A', 'P', 'R', 'P_0', 'hierarchical decomposition', 'O_0', 'HQI', 'converges', 'recursive', 'optimal policy', 'hierarchical policy', 'M']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 26, 'end': 32, 'text': 'an MDP'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 64, 'end': 90, 'text': 'hierarchical decomposition'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 128, 'end': 187, 'text': 'recursive optimal policy for the hierarchical policy of $M$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{GreedyPolicy}            \\\n","REQUIRE $u, s$ \\IF{            } \\ RETURN u \\ELSE \\ STATE            \\\n","RETURN Greedy Policy ( $u^*$ , $s$ ) \\ENDIF \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'GreedyPolicy', 'algorithm']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section , we prove that our HQI ( in the tabular case ) converges to the recursive optimal policy .\n","Assume that the policy at each subtask $M_i$ is ordered , such that it break ties deterministically ( e.g favor left to right ) , it defines a unique recursive optimal hierarchical policy , $\\pi^*_r$ , and a corresponding recursive optimal Q function $Q^*_r$ .\n","We then show that HQI converge to $\\pi^*_r$ and $Q^*_r$ .\n","The $r$ subscript refers to recursive optimality .\n","NOUN PHRASES:\n"," ['section', 'prove', 'HQI', 'tabular case', 'converges', 'recursive optimal policy', 'Assume', 'policy', 'M_i', 'break', 'ties', 'e.g', 'favor', 'left', 'right', 'defines', 'unique recursive optimal hierarchical policy', 'corresponding recursive optimal Q function', 'Q^*_r', 'then show', 'HQI converge', 'Q^*_r', 'subscript refers', 'recursive', 'optimality']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 139, 'end': 146, 'text': 'subtask'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 249, 'end': 295, 'text': 'a unique recursive optimal hierarchical policy'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 314, 'end': 358, 'text': 'a corresponding recursive optimal Q function'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\item We then show that HQI provides an order of training all the subtasks in the \\textit{DAG} graph , such that when training a subtask , $O_i$ , all of its children , $U_i$ already converged to their optimal recursive policies .\n","NOUN PHRASES:\n"," ['then show', 'HQI', 'provides', 'order', 'training', 'subtasks', 'DAG', 'graph', 'training', 'subtask', 'O_i', 'children', 'U_i', 'already converged', 'optimal recursive policies']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 127, 'end': 136, 'text': 'a subtask'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 158, 'end': 166, 'text': 'children'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Results show that both HQI with and without state abstraction consistently outperforms the FQI when there is limited training data .\n","When the dataset is large enough , they all converge to the same optimal performance , which is around $1.0$ .\n","We also notice that , occasionally , HQI with state abstraction can learn the optimal performance state abstraction with very limited samples , i.e $5000$ samples .\n","This demonstrates that with proper hierarchy constraints and good behavioral policy , HQI can generalize much faster than FQI .\n","Moreover , even the HQI without state abstraction consistently outperforms FQI in terms of sample efficiency .\n","This is different from the behavior of the on - policy MAXQ - Q algorithm reported in ~ \\cite{dietterich2000hierarchical} , which needs state abstraction in order to learn faster than Q - learning .\n","We argue that HQI without state abstraction is more sample efficient than FQI for the following reasons :\n","1 ) HQI uses all applicable primitive samples to update the Q - table for every subtask while MAXQ - Q only updates for the subtask that executes that particular action .\n","2 ) Upper level subtask in MAXQ - Q needs to wait for its children gradually converges to their greedy optimal policy before it can have have a good estimate of $P(s', N|s, u)$ while HQI does not have this limitation .\n","NOUN PHRASES:\n"," ['Results', 'show', 'HQI', 'state abstraction', 'consistently outperforms', 'FQI', 'data', 'dataset', 'converge', 'same optimal performance', 'also notice', 'HQI', 'state abstraction', 'learn', 'optimal performance state abstraction', 'limited samples', 'samples', 'demonstrates', 'proper hierarchy constraints', 'good behavioral policy', 'HQI', 'generalize', 'FQI', 'HQI', 'state abstraction', 'consistently outperforms', 'FQI', 'terms', 'sample efficiency', 'behavior', 'policy MAXQ', 'Q algorithm', 'reported', 'needs', 'state abstraction', 'order', 'learn', 'Q', 'learning', 'argue', 'HQI', 'state abstraction', 'sample efficient', 'FQI', 'following reasons', 'HQI', 'uses', 'applicable primitive samples', 'update', 'Q', 'table', 'subtask', 'MAXQ', 'Q', 'only updates', 'subtask', 'executes', 'particular action', 'Upper level subtask', 'MAXQ', 'Q', 'needs', 'wait', 'children', 'converges', 'greedy optimal policy', 'have have', 'good estimate', 'P', 's', 'N|s', 'u', 'HQI', 'does', 'not have', 'limitation']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An MDP , $M$ , can be decomposed into a finite set of subtasks $O=\\{O_0, O_1...O_n\\}$ with the convention that $O_0$ is the root subtask , i.e. solving $O_0$ solves the entire original MDP , $M$ .\n","$O_i$ is then a Semi-Markov Decision Process ( SMDP ) that shares the same $S$ , $R$ , $P$ with $M$ , and has an extra tuple $<\\beta_i, U_i>$ , where :\n","\\begin{enumerate} \\ item $\\beta_i(s)$ is the termination predicate of subtask $O_i$ that partition $S$ into a set of active states , $S_i$ and a set of terminal states $T_i$ .\n","If $O_i$ enters a state in $T_i$ , $O_i$ and its subtasks exit immediately , i.e. $\\beta_i(s)=1$ if $s\\in T_i$ , otherwise $\\beta_i(s)=0$ .\n","\\ item $U_i$ is a nonempty set of actions that can be performed by $O_i$ .\n","The actions can be either primitive actions from $A$ or other subtask , $O_j$ , where $i\\neq j$ .\n","We will refer to $U_i$ as the children of subtask $O_i$ .\n","\\end{enumerate}\n","It is evident that a valid hierarchical decomposition forms a direct acyclic graph ( DAG ) where each non-terminal node corresponds to a subtask , and each terminal node corresponds to a primitive action .\n","For later discussion , we will use \\textit{hierarchical decomposition} and \\textit{DAG} interchangeably .\n","NOUN PHRASES:\n"," ['MDP', 'M', 'finite set', 'O_0', 'O_1', 'convention', 'O_0', 'root subtask', 'i.e', 'solving', 'O_0', 'solves', 'entire original MDP', 'M', 'O_i', 'Semi-Markov Decision Process', 'SMDP', 'shares', 'S', 'R', 'P', 'M', 'has', 'U_i >', 'enumerate', 'termination predicate', 'O_i', 'partition', 'S', 'set', 'active states', 'S_i', 'set', 'terminal states', 'T_i', 'O_i', 'enters', 'state', 'T_i', 'O_i', 'subtasks exit', 'i.e', 's', '=1', 'T_i', '=0', 'U_i', 'nonempty set', 'actions', 'O_i', 'actions', 'primitive actions', 'other subtask', 'O_j', 'refer', 'U_i', 'children', 'O_i', 'enumerate', 'valid hierarchical decomposition', 'forms', 'direct acyclic graph', 'DAG', 'non-terminal node corresponds', 'subtask', 'terminal node corresponds', 'primitive action', 'later discussion', 'use', 'hierarchical decomposition', 'DAG']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 6, 'text': 'An MDP'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 38, 'end': 62, 'text': 'a finite set of subtasks'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 54, 'end': 62, 'text': 'subtasks'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 120, 'end': 136, 'text': 'the root subtask'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 176, 'end': 188, 'text': 'original MDP'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 211, 'end': 241, 'text': 'a Semi-Markov Decision Process'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 307, 'end': 321, 'text': 'an extra tuple'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 394, 'end': 432, 'text': 'termination predicate of subtask $O_i$'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 457, 'end': 479, 'text': 'a set of active states'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 492, 'end': 516, 'text': 'a set of terminal states'}, 'T28': {'eid': 'T28', 'label': 'PRIMARY', 'start': 541, 'end': 548, 'text': 'a state'}, 'T33': {'eid': 'T33', 'label': 'PRIMARY', 'start': 681, 'end': 737, 'text': 'a nonempty set of actions that can be performed by $O_i$'}, 'T34': {'eid': 'T34', 'label': 'PRIMARY', 'start': 796, 'end': 809, 'text': 'other subtask'}, 'T37': {'eid': 'T37', 'label': 'PRIMARY', 'start': 864, 'end': 893, 'text': 'the children of subtask $O_i$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A hierarchical policy , $\\pi$ , is a set of policies for each subtask , $O_i$ , $\\pi=\\{\\pi_0, \\pi_1...\\pi_n\\}$ .\n","In the terminology of $option$ framework , a subtask policy is a deterministic $option$ , with $\\beta_i(s)=1$ for $s\\in T_i$ , and $0$ otherwise .\n","NOUN PHRASES:\n"," ['hierarchical policy', 'set', 'policies', 'subtask', 'O_i', 'terminology', 'option', 'framework', 'subtask policy', 'option', '=1']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 21, 'text': 'A hierarchical policy'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 35, 'end': 69, 'text': 'a set of policies for each subtask'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 62, 'end': 69, 'text': 'subtask'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 44, 'end': 69, 'text': 'policies for each subtask'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 144, 'end': 153, 'text': 'framework'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The \\textbf{second} experiment is running HQI on different variations of hierarchical decomposition of the original MDP .\n","Figure ~ \\ref{fig:DAG2} and Figure ~ \\ref{fig:DAG3} show two different valid DAGs that could also solve the original MDP .\n","Figure ~ \\ref{fig:res2} demonstrates that with sufficient data all three DAG converge to their recursive optimal solution , which confirms that HQI is able converge for different hierarchies .\n","In terms of sample efficiency , three structures demonstrate slight different behavior .\n","We can notice that DAG $2$ learns particularly slower than the other two .\n","We argue that this is because of poor decomposition of the original MDP .\n","Based on the problem settings , \\textit{pick} and \\textit{drop} are all risky actions ( illegal execution lead to $-10$ reward ) , while in DAG $2$ these two actions are mixed with low - cost \\textit{move} actions while the other two DAGs isolated them in a higher level of decision making .\n","Therefore , designing good hierarchy is crucial to obtain performance gain versus flat RL approaches .\n","This emphasizes the importance of the off - policy nature of HQI , which allows developers to experiment with different DAG structures without collecting new samples .\n","How to effectively evaluate the performance of particular hierarchical decomposition without using a simulator is a part of our future research .\n","NOUN PHRASES:\n"," ['experiment', 'HQI', 'different variations', 'hierarchical decomposition', 'original MDP', 'fig', 'DAG2', 'fig', 'DAG3', 'show', 'different valid DAGs', 'also solve', 'original MDP', 'fig', 'res2', 'demonstrates', 'sufficient data', 'DAG converge', 'recursive optimal solution', 'confirms', 'HQI', 'able converge', 'different hierarchies', 'terms', 'sample efficiency', 'structures', 'demonstrate', 'slight different behavior', 'notice', 'DAG', 'learns', 'argue', 'poor decomposition', 'original MDP', 'Based', 'problem settings', 'pick', 'drop', 'risky actions', 'illegal execution lead', 'reward', 'DAG', 'actions', 'cost', 'move', 'actions', 'DAGs', 'isolated', 'level', 'decision making', 'designing', 'good hierarchy', 'obtain', 'performance gain', 'flat RL approaches', 'emphasizes', 'importance', 'policy nature', 'HQI', 'allows', 'developers', 'experiment', 'different DAG structures', 'collecting', 'new samples', 'effectively evaluate', 'performance', 'particular hierarchical decomposition', 'using', 'simulator', 'part', 'future research']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Then for subtasks with other subtask children , by definition , when we run SQI , the children of $O_i$ ( $U_i$ ) have converged to their unique deterministic optimal recursive policy .\n","This means that every action $u \\in U_i$ , is a deterministic \\textit{deterministic Markov option} as defined in the $option$ framework \\cite{sutton1999between} .\n","\\cite{sutton1999between} proved that \" for any set of \\textit{deterministic Markov options} one step intra-option Q - learning converges w. p. 1 to the optimal Q - values , for every option regardless of what options are executed during learning provided every primitive action gets executed in every state infinitely often \" .\n","Refer to the \\cite{sutton1999between} , for the detailed proof .\n","NOUN PHRASES:\n"," ['subtasks', 'other subtask children', 'definition', 'run', 'children', 'O_i', 'U_i', 'have converged', 'unique deterministic optimal recursive policy', 'means', 'action', 'deterministic Markov option', 'defined', 'option', 'proved', 'set', 'deterministic Markov options', 'step intra-option Q', 'learning converges', 'w.', 'optimal Q', 'values', 'option regardless', 'options', 'learning provided', 'primitive action', 'gets executed', 'state', 'Refer', 'detailed proof']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 82, 'end': 103, 'text': 'the children of $O_i$'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 208, 'end': 214, 'text': 'action'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 312, 'end': 321, 'text': 'framework'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An MDP $M$ is described by $(S, A, P, R, P_0)$            \\ item $S$ is the state space of            \\ item $A$ is a set of primitive actions that are available \\\n","item $P(s'|s,a)$ defines the transition probability of executing primitive action $a$ in state ,            \\ item $R(s'|s,a)$ is the reward function defined over $S$ and $A$ \\end{enumerate}\n","NOUN PHRASES:\n"," ['MDP', 'M', 'S', 'A', 'P', 'R', 'P_0', 'S', 'state space', 'item', 'set', 'primitive actions', 'P', \"s'|s\", 'defines', 'transition probability', 'executing', 'primitive action', 'state', 'item', 'R', \"s'|s\", 'reward function', 'defined', 'S', 'enumerate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 6, 'text': 'An MDP'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 72, 'end': 101, 'text': 'the state space of 9999999996'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 116, 'end': 161, 'text': 'a set of primitive actions that are available'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 189, 'end': 271, 'text': 'the transition probability of executing primitive action $a$ in state , 9999999992'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 229, 'end': 245, 'text': 'primitive action'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 294, 'end': 338, 'text': 'the reward function defined over $S$ and $A$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Mostly , we follow the definitions in the MAXQ framework .\n","However , for notation simplicity , we also borrow some notations from the $option$ framework .\n","NOUN PHRASES:\n"," ['follow', 'definitions', 'MAXQ framework', 'notation simplicity', 'also borrow', 'notations', 'option', 'framework']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 143, 'end': 152, 'text': 'framework'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The \\textbf{last} experiment utilizes Random Forests as the function approximator to model the Q - value function in DAG 1 .\n","The main purpose is to demonstrate the convergence of Fitted HQI .\n","For each subtask $O_i$ the Q - value function $Q_i(s, u)$ is modelled by a random forest with $[dest, pass, x, y]$ as the input feature .\n","Since $dest$ and $pass$ are categorical variables , we represent them as a one - hot vector , which transforms the state variable into a $11$ dimension vector ( 4 d for destination , 5 d for passenger and 2 d for the $x,y$ coordinate ) .\n","We report the mean average discounted rewards over 5 independent runs with different random samples of different sizes .\n","Figure ~ \\ref{fig:res3} shows that Fitted - HQI achieves similar performance compared to Tabular HQI .\n","NOUN PHRASES:\n"," ['experiment utilizes Random Forests', 'function approximator', 'model', 'Q', 'value function', 'DAG', 'main purpose', 'demonstrate', 'convergence', 'Fitted HQI', 'O_i', 'Q', 'value function', 'Q_i', 'random forest', '[ dest', 'pass', 'x', 'input feature', 'categorical variables', 'represent', 'hot vector', 'transforms', 'state', 'dimension vector', 'd', 'destination', 'd', 'passenger', 'd', 'coordinate', 'report', 'mean average', 'discounted', 'rewards', 'independent runs', 'different random samples', 'different sizes', 'fig', 'res3', 'shows', 'Fitted', 'HQI', 'achieves', 'similar performance', 'compared', 'Tabular HQI']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 201, 'end': 208, 'text': 'subtask'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 215, 'end': 237, 'text': 'the Q - value function'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 310, 'end': 327, 'text': 'the input feature'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 553, 'end': 563, 'text': 'coordinate'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 358, 'end': 379, 'text': 'categorical variables'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The dataset for each run were collected in advance by choosing actions uniformly at random with different sizes .\n","We evaluate the performance of algorithms by running greedy execution for $100$ times to obtain average discounted return at every $5000$ new samples and up to $60,000$ samples .\n","We repeat the experiments for $5$ times to evaluate the influence of different sample distribution .\n","The discounting factor is set to be $0.99$ .\n","NOUN PHRASES:\n"," ['dataset', 'run', 'advance', 'choosing', 'actions', 'random', 'different sizes', 'evaluate', 'performance', 'algorithms', 'running', 'greedy execution', 'times', 'obtain average discounted', 'return', 'new samples', 'samples', 'repeat', 'experiments', 'times', 'evaluate', 'influence', 'different sample distribution', 'discounting factor']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A \\textit{recursive optimal} policy for MDP $M$ with hierarchical decomposition is a hierarchical policy $\\pi=\\{\\pi_0...\\pi_n\\}$ , such that for each subtask , $O_i$ , the corresponding policy $\\pi_i$ is optimal for the SMDP defined by the set of states , $S_i$ , the set of actions $U_i$ , the state transition probability $P^{\\pi}(s', N|s,a)$ , and the rewards function $R(s'|s,a)$ .\n","NOUN PHRASES:\n"," ['policy', 'MDP', 'M', 'hierarchical decomposition', 'hierarchical policy', 'subtask', 'O_i', 'corresponding policy', 'SMDP', 'defined', 'set', 'states', 'S_i', 'set', 'actions', 'U_i', 'state transition probability', 'P^', 'N|s', 'rewards', 'function', 'R', \"s'|s\"]\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 40, 'end': 43, 'text': 'MDP'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 83, 'end': 104, 'text': 'a hierarchical policy'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 150, 'end': 157, 'text': 'subtask'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 168, 'end': 192, 'text': 'the corresponding policy'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 236, 'end': 253, 'text': 'the set of states'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 264, 'end': 282, 'text': 'the set of actions'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 291, 'end': 323, 'text': 'the state transition probability'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 351, 'end': 371, 'text': 'the rewards function'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The problem formulation is as following : given any finite set of samples , $D=\\big\\{ (s_m, a_m, r_m, s^{'}_m) |m=1,2,...,M \\big\\}$ and any valid hierarchical decomposition $O$ of the original MDP $M$ , we wish to learn the recursive optimal hierarchical policy $\\pi^{*}$ .\n","NOUN PHRASES:\n"," ['problem formulation', 'following', 'given', 'finite set', 'samples', 's_m', 'a_m', 'r_m', '_m', '|m=1,2', 'valid hierarchical decomposition', 'O', 'original MDP', 'M', 'wish', 'learn', 'recursive optimal hierarchical policy', '*']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 52, 'end': 73, 'text': 'finite set of samples'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 140, 'end': 172, 'text': 'valid hierarchical decomposition'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 180, 'end': 196, 'text': 'the original MDP'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 220, 'end': 261, 'text': 'the recursive optimal hierarchical policy'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The \\textbf{first} experiment compares HQI against flat Q - value Iteration ( FQI ) .\n","Also , as pointed out in ~ \\cite{dietterich2000hierarchical} , state abstraction is essential for MAXQ to have fast learning speed compared to flat Q learning .\n","As a result , we manually conduct state abstraction for each subtask in DAG $1$ .\n","However , different from the aggressive state abstraction described in ~ \\cite{dietterich2000hierarchical} , where every subtask and child pair has a different set of state variables , we only conduct a simple state abstraction at subtask level , i.e. all children of a subtask has the same state abstraction .\n","The final state abstraction is listed in Table ~ \\ref{tbl:DAG1} .\n","As described above , we run $5$ independent runs with different random samples of different sizes , we report the mean average discounted return over five runs in Figure ~ \\ref{fig:res0} , as well as the best average discounted reward of the five runs in Figure ~ \\ref{fig:res1} .\n","NOUN PHRASES:\n"," ['experiment compares', 'HQI', 'flat Q', 'value Iteration', 'FQI', 'pointed', 'state abstraction', 'MAXQ', 'have', 'fast learning', 'speed', 'compared', 'flat Q learning', 'result', 'manually conduct', 'state abstraction', 'subtask', 'DAG', 'aggressive state abstraction', 'described', 'subtask', 'child pair', 'has', 'different set', 'state variables', 'only conduct', 'simple state abstraction', 'subtask level', 'i.e', 'children', 'subtask', 'has', 'same state abstraction', 'final state abstraction', 'tbl', 'DAG1', 'described', 'run', 'independent runs', 'different random samples', 'different sizes', 'report', 'mean average', 'discounted', 'return', 'runs', 'fig', 'res0', 'average', 'discounted', 'reward', 'runs', 'fig', 'res1']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We applied our algorithm to the Taxi domain described in \\cite{sutton1999between} .\n","This is a simple grid world that contains a taxi , a passenger , and four specially - designated locations labeled R , G , B , and Y .\n","In the starting state , the taxi is in a randomly - chosen cell of the grid , and the passenger is at one of the four special locations .\n","The passenger has a desired destination that he / she wishes to reach , and the job of the taxi is to go to the passenger , pick him / her up , go to the passenger 's destination , and drop the passenger .\n","The taxi has six primitive actions available to it : move one step to one of the four directions ( north , south , east and west ) , pick up the passenger and put down the passenger .\n","To make the task more difficult , the move actions are not deterministic , so that it has $20\\%$ chance of moving in one of the other directions .\n","Also , every move in the grid will cost $-1$ reward .\n","Attempting to pick up or drop passenger at wrong location will cause $-10$ reward .\n","At last , successfully finish the task has $20$ reward .\n","The grid is described in figure \\ref{fig:taxi0} .\n","Therefore , there are 4 possible state for the destination , 5 possible state for the passenger ( 4 location and 5 is on the car ) , 25 possible locations , which results into $500*6=3000$ parameters in the Q - table that needs to be learned .\n","We denote the state variable as $[dest, pass, x, y]$ for later discussion .\n","NOUN PHRASES:\n"," ['applied', 'algorithm', 'Taxi domain', 'described', 'simple grid world', 'contains', 'taxi', 'passenger', 'designated locations', 'labeled', 'R', 'G', 'B', 'Y', 'starting state', 'taxi', 'chosen cell', 'grid', 'passenger', 'special locations', 'passenger', 'has', 'desired', 'destination', '/', 'wishes', 'reach', 'job', 'taxi', 'go', 'passenger', 'pick', '/', 'go', 'passenger', 'destination', 'drop', 'passenger', 'taxi', 'has', 'primitive actions', 'move', 'step', 'directions', 'pick', 'passenger', 'put', 'passenger', 'make', 'task', 'move actions', 'has', '%', 'chance', 'moving', 'other directions', 'move', 'grid', 'cost', 'reward', 'Attempting', 'pick', 'drop', 'passenger', 'wrong location', 'cause', 'reward', 'successfully finish', 'task', 'has', 'reward', 'grid', 'fig', 'taxi0', 'possible state', 'destination', 'possible state', 'passenger', 'location', 'car', 'possible locations', 'results', 'parameters', 'Q', 'table', 'needs', 'denote', 'state', 'pass', 'x', 'discussion']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 1393, 'end': 1411, 'text': 'the state variable'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," One challenge of training a subtask with subtask children is that we cannot use the optimal SMDP Bellman equation described in the MAXQ framework ~ \\cite{dietterich2000overview} , $Q_i(s, u)$ , which is the Q - value function for subtask , $O_i$ , at state , $s$ and action $u$ :\n","\\begin{equation}\n","\\label{eq:maxqq}\n","Q_i(s, u) = V(s, u) + \\sum_{s', N} P_i^{\\pi}(s', N | s, u)\\gamma^N Q_i^{\\pi}(s', \\pi_i(s')) \\\\\n","\\end{equation}\n","\\begin{equation}\n","V(s, u) = \\begin{cases}\n","max_{u'}(Q_u(s, u')) &\\text{            is subtask}\\\\\n","\\sum_{s'} P(s'|s,u)R(s'|s,u) &\\text{            is primitive}\n","\\end{cases}\n","\\end{equation}\n","The main problem of this equation \\eqref{eq:maxqq} is that in order to estimate the Q - value for a subtask children , $u$ , the parent $O_i$ needs to have an estimate about the transition probability $P_i^{\\pi}(s', N | s, u)$ , which is the distribution of $u$ 's exit state and number of primitive steps needed to reach its termination .\n","Although the termination states of the child $u$ are given by $T_u$ , it is difficult to estimate the joint distribution of termination steps $N$ and            ' if $u$ follows an policy that is different from the behavior policy without recollecting new samples .\n","This is because since the behavior policy is usually random and poor in performance , the collected samples do not provide information about how many steps the subtask $u$ would take to terminate if following a different ( optimal ) policy .\n","NOUN PHRASES:\n"," ['challenge', 'training', 'subtask', 'subtask children', 'not use', 'optimal SMDP Bellman equation', 'described', 'dietterich2000overview', 'Q_i', 'Q', 'value function', 'subtask', 'O_i', 'state', 'action', 'equation', 'eq', 'maxqq', 'Q_i', '= V', '+', 's', 'N', 'P_i^', 'N | s', 'u', 'Q_i^', 'equation', 'equation', 'V', '=', 'cases', 'max_', 'Q_u', 's', 'P', \"s'|s\", 'u', 'R', \"s'|s\", 'u', 'cases', 'equation', 'main problem', 'eq', 'maxqq', 'order', 'estimate', 'Q', 'value', 'subtask children', 'parent', 'O_i', 'needs', 'have', 'estimate', 'transition probability', 'P_i^', 'N | s', 'u', 'distribution', 'exit state', 'number', 'primitive steps', 'needed', 'reach', 'termination', 'termination states', 'T_u', 'estimate', 'joint distribution', 'termination steps', 'N', 'follows', 'policy', 'behavior policy', 'recollecting', 'new samples', 'behavior policy', 'performance', 'collected samples', 'do', 'not provide', 'information', 'many steps', 'take', 'terminate', 'following', 'policy']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 203, 'end': 277, 'text': 'the Q - value function for subtask , $O_i$ , at state , $s$ and action $u$'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 230, 'end': 237, 'text': 'subtask'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 251, 'end': 256, 'text': 'state'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 267, 'end': 273, 'text': 'action'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 706, 'end': 724, 'text': 'a subtask children'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 733, 'end': 743, 'text': 'the parent'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 782, 'end': 808, 'text': 'the transition probability'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 846, 'end': 945, 'text': \"the distribution of $u$ 's exit state and number of primitive steps needed to reach its termination\"}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 983, 'end': 992, 'text': 'the child'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 1370, 'end': 1381, 'text': 'the subtask'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We now propose Hierarchical Q - value Iteration , HQI , and we prove that it converges to the recursive optimal solution for any hierarchical decomposition given that the batch sample distribution has sufficient state action exploration .\n","The basic idea is to train every subtask using Subtask Q - value Iteration ( SQI ) in a bottom up fashion .\n","The training prerequisite of SQI for a specific subtask $O_i$ is that all of its children $U_i$ have converged to their greedy optimal policies .\n","In order to fulfil this constraint , HQI first topologically sorts the \\textit{DAG} and running SQI from subtasks whose children have only primitive actions .\n","After those subtasks converge to their optimal policy , the algorithm continues to other subtasks whose children are either converged or primitive actions .\n","We will show that there always exist an ordering of training every subtask in a valid \\textit{DAG} that fulfills the prerequisite of SQI .\n","NOUN PHRASES:\n"," ['now propose', 'Hierarchical Q', 'value Iteration', 'HQI', 'prove', 'converges', 'recursive optimal solution', 'hierarchical decomposition', 'given', 'batch sample distribution', 'has', 'sufficient state action exploration', 'basic idea', 'train', 'subtask', 'using', 'Subtask Q', 'value Iteration', 'SQI', 'bottom', 'fashion', 'training prerequisite', 'SQI', 'O_i', 'children', 'U_i', 'have converged', 'greedy optimal policies', 'order', 'fulfil', 'constraint', 'HQI', 'first topologically sorts', 'DAG', 'running', 'SQI', 'subtasks', 'children', 'have', 'primitive actions', 'subtasks', 'converge', 'optimal policy', 'algorithm', 'continues', 'other subtasks', 'children', 'converged', 'primitive actions', 'show', 'always exist', 'ordering', 'training', 'subtask', 'DAG', 'fulfills', 'prerequisite', 'SQI']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 395, 'end': 402, 'text': 'subtask'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 428, 'end': 436, 'text': 'children'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\textbf{Step 2}\n","By definition , a hierarchical decomposition is a Directed Acyclic Graph ( DAG ) with edges from parents to their children .\n","In this proof , we first reverse the edges so that they are from children to their parents .\n","Also we know from Graph Theory that any Directed Acyclic Graph has at least one topological sort , such that every edge $uv$ , $u$ comes before $v$ in the ordering ~ \\cite{cormen2001section} .\n","Therefore , we can to topologically sort the hierarchical decomposition with reversed edges such that SQI can always train the children before parents .\n","NOUN PHRASES:\n"," ['Step', 'definition', 'hierarchical decomposition', 'Directed Acyclic Graph', 'DAG', 'edges', 'parents', 'children', 'proof', 'first reverse', 'edges', 'children', 'parents', 'know', 'Graph Theory', 'Directed Acyclic Graph', 'has', 'topological sort', 'edge', 'comes', 'ordering', 'cormen2001section', 'topologically sort', 'hierarchical decomposition', 'reversed edges', 'SQI', 'always train', 'children', 'parents']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 349, 'end': 353, 'text': 'edge'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ noindent \\textbf{Hint:}\n","Predicate $\\mathtt{road}$ was used with arity $1$ which is unexpected .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Hint', 'Predicate', 'road', 'quotation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 37, 'end': 46, 'text': 'Predicate'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example with $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{preprocessing_err_2}$ we obtain $\\mi{wrongarity}(P_U,P_R) = \\{ (\\mt{road},1) \\}$ and we can give the following hint accordingly .\n","NOUN PHRASES:\n"," ['example', 'eqSymmetry', 'preprocessing_err_2', 'obtain', 'wrongarity', 'P_U', 'P_R', '=', 'road', ',1', 'give', 'following hint']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Note that if the student is unable to correct the problem after this hint , the next hint can reveal that the arity of $\\mt{road}$ in the sample solution is $2$ , without revealing the full solution .\n","NOUN PHRASES:\n"," ['Note', 'student', 'correct', 'problem', 'hint', 'next hint', 'reveal', 'arity', 'road', 'sample solution', 'revealing', 'full solution']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example if $P_R = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{eqExpected}$ and $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{preprocessing_err_1}$ then we obtain $\\mi{wrongpred}(P_U,P_R) = \\{ \\mt{obstacle} \\}$ .\n","This allows us to easily produce the following hint .\n","NOUN PHRASES:\n"," ['example', 'eqSymmetry', 'eqExpected', 'eqSymmetry', 'preprocessing_err_1', 'obtain', 'wrongpred', 'P_U', 'P_R', '=', 'obstacle', 'allows', 'easily produce', 'following hint']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ noindent \\textbf{Hint:}\n","Predicate $\\mathtt{obstacle}$ should not be used .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Hint', 'Predicate', 'obstacle', 'quotation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 37, 'end': 46, 'text': 'Predicate'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example with $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{preprocessing_err_3}$ we obtain $\\mi{wrongcons}(P_U,P_R) = \\{ \\mt{x}, \\mt{y} \\}$ and we can give the following hint accordingly .\n","NOUN PHRASES:\n"," ['example', 'eqSymmetry', 'preprocessing_err_3', 'obtain', 'wrongcons', 'P_U', 'P_R', '=', 'y', 'give', 'following hint']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If the student is not able to fix the problem , we can also mention the predicate .\n","\\begin{quotation} \\ noindent \\textbf{Hint:}\n","The answer set contains more true atoms of predicate $\\mt{open\\_road}$ than it should .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['student', 'fix', 'problem', 'also mention', 'predicate', 'quotation', 'Hint', 'answer set', 'contains', 'true atoms', 'quotation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 171, 'end': 180, 'text': 'predicate'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example the answer \\ref{semantic_err_1} works correctly only for obstacles between DÃƒÂ¼zce and Bolu , but not between DÃƒÂ¼zce and Zonguldak .\n","Given $P_R$ as above and $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{semantic_err_1}$ we obtain one answer set $I_R \\ins \\AS(P_R)$ and one answer set $I_U \\ins \\AS(P_U)$ such that $I_R \\setminus I_U = \\emptyset$ because the student 's solution reproduces all atoms that are true in the reference solution , moreover $I_U \\setminus I_R = \\{\n","\\mt{open\\_road(duzce,zonguldak)},\\allowbreak\n","\\mt{open\\_road(zonguldak,duzce)} \\}$ because the student 's solution additionally produces true atoms that should not be true .\n","NOUN PHRASES:\n"," ['example', 'semantic_err_1', 'works', 'obstacles', 'DÃƒÂ¼zce', 'Bolu', 'DÃƒÂ¼zce', 'Zonguldak', 'Given', 'P_R', 'above', 'eqSymmetry', 'semantic_err_1', 'obtain', 'answer', 'set', 'P_R', 'answer', 'set', 'P_U', 'student', 'solution', 'reproduces', 'atoms', 'reference solution', 'duzce', 'zonguldak', 'zonguldak', 'duzce', 'student', 'solution', 'additionally produces', 'true atoms']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 261, 'end': 271, 'text': 'answer set'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 300, 'end': 310, 'text': 'answer set'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\noindent\n","\\textbf{Hint:}\n","The program contains the following unexpected constants which are not required in the solution :\n","$\\mt{x}$ ,            .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Hint', 'program', 'contains', 'following unexpected constants', 'not required', 'solution', 'x', 'quotation']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," {\\bf Further Hints .~~}\n","Using $\\mi{bpreds}$ , $\\mi{hpreds}$ , $\\mi{bpredarities}$ , and $\\mi{hpredarities}$ , further hints that are more specific , can be produced , for example that a certain predicate should be used in the body of a rule in the solution .\n","NOUN PHRASES:\n"," ['Further Hints', '.~~', 'Using', 'bpreds', 'hpreds', 'bpredarities', 'hpredarities', 'further hints', 'example', 'certain predicate', 'body', 'rule', 'solution']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If even this does not help , we can concretely say which true atom should not be true .\n","\\begin{quotation} \\ noindent \\textbf{Hint:}\n","The answer set contains true atoms which should be false :\n","$\\mt{open\\_road(duzce,zonguldak)}$ and $\\mt{open\\_road(zonguldak,duzce)}$ .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['does', 'not help', 'concretely say', 'true atom', 'quotation', 'Hint', 'answer set', 'contains', 'true atoms', 'duzce', 'zonguldak', 'zonguldak', 'duzce', 'quotation']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Answer Set Programming ( ASP ) is a declarative logic programming paradigm \\cite{Gelfond1988,Lifschitz2008,Gebser2012aspbook} .\n","An atom is form $p(x_1,\\ldots,x_l)$ with $0 \\les l$ , and if $l\\eqs0$ we write the atom short as $p$ .\n","A program $P$ consists of a set of rules , a rule $r$ is of the form\n","\\begin{equation}\n","\\label{eqRule}\n","\\mathtt{\n","\\alpha_1 \\lor \\cdots \\lor \\alpha_k\n","\\leftimpl \\beta_1, \\ldots, \\beta_n,\n","\\naf \\beta_{n+1}, \\ldots, \\naf \\beta_m\n","}\n","\\end{equation}\n","where $\\alpha_i$ and $\\beta_i$ are atoms , called head and body atoms of $r$ , respectively .\n","We say that $H(r) = \\{ \\alpha_1,\\ldots, \\alpha_k \\}$ is the \\emph{head} of $r$ , and $B^+(r) = \\{ \\beta_1,\\ldots,\\beta_n\\}$ , respectively $B^-(r) = \\{ \\beta_{n+1},\\ldots,\\beta_m\\}$ are the positive , respectively negative \\emph{body} of $r$ .\n","We call a rule a \\emph{fact} if $m \\eqs 0$ , \\emph{disjunctive} if $k \\gts 1$ , and a \\emph{constraint} if $k \\eqs 0$ .\n","Atoms can contain constants , variables , and function terms , and a program must allow for a finite instantiation .\n","Semantics of an ASP program $P$ is defined based on the ground instantiation $\\grnd(P)$ and Herbrand base $\\HB_P$ of $P$ : an interpretation $I \\ins \\HB_P$ satisfies a rule $r$ iff $H(r) \\caps I \\neqs \\emptyset$ or $B^+(r) \\nsubseteqs I$ or $B^-(r) \\caps I \\neqs \\emptyset$ ; $I$ is a model of $P$ if it satisfies all rules in $P$ .\n","The reduct $P^I$ of $P$ wrt .\n","\\ $I$ is the set of rules $P^I \\eqs \\{ H(r) \\leftimpl B^+(r) \\mids B^-(r) \\caps I \\eqs \\emptyset \\}$ and an interpretation\n","$I$ is an answer set iff it is a $\\subseteq$ - minimal model of $P^I$ .\n","Details of syntactic restrictions , additional syntactic elements , and semantics of ASP are described in the ASP - Core - 2 standard ~% \\cite{Calimeri2012} .\n","NOUN PHRASES:\n"," ['Answer Set Programming', 'ASP', 'declarative logic', 'programming', 'Gelfond1988', 'Lifschitz2008', 'Gebser2012aspbook', 'atom', 'p', 'x_l', 'write', 'atom', 'program', 'P', 'consists', 'set', 'rules', 'rule', 'r', 'form', 'equation', 'eqRule', 'equation', 'called', 'head', 'body atoms', 'say', 'H', 'r', '=', 'head', 'B^+', 'r', '=', 'B^-', 'r', '=', 'body', 'call', 'rule', 'fact', 'm', 'k', 'constraint', 'k', 'Atoms', 'contain', 'constants', 'variables', 'function terms', 'program', 'allow', 'finite instantiation', 'Semantics', 'ASP program', 'P', 'ground instantiation', 'P', 'Herbrand', 'base', 'P', 'interpretation', 'satisfies', 'rule', 'r', 'H', 'r', 'B^+', 'r', 'B^-', 'r', 'model', 'P', 'satisfies', 'rules', 'P', 'reduct', 'P^I', 'P', 'wrt', 'set', 'rules', 'H', 'r', 'B^+', 'r', 'B^-', 'r', 'interpretation', 'answer set iff', 'minimal model', 'P^I', 'Details', 'syntactic restrictions', 'additional syntactic elements', 'semantics', 'ASP', 'ASP', 'Core', '%', 'Calimeri2012']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 128, 'end': 135, 'text': 'An atom'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 207, 'end': 215, 'text': 'the atom'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 231, 'end': 240, 'text': 'A program'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 274, 'end': 280, 'text': 'a rule'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 503, 'end': 508, 'text': 'atoms'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 518, 'end': 522, 'text': 'head'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 527, 'end': 544, 'text': 'body atoms of $r$'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 618, 'end': 640, 'text': 'the \\\\emph{head} of $r$'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 785, 'end': 803, 'text': '\\\\emph{body} of $r$'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 752, 'end': 760, 'text': 'positive'}, 'T22': {'eid': 'T22', 'label': 'PRIMARY', 'start': 776, 'end': 803, 'text': 'negative \\\\emph{body} of $r$'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 1056, 'end': 1070, 'text': 'an ASP program'}, 'T25': {'eid': 'T25', 'label': 'PRIMARY', 'start': 1095, 'end': 1119, 'text': 'the ground instantiation'}, 'T27': {'eid': 'T27', 'label': 'PRIMARY', 'start': 1135, 'end': 1148, 'text': 'Herbrand base'}, 'T32': {'eid': 'T32', 'label': 'PRIMARY', 'start': 1166, 'end': 1183, 'text': 'an interpretation'}, 'T34': {'eid': 'T34', 'label': 'PRIMARY', 'start': 1209, 'end': 1215, 'text': 'a rule'}, 'T44': {'eid': 'T44', 'label': 'PRIMARY', 'start': 1326, 'end': 1340, 'text': 'a model of $P$'}, 'T45': {'eid': 'T45', 'label': 'PRIMARY', 'start': 1376, 'end': 1386, 'text': 'The reduct'}, 'T51': {'eid': 'T51', 'label': 'PRIMARY', 'start': 1415, 'end': 1431, 'text': 'the set of rules'}, 'T54': {'eid': 'T54', 'label': 'PRIMARY', 'start': 1511, 'end': 1528, 'text': 'an interpretation'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," While writing programs students are making different kind of mistakes .\n","Our methodology for giving hints is shown in Figure ~ \\ref{figFlowchart} .\n","The input consists of the student 's answer program $P_U$ and the reference solution $P_R$ , where we assume that all parts of the example are combined into a single program .\n","Analyzing the input and giving hints is based on three phases which address different kinds of mistakes .\n","NOUN PHRASES:\n"," ['writing', 'programs students', 'different kind', 'mistakes', 'methodology', 'giving', 'hints', 'figFlowchart', 'input', 'consists', 'student', 'program', 'P_U', 'reference solution', 'P_R', 'assume', 'parts', 'example', 'single program', 'Analyzing', 'input', 'giving', 'hints', 'phases', 'address', 'different kinds', 'mistakes']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 169, 'end': 198, 'text': \"the student 's answer program\"}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 209, 'end': 231, 'text': 'the reference solution'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 147, 'end': 156, 'text': 'The input'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\\n","noindent \\textbf{Question:} {\\ it\n","Write a rule which defines predicate $\\mathtt{open\\_road(From, To)}$ that is true for all pairs of cities where the direct road connection from $\\mathtt{From}$ to $\\mathtt{To}$ is not blocked . }\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Question', 'Write', 'rule', 'defines', 'From', 'pairs', 'cities', 'direct road connection', 'not blocked', 'quotation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 74, 'end': 83, 'text': 'predicate'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{quotation} {\\ it\n","We have a graph which represents the map of cities connected by roads .\n","Cities are nodes , roads are edges , all roads are bidirectional , and roads might be blocked .\n","The task is is to find a route from city $X$ to city $Y$ in the road network such that the path uses only roads that are not blocked .\n","The graph is shown in Figure ~ \\ref{figCities} and is represented in the following set of facts . }\n","\\end{quotation}\n","\\begin{equation}\n","\\label{eqExGiven}\n","\\begin{split}\n","\\begin{array}{@{\\!\\!}l@{\\ }l@{}}\n","\\mathtt{road(istanbul,kocaeli).}\n","&\\mathtt{road(karabuk,bolu).}\\\\\n","\\mathtt{road(kocaeli,sakarya).}\n","&\\mathtt{road(duzce,karabuk).}\\\\\n","\\mathtt{blocked(duzce,zonguldak).}\n","&\\mathtt{road(bolu,zonguldak).}\\\\\n","\\mathtt{road(duzce,zonguldak).}\n","&\\mathtt{road(sakarya,duzce).}\n","\\end{array}\n","\\end{split}\n","\\end{equation}\n","\\begin{quotation} {\\ it Moreover the fact that roads are bidirectional is represented in the following rule . }\n","\\end{quotation}\n","\\begin{equation}\n","\\label{eqSymmetry}\n","\\mathtt{road(X,Y)} \\ \\leftimpl \\ \\mathtt{road(Y,X).}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['quotation', 'have', 'graph', 'represents', 'map', 'cities', 'connected', 'roads', 'Cities', 'nodes', 'roads', 'edges', 'roads', 'roads', 'task', 'find', 'route', 'city', 'X', 'city', 'Y', 'road network', 'path', 'uses', 'roads', 'not blocked', 'graph', 'figCities', 'following set', 'facts', 'quotation', 'equation', 'split', 'array', 'l @', 'road', 'road', 'bolu', 'road', 'sakarya', 'road', 'duzce', 'karabuk', 'blocked', 'duzce', 'zonguldak', 'road', 'bolu', 'zonguldak', 'road', 'duzce', 'zonguldak', 'road', 'duzce', 'array', 'split', 'equation', 'quotation', 'fact', 'roads', 'following rule', 'quotation', 'equation', 'eqSymmetry', 'road', 'X', 'Y', 'road', 'Y', 'X', 'equation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 228, 'end': 232, 'text': 'city'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 240, 'end': 244, 'text': 'city'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This indicates the location of the mistake in the source code ( line $3$ , characters $8{-}9$ ) and allows us to display the error with additional visual support to the student , for example as the following hint .\n","\\begin{align*}\n","\\begin{array}\n","{@{}l@{}}\n","           \\\\\n","           \\\\\n","           \\up arrow }\\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","\\text{or similar.}  \\end{array}  \\end{align*}\n","NOUN PHRASES:\n"," ['indicates', 'location', 'mistake', 'source code', 'line', 'characters', 'allows', 'display', 'error', 'additional visual support', 'student', 'example', 'following hint', 'align*', 'array', 'l @', 'array', 'align*']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Given a set of atoms $A \\eqs \\{ p_1(t_1,\\ldots,t_{k_1}), \\ldots, p_m(t_1,\\ldots,t_{k_m}) \\}$ we define function $\\mi{preds}(A)$ which returns the set of predicates in $A$ , the function $\\mi{predarities}(A)$ which returns a set of tuples of predicates with their arity as occuring in $A$ , and the function $\\mi{constants}(A)$ which returns the constants used in $A$ .\n","\\begin{align} & & \\mi{preds} ( A ) \\eqs& \\{ {p} &\\ mids & p ( t_1,\\ldots , t_k ) \\ins A \\} && &\\\\ &&\n","\\quad\\mi{predarities} ( A ) \\eqs& \\{ {(p,k)}            &\\ mids & p ( t_1 ,\\ldots , t_k ) \\ins A \\} && &\\\\ && \\mi{constants} ( A ) \\eqs& \\{ {t_i} & \\mids&p(t_1,\\ldots,t_k) \\ins A,\\ 1 \\les i \\les k, \\text{and } t_i \\text{ is a constant term}  \\}. && \\hspace*{2em} & \\end{align}\n","NOUN PHRASES:\n"," ['Given', 'set', 'atoms', 'p_1', 't_1', 'k_1', 't_1', 'k_m', 'define', 'preds', 'returns', 'set', 'predicates', 'function', 'predarities', 'returns', 'set', 'tuples', 'predicates', 'arity', 'occuring', 'function', 'constants', 'returns', 'constants', 'used', 'align', 'preds', 'p', 'p', 't_1', 't_k', 'predarities', 'p', 'k', 'p', 't_1', 't_k', 'constants', 't_i', 'p', 't_1', 't_k', 'A', 'k', 't_i', 'constant term', 'align']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 6, 'end': 20, 'text': 'a set of atoms'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 103, 'end': 111, 'text': 'function'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 173, 'end': 185, 'text': 'the function'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 294, 'end': 306, 'text': 'the function'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 694, 'end': 709, 'text': 'a constant term'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Given a reference solution $P_R$ and student input $P_U$ we can use the above definitions to give hints without revewaling $P_R$ .\n","NOUN PHRASES:\n"," ['Given', 'reference solution', 'P_R', 'student', 'P_U', 'use', 'above definitions', 'give', 'hints', 'revewaling', 'P_R']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 6, 'end': 26, 'text': 'a reference solution'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 37, 'end': 50, 'text': 'student input'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We next define functions for obtaining information about predicates and their arities in heads and bodies of rules and programs .\n","\\begin{align*}\n","\\mi{bpreds} ( r ) \\eqs& \\mi{preds} ( B ^{+ } ( r ) \\cups B^{-} ( r ) ) \\\\ \\mi{hpreds} ( r ) \\eqs& \\mi{preds} ( H ( r ) ) \\\\\n","\\mi{bpredarities} ( r ) \\eqs& \\mi{predarities} ( B ^{+ } ( r ) \\cups B^{-} ( r ) ) \\\\ \\mi{hpredarities} ( r ) \\eqs& \\mi{predarities} ( H ( r ) ) \\\\ \\intertext{where            is a rule of form \\eqref{eqRule} , moreover }\n","\\mi{bpreds} ( P ) \\eqs& \\bigcup_{r \\ins P}  \\mi{bpreds} ( r ) \\\\\n","\\mi{hpreds} ( P ) \\eqs& \\bigcup_{r \\ins P}\n","\\mi{hpreds} ( r ) \\\\\n","\\mi{bpredarities} ( P ) \\eqs& \\bigcup_{r \\ins P}  \\mi{bpredarities} ( r ) \\\\\n","\\mi{hpredarities} ( P ) \\eqs& \\bigcup_{r \\ins P}  \\mi{hpredarities} ( r ) \\\\\n","\\mi{preds} ( P ) \\eqs& \\mi{bpreds} ( P ) \\cups \\mi{hpreds} ( P ) \\\\\n","\\mi{predarities} ( P ) \\eqs& \\mi{bpredarities} ( P ) \\cups \\mi{hpredarities} ( P ) \\end{align*} where $P$ is a program --- a set of rules of form \\eqref{eqRule} .\n","NOUN PHRASES:\n"," ['next define functions', 'obtaining', 'information', 'predicates', 'arities', 'heads', 'bodies', 'rules', 'programs', 'align*', 'bpreds', 'r', 'preds', 'B ^', '+', 'r', 'B^', 'r', 'hpreds', 'r', 'preds', 'H', 'r', 'bpredarities', 'r', 'predarities', 'B ^', '+', 'r', 'B^', 'r', 'hpredarities', 'r', 'predarities', 'H', 'r', 'rule', 'eqRule', 'bpreds', 'P', 'r', 'P', 'bpreds', 'r', 'hpreds', 'P', 'r', 'P', 'hpreds', 'r', 'bpredarities', 'P', 'r', 'P', 'bpredarities', 'r', 'hpredarities', 'P', 'r', 'P', 'hpredarities', 'r', 'preds', 'P', 'bpreds', 'P', 'hpreds', 'P', 'predarities', 'P', 'bpredarities', 'P', 'hpredarities', 'P', 'align*', 'P', 'program', 'set', 'rules', 'eqRule']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 951, 'end': 960, 'text': 'a program'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 965, 'end': 1002, 'text': 'a set of rules of form \\\\eqref{eqRule}'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure ~ \\ref{fig:turks} shows box plots of the precision for the four models on three corpora .\n","In most cases , HD TM performs the best .\n","As in ~ \\cite{Chang2009} , the likelihood scores do not necessarily correspond to human judgments .\n","Paired , two - tailed t - tests of statistical significants ( $p<0.05$ ) performed between HD TM $\\gamma=0.95$ and $\\gamma=0.05$ and the other models are represented by $\\ast$ and $\\circ$ in Figure ~ \\ref{fig:turks} respectively .\n","NOUN PHRASES:\n"," ['Figure', '~', 'fig', 'turks', 'shows', 'box plots', 'precision', 'models', 'corpora', 'cases', 'HD TM', 'performs', 'Chang2009', 'likelihood scores', 'do', 'not necessarily correspond', 'human judgments', 'Paired', 'tailed', 'tests', 'statistical significants', 'p', 'performed', 'HD TM', 'other models', 'fig', 'turks']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $\\mathbbm{1}(\\cdot)$ is the indicator function and $J$ is the number of judges .\n","The model precision is basically the fraction of judges agreeing with the model .\n","NOUN PHRASES:\n"," ['indicator function', 'J', 'number', 'judges', 'model precision', 'fraction', 'judges', 'agreeing', 'model']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 41, 'end': 63, 'text': 'the indicator function'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 75, 'end': 95, 'text': 'the number of judges'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 89, 'end': 95, 'text': 'judges'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ centering \\includegraphics[width=.85\\textwidth]{./fig/dblphierarchy.pdf}  \\caption{Constructed hierarchy of bibliographic network with HDTM            . Words at the root document represent the most probable words in the root topic. Most probable words for other documents are not shown due to space constraints.}\n","\\label{fig:dblphierarchy}  \\end{figure}\n","NOUN PHRASES:\n"," ['centering', './fig/dblphierarchy.pdf', 'Constructed hierarchy', 'bibliographic network', 'HDTM', 'Words', 'root document', 'represent', 'probable words', 'root topic', 'probable words', 'other documents', 'not shown', 'space constraints', 'fig', 'dblphierarchy', 'figure']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In classic Gibbs sampling , maximum a posteriori estimation is determined by the the mode of the samples .\n","For some node $d$ , this translates to choosing the parent that appeared in the most samples as the final parent ; in the above case node $d$ would chose node $y$ as its parent because node $y$ was sampled 15 times compared to only 5 samples of node $x$ .\n","To answer the second question , and therefore model the certainty of an inferred hierarchy , we calculate the number of times some node $d$ picks its final parent over the total number of samples normalized by the number of possible choices , { \\em i.e. } , parents , node $d$ has .\n","This results in a certainty score for node $d$ :\n","NOUN PHRASES:\n"," ['classic Gibbs sampling', 'maximum', 'posteriori estimation', 'mode', 'samples', 'translates', 'choosing', 'parent', 'appeared', 'samples', 'final parent', 'above case', 'chose', 'parent', 'times', 'compared', 'samples', 'answer', 'second question', 'therefore model', 'certainty', 'inferred hierarchy', 'calculate', 'number', 'times', 'picks', 'final parent', 'total number', 'samples', 'normalized', 'number', 'possible choices', 'parents', 'has', 'results', 'certainty score']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 116, 'end': 120, 'text': 'node'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 240, 'end': 244, 'text': 'node'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 261, 'end': 265, 'text': 'node'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 292, 'end': 296, 'text': 'node'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 352, 'end': 356, 'text': 'node'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 494, 'end': 498, 'text': 'node'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 631, 'end': 635, 'text': 'node'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 684, 'end': 688, 'text': 'node'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," where $n$ represents the total number of samples , $n_{p}$ is the number of times the final parent $p$ is sampled , and $deg^{-}(d)$ is the indegree of node $d$ representing the total number of parents that node $d$ could pick from .\n","In the certainty score the outside fraction is used to measure the normalized difference between the raw probability and probability of random guess .\n","Applying this function to the above example we would calculate that the certainty score of node $d$ would be $(\\frac{15}{20}-\\frac{1}{2}) / \\frac{15}{20} = .33$ .\n","NOUN PHRASES:\n"," ['represents', 'total number', 'samples', 'p', 'number', 'times', 'final parent', 'deg^', 'd', 'indegree', 'representing', 'total number', 'parents', 'node', 'pick', 'certainty', 'score', 'outside fraction', 'measure', 'normalized difference', 'raw probability', 'probability', 'random guess', 'Applying', 'function', 'above example', 'calculate', 'certainty score', '/']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 21, 'end': 48, 'text': 'the total number of samples'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 41, 'end': 48, 'text': 'samples'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 62, 'end': 113, 'text': 'the number of times the final parent $p$ is sampled'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 76, 'end': 113, 'text': 'times the final parent $p$ is sampled'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 82, 'end': 98, 'text': 'the final parent'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 136, 'end': 160, 'text': 'the indegree of node $d$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 152, 'end': 156, 'text': 'node'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 174, 'end': 231, 'text': 'the total number of parents that node $d$ could pick from'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 184, 'end': 231, 'text': 'number of parents that node $d$ could pick from'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 207, 'end': 211, 'text': 'node'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 476, 'end': 480, 'text': 'node'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The bibliography network data had relatively low precision scores .\n","This is almost certainly because it was more difficult for the judges , who were probably not computer scientists , to differentiate between the topics in research paper titles .\n","Figure ~ \\ref{fig:dblphierarchy} shows a small portion of the document hierarchy for the bibliographic network data set constructed with HDTM $\\gamma=.95$ .\n","The root document has 20 children in the hierarchy despite having 145 in - collection links .\n","The remaining 120 documents live deeper in the hierarchy because HDTM has determined that they are too specific to warrant a first level position , and have a better fit in one of the subtrees .\n","NOUN PHRASES:\n"," ['bibliography network data', 'had', 'low precision scores', 'judges', 'computer scientists', 'differentiate', 'topics', 'research paper titles', 'fig', 'dblphierarchy', 'shows', 'small portion', 'document hierarchy', 'bibliographic network data', 'set constructed', 'HDTM', 'root document', 'has', 'children', 'hierarchy', 'having', 'collection links', 'remaining', 'documents live deeper', 'hierarchy', 'HDTM', 'has determined', 'warrant', 'first level position', 'have', 'fit', 'subtrees']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{thebibliography} { 55 } % \\ makeat letter \\ providecommand \\@ifxundefined [ 1 ]\n","{%            }% \\providecommand \\@ifnum [ 1 ]\n","{% \\ifnum #1\\ expandafter \\@firstoftwo \\else \\ expandafter \\@secondoftwo \\fi }% \\\n","providecommand \\@ifx [ 1 ]\n","{% \\ifx #1\\ expandafter \\@firstoftwo \\else \\ expandafter \\@secondoftwo \\fi } %            %            %            %            %            %            %            %            \\@@href }%            % \\\n","providecommand \\@sanitize@url [ 0 ]\n","{\\ catcode `\\\\12\\catcode `\\            \\alpha\n","$, however, this would be ambiguous with the Dirichlet hyper-parameter\n","$            %            \\ \\emph {et~al.} ( 2010 ) \\\n","citename font { Malewicz } , \\citenamefont {Austern} , \\citenamefont {Bik} , \\ citename font { Dehnert } , \\citenamefont {Horn} , \\citenamefont {Leiser} ,\\ and\\ \\ citename font { Czajkowski } } ]\n","{ Malewicz 2010 } % \\Bibitem Open\n","\\bibfield {author} { \\bibinfo {author} {            ~\\ bibname font { Malewicz } } , \\bibinfo {author} { \\bibfnamefont {M.~H.} \\ \\ bibname font { Austern } } , \\bibinfo {author} { \\bibfnamefont {A.~J.}  \\ \\bibnamefont {Bik} } , \\bibinfo {author} { \\bibfnamefont {J.~C.}  \\ \\bibnamefont {Dehnert} } , \\ bibinfo { author } {            ~ \\bibnamefont {Horn} } , \\bibinfo {author} {            ~ \\bibnamefont {Leiser} } , \\ and\\ \\bibinfo {author} {            ~ \\bibnamefont {Czajkowski} } , \\ } in \\ \\href@noop {} {\\ emph { \\bibinfo {booktitle} { Proceedings of the 2010 ACM SIGMOD International Conference on Management of data } } } \\ (\\bibinfo {organization} { ACM } , \\ \\bibinfo {year} { 2010 } ) \\ pp.\\ \\bibinfo {pages} { 135-- 146 } \\\n","Bibitem Shut { NoStop } % \\bibitem [{\\citenamefont {Zaharia}  \\ \\emph {et~al.} ( 2010 ) \\ citename font { Zaharia } , \\citenamefont {Chowdhury} , \\citenamefont {Franklin} , \\ citename font { Shenker } , \\ and\\ \\citenamefont {Stoica} } ]\n","{ zaharia2010spark } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} {            ~\\ bibname font { Zaharia } } , \\bibinfo {author} { \\bibfnamefont {M.} ~ \\bibnamefont {Chowdhury} } , \\bibinfo {author} { \\bibfnamefont {M.~J.}  \\ \\bibnamefont {Franklin} } , \\ bibinfo { author } {            ~            } , \\ and \\ \\ bibinfo { author } {            ~            } , \\ } in \\ \\href@noop {}\n","{ \\emph {\\bibinfo {booktitle} { Proceedings of the 2nd USENIX conference on Hot topics in cloud computing } } } , \\ Vol.~\\bibinfo {volume} { 10 } \\ (\\bibinfo {year} { 2010 } ) \\ p.~\\bibinfo {pages} { 10 } \\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {Lee}  \\ \\emph {et~al.} ( 2012 ) \\citenamefont {Lee} , \\citenamefont {Lee} , \\citenamefont {Choi} , \\citenamefont {Chung} , \\ and \\ \\citenamefont {Moon} } ]\n","{ Lee : 2012 : PDP : 2094114.2094118 }\n","% \\ Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {K.-H.} \\ \\ bibname font { Lee } } , \\bibinfo {author} { \\bibfnamefont {Y.-J.}  \\ \\bibnamefont {Lee} } , \\bibinfo {author} { \\bibfnamefont {H.} ~ \\bibnamefont {Choi} } , \\bibinfo {author} { \\bibfnamefont {Y.~D.}  \\ \\bibnamefont {Chung} } , \\ and\\ \\bibinfo {author} { \\bibfnamefont {B.} ~ \\bibnamefont {Moon} } , \\ } \\ href {\\ doibase 10.1145 / 2094114.2094118 } { \\bibfield {journal} { \\bibinfo {journal} { SIGMOD Record } \\ }\\textbf {\\bibinfo {volume} { 40 } } , \\ \\bibinfo {pages} { 11 } (\\ bibinfo { year } { 2012 } ) } \\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {Zou}  \\ \\emph {et~al.} ( 2013 ) \\citenamefont {Zou} , \\citenamefont {Li} , \\citenamefont {Jiang} , \\citenamefont {Lin} , \\ citename font { Li } , \\ and\\ \\citenamefont {Chen} } ]\n","{ zou2013survey } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {Q.} ~\\ bibname font { Zou } } , \\bibinfo {author} { \\bibfnamefont {X.-B.}  \\ \\bibnamefont {Li} } , \\bibinfo {author} { \\bibfnamefont {W.-R.}  \\ \\bibnamefont {Jiang} } , \\ bibinfo { author } { \\bibfnamefont {Z.-Y.}  \\ \\bibnamefont {Lin} } , \\bibinfo {author} { \\bibfnamefont {G.-L.}  \\ \\bibnamefont {Li} } , \\ and\\ \\bibinfo {author} { 9999999884 ~ \\bibnamefont {Chen} } , \\ } \\ href@ noop {} {\\ bibfield { journal } { \\bibinfo {journal} { Briefings in bioinformatics } \\ , \\ \\ bibinfo { pages } { bbs088 } } ( \\bibinfo {year} { 2013 } ) } \\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {McCune}  \\ \\emph {et~al.} ( 2015 ) \\citenamefont {McCune} , \\citenamefont {Weninger} , \\ and\\ \\citenamefont {Madey} }]\n","{ McCune 2015 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999872 \\ \\ bibname font { McCune } } , \\bibinfo {author} { 9999999870 ~ \\bibnamefont {Weninger} } , \\ and \\ \\bibinfo {author} { 9999999867 ~ \\bibnamefont {Madey} } , \\ } \\href@noop {} {\\bibfield {journal} { \\bibinfo {journal} { ACM Computing Surveys } 9999999863 { 2015 } ) }\n","\\BibitemShut {NoStop} % 9999999861 \\ \\emph {et~al.} ( 2009 )\\\n","citename font { Mccallum } , \\citenamefont {Mimno} ,\\ and \\ \\ citename font { Wallach } } ]\n","{ Mccallum 2009} % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999856 ~\\ bibname font { Mccallum } } , \\bibinfo {author} { \\bibfnamefont {D.~M.}  \\ \\bibnamefont {Mimno} } , \\ and\\ \\bibinfo {author} { \\bibfnamefont {H.~M.}  \\ \\bibnamefont {Wallach} } ,\\ } in \\ \\href@noop {} {\\emph {\\bibinfo {booktitle} { Advances in neural information processing systems } }} \\ (\\bibinfo {year} { 2009 } )\\ pp. \\ \\ bibinfo { pages } { 1973 - - 1981 }\n","9999999847 % 9999999846 \\ \\emph {et~al.} ( 1998 ) \\citenamefont {Giles} , \\citenamefont {Bollacker} ,\\ and \\ \\ citename font { Lawrence } } ]\n","{ giles1998citeseer} % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {C.~L.} \\ \\ bibname font { Giles } } , \\bibinfo {author} { \\bibfnamefont {K.~D.} \\ \\ bibname font { Bollacker } } , \\ and\\ \\bibinfo {author} { 9999999836 ~\\ bibname font { Lawrence } } , \\ }in\\ \\href@noop {} {\\emph {\\bibinfo {booktitle} { Proceedings of the third ACM conference on Digital libraries } }} \\ (\\bibinfo {organization} { ACM } , \\ \\bibinfo {year} { 1998 } ) \\ pp.\\ \\bibinfo {pages} { 89-- 98 } \\\n","Bibitem Shut { No Stop } % \\bibitem [{\\citenamefont {Ley} ( 2002 ) }]\n","{ ley2002dblp } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999828 ~\\ bibname font { Ley } } , \\ }in\\ \\href@noop {} {\\emph {\\bibinfo {booktitle} { String Processing and Information Retrieval } }} \\ (\\bibinfo {organization} { Springer } , \\ \\ bibinfo { year } { 2002 } ) \\ pp.\\ \\bibinfo {pages} { 1 -- 10 }\n","\\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {Tang}\n","\\ \\emph {et~al.} ( 2008 ) \\citenamefont {Tang} , \\citenamefont {Zhang} , \\citenamefont {Yao} , \\citenamefont {Li} , \\ citename font { Zhang } , \\ and\\ \\citenamefont {Su} }]\n","{ Tang 2008 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {J.} ~\\ bibname font { Tang } } , \\bibinfo {author} { \\bibfnamefont {J.} ~ \\bibnamefont {Zhang} } , \\bibinfo {author} { \\bibfnamefont {L.} ~ \\bibnamefont {Yao} } , \\bibinfo {author} { \\bibfnamefont {J.} ~ \\bibnamefont {Li} } , \\bibinfo {author} {\\ bibfname font { L. } ~ \\bibnamefont {Zhang} } , \\ and\\ \\bibinfo {author} {\\ bibfname font { Z. } ~ \\bibnamefont {Su} } , \\ }in\\ \\href {\\doibase 10.1145/1401890.1402008} { \\emph {\\bibinfo {booktitle} { SIGKDD } }} \\ (\\bibinfo {address} { New York , New York , USA } , \\ \\bibinfo {year} { 2008 } ) \\ p.\\ \\bibinfo {pages} { 990 }\\ Bibitem Shut { No Stop } % 9999999795 \\ and \\ \\ citename font { Croft } ( 1998 ) }]\n","{ Ponte 1998 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {J.~M.} \\ \\ bibname font { Ponte } } \\ and\\ \\bibinfo {author} { \\bibfnamefont {W.~B.} \\ \\ bibname font { Croft } } , \\ }in\\ \\href {\\doibase 10.1145/290941.291008}\n","{\\ emph {\\ bibinfo { booktitle } { SIGIR } } } \\ (\\bibinfo {address} { New York , New York , USA } , \\ \\bibinfo {year} { 1998 } ) \\ pp.\\ \\bibinfo {pages} { 275-- 281 }\\ Bibitem Shut { NoStop } % \\bibitem [{\\citenamefont {Faloutsos}  \\ \\emph {et~al.} ( 2013 )\\ citename font { Faloutsos } , \\citenamefont {Koutra} ,\\ and \\ \\ citename font { Vogelstein } }]\n","{ Faloutsos : 2013 dn } % \\\n","Bibitem Open \\bibfield {author} { 9999999781 { 9999999780 ~\\ bibname font { Faloutsos } } , \\bibinfo {author} { 9999999778 ~ \\bibnamefont {Koutra} } , \\ and \\ \\bibinfo {author} { \\bibfnamefont {J.~T.}  \\ \\bibnamefont {Vogelstein} } ,\\ } \\href@noop {} {\\bibfield {journal} { \\bibinfo {journal} { SDM } \\ , \\ \\ bibinfo { pages } { 162 } } ( \\bibinfo {year} { 2013 } ) } 9999999770 % 9999999769 ( 2002 ) }]\n","{ Mccallum : 2002fe } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {A.~K.} \\ \\ bibname font { McCallum } } , \\ }\\href@noop {} {\\bibfield {journal} { \\bibinfo {journal} { ( 2002 ) } \\ } (\\bibinfo {year} { 2002 } ) } 9999999762 % \\bibitem [{\\citenamefont {Chang}  \\ \\emph {et~al.} ( 2009 ) \\citenamefont {Chang} , \\citenamefont {Gerrish} , \\citenamefont {Wang} , \\citenamefont {Boyd-graber} ,\\ and \\ \\citenamefont {Blei} }] { Chang 2009 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999752 ~\\ bibname font { Chang } } , \\bibinfo {author} { 9999999750 ~ \\bibnamefont {Gerrish} } , \\bibinfo {author} { 9999999747 ~ \\bibnamefont {Wang} } , \\bibinfo {author} { \\bibfnamefont {J.~L.}  \\ \\bibnamefont {Boyd-graber} } , \\ and\\ \\bibinfo {author} { \\bibfnamefont {D.~M.}  \\ \\bibnamefont {Blei} } , \\ } in \\ \\ href@noop {} {\\ emph { \\bibinfo {booktitle} { Advances in neural information processing systems } } }\\ ( \\bibinfo {year} { 2009 } ) \\ pp.\\ \\bibinfo {pages} { 288--296 }\\\n","Bibitem Shut { NoStop } %\n","\\end{thebibliography} %\n","NOUN PHRASES:\n"," ['thebibliography', 'ifxundefined', ']', '%', '[', ']', '%', '[', ']', '%', 'href', 'sanitize', '@', 'Dirichlet', 'et~al', 'citename', 'Malewicz', 'Austern', 'Bik', 'Dehnert', 'Horn', 'Leiser', 'Czajkowski', ']', 'Malewicz', 'author', 'author', 'Malewicz', 'author', 'M.~H', 'author', 'A.~J', 'Bik', 'author', 'J.~C', 'Dehnert', 'bibinfo', 'author', 'Horn', 'author', 'Leiser', 'author', 'Czajkowski', 'booktitle', 'Proceedings', 'ACM SIGMOD International Conference', 'Management', 'data', 'organization', 'ACM', 'year', 'pages', 'NoStop', 'Zaharia', 'et~al', 'citename', 'Zaharia', 'Chowdhury', 'Franklin', 'Shenker', 'Stoica', ']', 'zaharia2010spark', 'author', 'author', 'Zaharia', 'author', 'M.', 'Chowdhury', 'author', 'M.~J', 'Franklin', 'bibinfo', 'author', 'author', 'booktitle', 'Proceedings', 'USENIX conference', 'Hot topics', 'cloud computing', 'volume', 'year', 'pages', 'NoStop', 'Lee', 'et~al', 'Lee', 'Lee', 'Choi', 'Chung', 'Moon', ']', 'Lee', 'PDP', 'author', 'author', 'K.-H.', 'Lee', 'author', 'Lee', 'author', 'H.', 'Choi', 'author', 'Y.~D', 'Chung', 'author', 'B', 'Moon', '/', 'SIGMOD Record', 'volume', 'pages', 'year', 'NoStop', 'Zou', 'et~al', 'Zou', 'Li', 'Jiang', 'Lin', 'Li', 'Chen', ']', 'zou2013survey', 'author', 'author', 'Q', 'Zou', 'author', 'Li', 'author', 'Jiang', 'bibinfo', 'author', 'Z.-Y', 'Lin', 'author', 'Li', 'author', 'Chen', 'Briefings', 'bioinformatics', 'pages', 'year', 'NoStop', 'McCune', 'et~al', 'McCune', 'Weninger', 'Madey', ']', 'McCune', 'author', 'author', 'McCune', 'author', 'Weninger', 'author', 'Madey', 'ACM Computing Surveys', 'NoStop', '%', 'et~al', 'citename', 'Mccallum', 'Mimno', 'Wallach', ']', 'Mccallum', 'author', 'author', 'Mccallum', 'author', 'D.~M', 'Mimno', 'author', 'H.~M', 'Wallach', 'booktitle', 'Advances', 'neural information', 'processing', 'systems', 'year', 'pp', 'pages', '%', 'et~al', 'Giles', 'Bollacker', 'Lawrence', ']', 'giles1998citeseer', 'author', 'author', 'C.~L', 'Giles', 'author', 'K.~D', 'Bollacker', 'author', 'Lawrence', 'booktitle', 'Proceedings', 'third ACM conference', 'Digital libraries', 'organization', 'ACM', 'year', 'pages', 'No Stop', 'Ley', 'ley2002dblp', 'author', 'author', 'Ley', 'booktitle', 'String', 'Processing', 'Information Retrieval', 'organization', 'Springer', 'year', 'pages', 'NoStop', 'Tang', 'et~al', 'Tang', 'Zhang', 'Yao', 'Li', 'Zhang', 'Su', ']', 'Tang', 'author', 'author', 'J', 'Tang', 'author', 'J', 'Zhang', 'author', 'L.', 'Yao', 'author', 'J', 'Li', 'author', 'L.', 'Zhang', 'author', 'Z', 'Su', 'booktitle', 'SIGKDD', 'address', 'New York', 'New York', 'USA', 'year', 'pages', 'No Stop', '%', 'Croft', 'Ponte', 'author', 'author', 'J.~M', 'Ponte', 'author', 'W.~B', 'Croft', 'booktitle', 'SIGIR', 'address', 'New York', 'New York', 'USA', 'year', 'pages', 'NoStop', 'Faloutsos', 'et~al', 'citename', 'Faloutsos', 'Koutra', 'Vogelstein', ']', 'Faloutsos', 'dn', 'author', 'Faloutsos', 'author', 'Koutra', 'author', 'J.~T', 'Vogelstein', 'SDM', 'pages', 'year', '%', 'Mccallum', 'author', 'author', 'A.~K', 'McCallum', 'year', 'Chang', 'et~al', 'Chang', 'Wang', 'Boyd-graber', 'Blei', ']', 'Chang', 'author', 'author', 'Chang', 'author', 'Gerrish', 'author', 'Wang', 'author', 'J.~L', 'Boyd-graber', 'author', 'D.~M', 'Blei', 'booktitle', 'Advances', 'neural information', 'processing', 'systems', 'year', 'pages', 'NoStop', '%', 'thebibliography', '%']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Using either the traditional inference model from Sec. ~ \\ref{sec:inference} or the high - throughput distributed sampling algorithm from Sec. ~ \\ref{sec:distinference} we basically sample a full hierarchy $\\textbf{c}_d$ by sampling paths for each node $c_d$ except the root and the words in the document $z_{d,n}$ .\n","Given the state of the sampler at some time $t$ , \\ie,            and            , we iteratively sample each variable conditioned on the others as illustrated in~\\ref{sec:generating} .\n","NOUN PHRASES:\n"," ['Using', 'traditional inference model', 'Sec', '~', 'sec', 'inference', 'throughput', 'distributed sampling', 'algorithm', 'Sec', '~', 'sec', 'distinference', 'basically sample', 'full hierarchy', 'c', '_d', 'sampling', 'paths', 'root', 'words', 'd', 'Given', 'state', 'sampler', 'time', 'iteratively sample', 'variable conditioned', 'others', 'sec', 'generating']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 189, 'end': 205, 'text': 'a full hierarchy'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 233, 'end': 252, 'text': 'paths for each node'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 296, 'end': 304, 'text': 'document'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 356, 'end': 360, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Stochastic block models ( SBM ) are an alternative line of network clustering research that partitions nodes into communities in order to generatively infer link probabilities ~ \\citep{Holland1983} .\n","Several extensions to the original SBM have since been proposed ( for a survey see ~ \\citep{Goldenberg2009} ) .\n","One downside to block - model processes is that they assign probabilities to every possible edge requiring $\\mathcal{O}(N^2)$ complexity in every sampling iteration .\n","Furthermore , SBM methods typically are not concerned with topical / conceptual properties of the nodes .\n","NOUN PHRASES:\n"," ['Stochastic block models', 'SBM', 'alternative line', 'network', 'clustering', 'research', 'partitions', 'nodes', 'communities', 'order', 'generatively infer', 'link probabilities', 'Holland1983', 'Several extensions', 'original SBM', 'have', 'Goldenberg2009', 'downside', 'block', 'model', 'assign', 'probabilities', 'possible edge', 'requiring', 'O', 'N^2', 'complexity', 'sampling iteration', 'Furthermore', 'SBM methods', 'not concerned', 'topical / conceptual properties', 'nodes']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 438, 'end': 448, 'text': 'complexity'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The nested version of the CRP extends the original analogy as follows :\n","At each table in the Chinese restaurant are cards with the name of another Chinese restaurant .\n","When a customer sits at a given table , he reads the card , gets up and goes to that restaurant , where he is reseated according to the CRP .\n","Each customer visits $L$ restaurants until he is finally seated and is able to eat .\n","This process creates a tree with a depth of $L$ and a width determined by the $\\gamma$ parameter .\n","This process has also been called the Chinese Restaurant Franchise because of this analogy ~ \\citep{Blei2003} .\n","NOUN PHRASES:\n"," ['nested version', 'CRP', 'extends', 'original analogy', 'follows', 'table', 'Chinese restaurant', 'cards', 'name', 'Chinese restaurant', 'customer', 'sits', 'given', 'table', 'reads', 'card', 'gets', 'goes', 'restaurant', 'CRP', 'customer', 'visits', 'L', 'restaurants', 'finally seated', 'eat', 'process', 'creates', 'tree', 'depth', 'L', 'width', 'determined', 'parameter', 'process', 'has', 'Chinese Restaurant Franchise', 'Blei2003']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 335, 'end': 346, 'text': 'restaurants'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 428, 'end': 435, 'text': 'a depth'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 482, 'end': 491, 'text': 'parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ centering \\ small { \\begin{tabular} {l| c c c c } & Wikipedia ( cat ) & Wikipedia ( article ) & CompSci Web site & Bib. Network \\\\ \\noalign{\\hrule height 1.5pt} documents & 609 & 1,957 ,268 & 1,078 & 4,713 \\\\ tokens & 5,570,868 & 1,316,879,537 & 771,309 & 43,345 \\\\ links & 2,014 & 44,673,134 & 63,052 & 8,485\\\\ vocabulary & 146,624 & 4,225,765 & 15,101 & 3,908 \\\\ \\noalign{\\hrule height 1.5pt}  \\end{tabular} }\n","\\caption{Comparison of most probable words in top document (in            ) and in root topic (in hLDA)}  \\label{tab:topdoc}  \\end{table}\n","NOUN PHRASES:\n"," ['centering', 'l| c c', 'c', 'c', 'Wikipedia', 'cat', 'Wikipedia', 'article', 'CompSci Web site', 'Bib', 'height', 'documents', ',268', 'height', 'Comparison', 'probable words', 'top document', 'root topic', 'hLDA', 'tab', 'topdoc', 'table']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Beginning with a document graph $G=\\{V,E\\}$ of documents $V$ and edges $E$ .\n","Each document is a collection of words , where a word $w$ is an item in a vocabulary .\n","The basic assumption of HD TM and similar models is that each document can be generated by probabilistically mixing words from among topics .\n","Distributions over topics are represented by $z$ , which is a multinomial variable with an associated set of distributions over words $p(w|z,\\beta)$ , where $\\beta$ is a Dirichlet hyper-parameter .\n","Document - specific mixing proportions are denoted by the vector $\\theta$ .\n","Parametric - Bayes topic models also include a $K$ parameter that denotes the number of topics , wherein $z$ is one of $K$ possible values and $\\theta$ is a $K$ - D vector .\n","HDTM , and other non-parametric Bayesian models , do not require a $K$ parameter as input .\n","Instead , in HD TM there exist $|V|$ topics , one for each graph node , and each document is a mixture of the topics on the path between itself and the root document .\n","NOUN PHRASES:\n"," ['Beginning', 'document graph', 'V', 'documents', 'V', 'edges', 'E', 'document', 'collection', 'words', 'word', 'item', 'basic assumption', 'HD TM', 'similar models', 'document', 'probabilistically mixing', 'words', 'topics', 'Distributions', 'topics', 'multinomial variable', 'associated set', 'distributions', 'words', 'p', 'w|z', 'Dirichlet hyper-parameter', 'Document', 'specific mixing proportions', 'vector', 'Parametric', 'Bayes topic models', 'also include', 'K', 'parameter', 'denotes', 'number', 'topics', 'K', 'possible values', 'K', 'D vector', 'HDTM', 'other non-parametric Bayesian models', 'do', 'not require', 'K', 'parameter', 'input', 'HD TM', 'exist', 'topics', 'graph node', 'document', 'mixture', 'topics', 'path', 'root document']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 15, 'end': 31, 'text': 'a document graph'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 47, 'end': 56, 'text': 'documents'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 65, 'end': 70, 'text': 'edges'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 124, 'end': 130, 'text': 'a word'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 138, 'end': 161, 'text': 'an item in a vocabulary'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 306, 'end': 331, 'text': 'Distributions over topics'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 366, 'end': 388, 'text': 'a multinomial variable'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 394, 'end': 439, 'text': 'an associated set of distributions over words'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 474, 'end': 501, 'text': 'a Dirichlet hyper-parameter'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 558, 'end': 568, 'text': 'the vector'}, 'T22': {'eid': 'T22', 'label': 'PRIMARY', 'start': 504, 'end': 542, 'text': 'Document - specific mixing proportions'}, 'T24': {'eid': 'T24', 'label': 'PRIMARY', 'start': 631, 'end': 640, 'text': 'parameter'}, 'T25': {'eid': 'T25', 'label': 'PRIMARY', 'start': 654, 'end': 674, 'text': 'the number of topics'}, 'T26': {'eid': 'T26', 'label': 'PRIMARY', 'start': 668, 'end': 674, 'text': 'topics'}, 'T29': {'eid': 'T29', 'label': 'PRIMARY', 'start': 692, 'end': 718, 'text': 'one of $K$ possible values'}, 'T31': {'eid': 'T31', 'label': 'PRIMARY', 'start': 735, 'end': 751, 'text': 'a $K$ - D vector'}, 'T34': {'eid': 'T34', 'label': 'PRIMARY', 'start': 825, 'end': 834, 'text': 'parameter'}, 'T36': {'eid': 'T36', 'label': 'PRIMARY', 'start': 883, 'end': 889, 'text': 'topics'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The processes that typically defines most hierarchical clustering algorithms can be made to fit in a probabilistic setting that build bottom - up hierarchies based on Bayesian hypothesis testing ~ \\citep{Heller2005} .\n","On the other hand , a lot of recent work uses Bayesian generative models to find the most likely explanation of observed text and links .\n","The first of these hierarchical generative models was hierarchical latent Dirichlet allocation ( hLDA ) .\n","In hLDA each document sits at a leaf in a tree of fixed depth $L$ as illustrated in Figure ~ \\ref{fig:rwhlda} .\n","Note that all non-leave nodes in Figure ~ \\ref{fig:rwhlda} are conceptual topics containing word distribution instead of a document .\n","Each document is represented by a mixture of multinomials along the path through the taxonomy from the document to the root .\n","Documents are placed at their respective leaf nodes stochasically using the nested Chinese restaurant process ( nCRP ) along side an LDA - style word sampling process .\n","NOUN PHRASES:\n"," ['processes', 'typically defines', 'hierarchical clustering', 'algorithms', 'fit', 'probabilistic setting', 'build', 'hierarchies', 'based', 'Bayesian hypothesis', 'testing', 'Heller2005', 'other hand', 'lot', 'recent work', 'uses', 'Bayesian generative models', 'find', 'likely explanation', 'observed text', 'links', 'hierarchical generative models', 'hierarchical latent Dirichlet allocation', 'hLDA', 'document', 'sits', 'leaf', 'tree', 'L', 'illustrated', 'fig', 'rwhlda', 'Note', 'non-leave nodes', 'fig', 'rwhlda', 'conceptual topics', 'containing', 'word distribution', 'document', 'document', 'mixture', 'multinomials', 'path', 'taxonomy', 'document', 'root', 'Documents', 'respective leaf nodes', 'stochasically using', 'nested Chinese restaurant process', 'nCRP', 'side', 'LDA', 'style word', 'sampling', 'process']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 518, 'end': 523, 'text': 'depth'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," NCRP is a recursive version of the standard Chinese Restaurant Process ( CRP ) , which progresses according to the following analogy :\n","An empty Chinese restaurant has an infinite number of tables , and each table has an infinite number of chairs .\n","When the first customer arrives he sits in the first chair at the first table with probability of 1 .\n","The second customer can then chose to sit at an occupied table with probability of $\\frac{n_i}{\\gamma+n-1}$ or sit at a new , unoccupied table with probability of $\\frac{\\gamma}{\\gamma+n-1}$ , where $n$ is the current customer , $n_i$ is the number of customers currently sitting at table $i$ , and $\\gamma$ is a parameter that defines the affinity to sit at a previously occupied table .\n","NOUN PHRASES:\n"," ['NCRP', 'recursive version', 'standard Chinese Restaurant Process', 'CRP', 'progresses according', 'following analogy', 'empty Chinese restaurant', 'has', 'infinite number', 'tables', 'table', 'has', 'infinite number', 'chairs', 'first customer', 'arrives', 'sits', 'first chair', 'first table', 'probability', 'second customer', 'then chose', 'sit', 'occupied table', 'probability', 'sit', 'unoccupied table', 'probability', 'current customer', 'number', 'customers', 'currently sitting', 'parameter', 'defines', 'affinity', 'sit', 'previously occupied', 'table']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 418, 'end': 429, 'text': 'probability'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 498, 'end': 509, 'text': 'probability'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 556, 'end': 576, 'text': 'the current customer'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 588, 'end': 642, 'text': 'the number of customers currently sitting at table $i$'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 602, 'end': 642, 'text': 'customers currently sitting at table $i$'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 633, 'end': 638, 'text': 'table'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 661, 'end': 736, 'text': 'a parameter that defines the affinity to sit at a previously occupied table'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In the original LDA model , a single document mixture distribution is $p(w|\\theta) = \\sum_{i=1}^{K}\\theta_i p(w|z=i, \\beta_i)$ .\n","The process for generating a document is ( 1 ) choose a $\\theta$ of topic proportions from a distribution $p(\\theta|\\alpha)$ , where $p(\\theta|\\alpha)$ is a Dirichlet distribution ; ( 2 ) sample words from the mixture distribution $p(w |\\theta)$ for the $\\theta$ chosen in step 1 .\n","NOUN PHRASES:\n"," ['original LDA model', 'single document mixture distribution', 'p', '=', '^', 'K', 'w|z=i', 'process', 'generating', 'document', 'choose', 'topic proportions', 'distribution', 'p', 'p', 'Dirichlet distribution', 'sample words', 'mixture distribution', 'p', 'chosen', 'step']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 28, 'end': 66, 'text': 'a single document mixture distribution'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 197, 'end': 214, 'text': 'topic proportions'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 220, 'end': 234, 'text': 'a distribution'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 284, 'end': 308, 'text': 'a Dirichlet distribution'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 335, 'end': 359, 'text': 'the mixture distribution'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Say we wish to find the RWR - probability between some node $u$ and some target node $k$ .\n","We model this by a random walker visiting document $u$ at time $t$ .\n","In the next time step , the walker chooses a document $v_i$ from among $u$ 's outgoing neighbors $\\{v|u\\rightarrow_{T} v\\}$ in the hierarchy $T$ uniformly at random .\n","In other words , at time $t + 1$ , the walker lands at node $v_i \\in \\{v|u\\rightarrow_{T} v\\}$ with probability $1/deg(u)$ , where $deg(u)$ is the outdegree of some document $u\\in G$ .\n","If at any time , there exists an edge to $k \\in \\{v|u\\rightarrow_{G} v\\}$ , { \\em i.e}, an edge between the current node            and the target node            in the original graph            , then we record the probability of that new path possibility for later sampling. Alg.~\\ref{alg:rwr} describes this process algorithmically .\n","This procedure allows for new paths from the root $r\\leadsto k$ to be probabilistically generated based on the current hierarchy effectively allowing for documents to migrate up , down and through the hierarchy during sampling .\n","NOUN PHRASES:\n"," ['Say', 'wish', 'find', 'RWR', 'probability', 'target', 'model', 'random walker', 'visiting', 'time', 'next time step', 'walker', 'chooses', 'outgoing', 'neighbors', 'T', 'T', 'random', 'other words', 'time', 't +', 'walker', 'lands', 'T', 'probability', 'u', 'deg', 'outdegree', 'time', 'exists', 'edge', 'k', 'G', 'edge', 'current node', 'target node', 'original graph', 'record', 'probability', 'new path possibility', 'later sampling', 'alg', 'rwr', 'describes', 'process', 'procedure', 'allows', 'new paths', 'root', 'probabilistically generated based', 'current hierarchy', 'effectively allowing', 'documents', 'migrate', 'hierarchy', 'sampling']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 55, 'end': 59, 'text': 'node'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 73, 'end': 84, 'text': 'target node'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 133, 'end': 141, 'text': 'document'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 149, 'end': 153, 'text': 'time'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 203, 'end': 213, 'text': 'a document'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 231, 'end': 256, 'text': \"$u$ 's outgoing neighbors\"}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 287, 'end': 300, 'text': 'the hierarchy'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 347, 'end': 351, 'text': 'time'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 382, 'end': 386, 'text': 'node'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 427, 'end': 438, 'text': 'probability'}, 'T25': {'eid': 'T25', 'label': 'PRIMARY', 'start': 470, 'end': 509, 'text': 'the outdegree of some document $u\\\\in G$'}, 'T28': {'eid': 'T28', 'label': 'PRIMARY', 'start': 492, 'end': 500, 'text': 'document'}, 'T32': {'eid': 'T32', 'label': 'PRIMARY', 'start': 616, 'end': 632, 'text': 'the current node'}, 'T34': {'eid': 'T34', 'label': 'PRIMARY', 'start': 648, 'end': 663, 'text': 'the target node'}, 'T36': {'eid': 'T36', 'label': 'PRIMARY', 'start': 678, 'end': 696, 'text': 'the original graph'}, 'T38': {'eid': 'T38', 'label': 'PRIMARY', 'start': 895, 'end': 899, 'text': 'root'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\Input{Path Probs.            , Current Node            , Target            , Weight            }  \\Globals{Graph            , Hierarchy            , Restart Prob.            }  \\Output{            } \\ Blank Line \\ForEach(\\tcc*[f]{\\small{child of            in            } }) { $v_i \\in \\T.\\Ch(u)$ } { \\If{            } { $w \\gets w + \\operatorname{log}\\left(\\frac{1-\\gamma}{\\operatorname{len}\\left(\\T.\\Ch(u)\\right)}\\right)$ \\; \\RWR(            ,            ,            ,            )\\tcc*[r]{\\small{Recur} } } } \\If(\\tcc*[f]{\\small{Edge            to            exists in            } }) { $u \\rightarrow_{\\G} k$ } { $P.\\Put(u, w)$ \\; } \\caption{Random Walk with Restart}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['Path Probs', 'Current Node', 'Target', 'Weight', 'Graph', 'Hierarchy', 'Restart Prob', 'f', ']', 'child', 'w', 'log', '[ r ]', 'Recur', 'f', ']', 'Edge', 'exists', 'k', 'w', 'Random', 'Walk', 'Restart', 'algorithm']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 7, 'end': 17, 'text': 'Path Probs'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 32, 'end': 44, 'text': 'Current Node'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 58, 'end': 64, 'text': 'Target'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 78, 'end': 84, 'text': 'Weight'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 108, 'end': 113, 'text': 'Graph'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 127, 'end': 136, 'text': 'Hierarchy'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 150, 'end': 163, 'text': 'Restart Prob.'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," HLDA is an extension of LDA in which the topics are situated in a taxonomy $T$ of fixed depth $L$ .\n","The hierarchy is generated by the nested Chinese restaurant process ( nCRP ) which represents $\\theta$ as an $L$ - dimensional vector , defining an $L$ - level path through $T$ from root to document .\n","Because of the nCRP process , every document lives at a leaf and the words in each document are a mixture of the topic - words on the path from it to the root .\n","NOUN PHRASES:\n"," ['HLDA', 'extension', 'LDA', 'topics', 'T', 'fixed', 'L', 'hierarchy', 'nested Chinese restaurant process', 'nCRP', 'represents', 'L', 'dimensional vector', 'defining', 'L', 'level path', 'T', 'root', 'document', 'nCRP process', 'document', 'lives', 'leaf', 'words', 'document', 'mixture', 'topic', 'words', 'path', 'root']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 64, 'end': 74, 'text': 'a taxonomy'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 88, 'end': 93, 'text': 'depth'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 206, 'end': 233, 'text': 'an $L$ - dimensional vector'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 245, 'end': 298, 'text': 'an $L$ - level path through $T$ from root to document'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 254, 'end': 259, 'text': 'level'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The first term in Eq. ~ \\ref{eq:pathsample} is the probability of a given word based on the current path $\\mathbf{c}$ and topic assignment $\\mathbf{z}$ :\n","\\begin{equation}\n","\\begin{split}\n","p(\\mathbf{w}_{d} | \\mathbf{c}, \\mathbf{w}_{-d}, \\mathbf{z}, \\eta ) =&\\prod_{k=1}^{\\max(\\mathbf{z}_d)}{\n","\\frac{ \\Gamma(\\sum_w{ \\#[\\mathbf{c}_{-d,k} = c_{d,k}, \\mathbf{w}_{-d} = w] + W\\eta}) }\n","{ \\prod_w{ \\Gamma( \\#[\\mathbf{c}_{-d,k} = c_{d,k}, \\mathbf{w}_{-d} = w] + \\eta}) } } \\times \\\\\n","&\\quad\\hspace{1.1cm} \\frac{ \\prod_w{\\Gamma( \\#[\\mathbf{z} = k, \\mathbf{c}_{k} = c_{d,k}, \\mathbf{w} = w] + \\eta) } }\n","{ \\Gamma( \\sum{ \\#[\\mathbf{z} = k, \\mathbf{c}_{k} = c_{d,k}, \\mathbf{w} = w]} + W\\eta) },\n","\\label{eq:pwgivenc}\n","\\end{split}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['first term', 'Eq', '~', 'eq', 'pathsample', 'probability', 'given', 'word', 'based', 'current path', 'c', 'topic', 'z', 'equation', 'split', 'p', '_', 'd', 'c', 'w', '_', '-d', 'z', '=', 'k=1', '^', 'z', '_d', 'c', '_', '-d', 'k', '= c_', 'd', 'k', 'w', '_', '-d', 'c', '_', '-d', 'k', '= c_', 'd', 'k', 'w', '_', '-d', 'z', '= k', 'c', '_', 'k', '= c_', 'd', 'k', 'w', 'z', '= k', 'c', '_', 'k', '= c_', 'd', 'k', 'w', '= w ]', 'eq', 'pwgivenc', 'split', 'equation']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 88, 'end': 104, 'text': 'the current path'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 122, 'end': 138, 'text': 'topic assignment'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent where            counts the elements of an array that satisfy the given condition, and            is the maximum depth of the current hierarchy state. The expression            counts: (ii)            , {\\em i.e.} , the number of words $w$ that do not appear in $d$ , for each ( i ) $\\mathbf{c}_{-d,k} = c_{d,k}$ , { \\em i.e.}, the number of paths to the current document            except those where the path length is            . The expression            counts: (iii)            , {\\em i.e.} , the number of words $w$ such that , ( ii ) $\\mathbf{c}_{k} = c_{d,k}$ , { \\em i.e.}, the words appear in document            and are situated at the end of a path of length            , where (i)            , {\\em i.e.} , $k$ is one of the topics in $\\mathbf{z}$ .\n","$W$ is the size of the vocabulary .\n","Eq. ~ \\ref{eq:pwgivenc} is adapted from the standard ratio of normalizing constants for the Dirichlet distribution ~ \\citep{Blei2010} .\n","NOUN PHRASES:\n"," ['counts', 'elements', 'array', 'satisfy', 'given', 'condition', 'maximum depth', 'current hierarchy state', 'expression counts', 'ii', 'number', 'words', 'do', 'not appear', 'i', 'c', '_', '-d', 'k', '= c_', 'd', 'k', 'number', 'paths', 'current document', 'path length', 'expression counts', 'iii', 'number', 'words', 'ii', 'c', '_', 'k', '= c_', 'd', 'k', 'words', 'appear', 'document', 'end', 'path', 'length', 'i', 'topics', 'z', 'W', 'size', 'Eq', '~', 'eq', 'pwgivenc', 'standard ratio', 'normalizing', 'constants', 'Blei2010']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 34, 'end': 91, 'text': 'the elements of an array that satisfy the given condition'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 111, 'end': 159, 'text': 'the maximum depth of the current hierarchy state'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 226, 'end': 322, 'text': 'the number of words $w$ that do not appear in $d$ , for each ( i ) $\\\\mathbf{c}_{-d,k} = c_{d,k}$'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 361, 'end': 381, 'text': 'the current document'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 412, 'end': 427, 'text': 'the path length'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 676, 'end': 682, 'text': 'length'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 615, 'end': 623, 'text': 'document'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 739, 'end': 772, 'text': 'one of the topics in $\\\\mathbf{z}$'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 782, 'end': 808, 'text': 'the size of the vocabulary'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 524, 'end': 529, 'text': 'words'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The nCRP stochastic process could not be used to infer document hierarchies because the nCRP process forces documents to the leaves in the tree .\n","HDTM replaces nCRP with random walk with restart ( RWR ) ( which is also known as Personalized PageRank ( PPR ) ) ~ \\citep{Bahmani2010} .\n","In contrast , random walk with teleportation ( aka PageRank ) random walks by selecting a random starting point , and , with probability $(1-\\gamma)$ , the walker randomly walks to a new , connected location or chooses to jump to a random location with probability $\\gamma$ , where $\\gamma$ is called the jumping probability \\footnote{Most related works denote the jumping probability as            , however, this would be ambiguous with the Dirichlet hyper-parameter            .} .\n","NOUN PHRASES:\n"," ['nCRP stochastic process', 'infer', 'document hierarchies', 'nCRP process forces documents', 'leaves', 'tree', 'HDTM', 'replaces', 'random walk', 'restart', 'RWR', 'also known', 'Personalized PageRank', 'PPR', '~', 'Bahmani2010', 'contrast', 'random walk', 'teleportation', 'aka PageRank', 'random walks', 'selecting', 'random', 'starting', 'point', 'walker', 'randomly walks', 'connected', 'location', 'chooses', 'jump', 'random location', 'probability', 'jumping probability', 'related', 'works', 'denote', 'jumping probability', 'Dirichlet hyper-parameter']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 409, 'end': 420, 'text': 'probability'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 537, 'end': 548, 'text': 'probability'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 585, 'end': 608, 'text': 'the jumping probability'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\label{sec:generating}\n","Because a document hierarchy is a tree , each document - node can only have one parent .\n","Selecting a path for a document $d$ in the graph $G$ is akin to selecting a parent $u = Pa(d)$ ( and grandparents , etc. ) from $\\{d|u\\rightarrow_G d\\}$ in the document graph $G$ .\n","HDTM creates and samples from a probability distribution over each documents ' parent , where the probability of document $u$ being the parent of $d$ is defined as :\n","NOUN PHRASES:\n"," ['sec', 'generating', 'document hierarchy', 'tree', 'document', 'node', 'only have', 'parent', 'Selecting', 'path', 'G', 'selecting', 'parent', 'u = Pa', 'd', 'grandparents', 'document', 'graph', 'G', 'HDTM', 'creates', 'samples', 'probability distribution', 'documents', 'parent', 'probability', 'parent']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 133, 'end': 143, 'text': 'a document'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 155, 'end': 160, 'text': 'graph'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 186, 'end': 194, 'text': 'a parent'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 268, 'end': 286, 'text': 'the document graph'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 406, 'end': 414, 'text': 'document'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Sampling word levels}\n","Given the current state of all the variables , the word sampler must first pick an assignment $z$ for word $n$ in document $d$ .\n","The sampling distribution of $z_{d,n}$ is\n","\\begin{equation}\n","\\begin{split}\n","p(z_{d,n} | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}, \\eta, \\gamma) & \\propto p(w_{d,n}, z_{d,n} | \\mathbf{c}, \\mathbf{z}_{-(d,n)}, \\mathbf{w}_{-(d,n)}, \\eta, \\gamma) \\\\\n","&= p(w_{d,n} | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}_{-(d,n)}, \\eta) p(z_{d,n} | \\mathbf{z}_{d,-n}, \\mathbf{c}, \\gamma)\n","\\end{split}\n","\\label{eq:wordsample}\n","\\end{equation}\n","where $\\mathbf{z}_{d,-n} = \\{z_{d,\\cdot}\\} \\setminus z_{d,n}$ and $\\mathbf{w}_{-(d,n)} = \\{w\\} \\setminus w_{d,n}$ .\n","The first term is a distribution over word assignments :\n","\\begin{equation}\n","\\begin{split}\n","p(w_{d,n} | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}_{-(d,n)}, \\eta) &\\propto \\#[\\mathbf{z}_{-(d,n)} = z_{d,n}, \\mathbf{c}_{z_{d,n}} = c_{d, z_{d,n}}, \\mathbf{w}_{-(d,n)} = w_{d,n}] + \\eta\n","\\end{split}\n","\\end{equation}\n","which is the $\\eta$ - smoothed frequency of seeing word $w_{d,n}$ in the topic at level $z_{d,n}$ in the path $c_d$ .\n","NOUN PHRASES:\n"," ['Sampling', 'word levels', 'Given', 'current state', 'variables', 'word sampler', 'first pick', 'word', 'sampling', 'distribution', 'd', 'equation', 'split', 'p', 'd', 'c', 'z', 'w', 'p', 'd', 'd', 'c', 'z', '_', 'd', 'n', 'w', '_', 'd', 'n', '= p', 'd', 'c', 'z', 'w', '_', 'd', 'n', 'p', 'd', 'z', '_', 'd', '-n', 'c', 'split', 'eq', 'wordsample', 'equation', 'z', '_', 'd', 'z_', 'd', 'd', 'w', '_', 'd', 'n', 'd', 'first term', 'distribution', 'word assignments', 'equation', 'split', 'p', 'd', 'c', 'z', 'w', '_', 'd', 'n', 'z', '_', 'd', 'n', '= z_', 'd', 'c', '_', 'z_', 'd', '= c_', 'd', 'z_', 'd', 'w', '_', 'd', 'n', '=', 'd', 'split', 'equation', 'smoothed', 'frequency', 'seeing', 'word', 'd', 'topic', 'd']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 117, 'end': 130, 'text': 'an assignment'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 139, 'end': 143, 'text': 'word'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 151, 'end': 159, 'text': 'document'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 166, 'end': 204, 'text': 'The sampling distribution of $z_{d,n}$'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 685, 'end': 699, 'text': 'The first term'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 703, 'end': 739, 'text': 'a distribution over word assignments'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 1034, 'end': 1038, 'text': 'word'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 1065, 'end': 1070, 'text': 'level'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 1088, 'end': 1092, 'text': 'path'}, 'T22': {'eid': 'T22', 'label': 'PRIMARY', 'start': 992, 'end': 1098, 'text': 'the $\\\\eta$ - smoothed frequency of seeing word $w_{d,n}$ in the topic at level $z_{d,n}$ in the path $c_d$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The variables needed by the Gibbs sampler are : $w_{d,n}$ , the $n$ th word in document $d$ ; $z_{d,n}$ , the assignment of the $n$ th word in document $d$ ; and $c_{d,z}$ , the topic corresponding to document at the $z$ th level .\n","The $\\theta$ and $\\beta$ variables are integrated out forming a collapsed Gibbs sampler .\n","NOUN PHRASES:\n"," ['variables', 'needed', 'Gibbs sampler', 'd', 'th word', 'd', 'assignment', 'th word', 'c_', 'd', 'z', 'topic', 'corresponding', 'document', 'th level', 'variables', 'forming', 'collapsed Gibbs sampler']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 41, 'text': 'The variables needed by the Gibbs sampler'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 79, 'end': 87, 'text': 'document'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 143, 'end': 151, 'text': 'document'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 257, 'end': 266, 'text': 'variables'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $f(w;d)$ is the frequency of term $w$ in a document $d$ before propagation , $f^{\\prime}(w;d)$ is the frequency of term $w$ in document $d$ after propagation , $c$ is a child page of $d$ in the sitemap $\\mathcal{T}$ , and $\\alpha$ is a parameter to control the mixing factor of the children .\n","This propagation algorithm assumes that the sitemap , $\\mathcal{T}$ , is constructed ahead of time .\n","NOUN PHRASES:\n"," ['f', 'w', 'd', 'frequency', 'term', 'propagation', 'd', 'frequency', 'term', 'propagation', 'child page', 'T', 'parameter', 'control', 'mixing', 'factor', 'children', 'propagation', 'algorithm assumes', 'sitemap', 'T', 'time']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 29, 'end': 91, 'text': 'the frequency of term $w$ in a document $d$ before propagation'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 46, 'end': 50, 'text': 'term'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 60, 'end': 68, 'text': 'document'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 115, 'end': 174, 'text': 'the frequency of term $w$ in document $d$ after propagation'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 132, 'end': 136, 'text': 'term'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 144, 'end': 152, 'text': 'document'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 184, 'end': 232, 'text': 'a child page of $d$ in the sitemap $\\\\mathcal{T}$'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 207, 'end': 218, 'text': 'the sitemap'}, 'T23': {'eid': 'T23', 'label': 'PRIMARY', 'start': 251, 'end': 307, 'text': 'a parameter to control the mixing factor of the children'}, 'T24': {'eid': 'T24', 'label': 'PRIMARY', 'start': 350, 'end': 361, 'text': 'the sitemap'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For the purposes of Web information retrieval language models are often used to normalize and smooth word distributions .\n","For illustration purposes , we apply a Dirichlet prior smoothing function ~ \\citep{Zhai2004} to smooth the term distribution where the $f^{\\prime}(w;d)$ from above is used in place of the usual $c(w;d)$ from the original Dirichlet prior smoothing function yielding :\n","NOUN PHRASES:\n"," ['purposes', 'Web information retrieval language models', 'often used', 'normalize', 'smooth', 'word distributions', 'illustration purposes', 'apply', 'Dirichlet', 'prior smoothing', 'Zhai2004', 'smooth', 'term distribution', 'd', 'above', 'place', 'c', 'w', 'd', 'original Dirichlet', 'prior smoothing', 'function yielding']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The sampling is performed in two parts : ( 1 ) given the current level allocations of each word $z_{d,n}$ sample the path $c_{d,z}$ , ( 2 ) given the current state of the hierarchy , sample $z_{d,n}$ .\n","In other words , we use the topic distributions to inform the path selections that make up the hierarchy , and the hierarchy topology to inform the topic distributions .\n","NOUN PHRASES:\n"," ['sampling', 'parts', 'given', 'current level allocations', 'word', 'd', 'd', 'z', 'given', 'current state', 'hierarchy', 'd', 'other words', 'use', 'topic distributions', 'inform', 'path selections', 'make', 'hierarchy', 'hierarchy topology', 'inform', 'topic distributions']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 53, 'end': 95, 'text': 'the current level allocations of each word'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 113, 'end': 121, 'text': 'the path'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The second term is the distribution over levels\n","\\begin{equation}\n","\\begin{split}\n","p(z_{d,n} = k | \\mathbf{z}_{d,-n}, \\mathbf{c}, \\gamma)\n","= & \\left(\\prod_{j=1}^{k-1}{ \\frac{1-\\gamma}{\\mathrm{deg}_T(d_{j-1})} \\frac{\\#[\\mathbf{z}_{d,-n} > j]}{\\#[\\mathbf{z}_{d,-n} \\ge j]} }\\right) \\times \\\\\n","& \\frac{1-\\gamma}{\\mathrm{deg}_T(d_{k-1})} \\frac{\\#[\\mathbf{z}_{d,-n} = k]}{\\#[\\mathbf{z}_{d,-n} \\ge k]},\n","\\end{split}\n","\\label{eq:abuse}\n","\\end{equation}\n","where $\\#[\\cdot]$ is the number of elements in the vector which satisfy the given condition .\n","Eq. ~ \\ref{eq:abuse} abuses notation so that the product from $j=1$ to $k-1$ combines terms representing nodes at the $j$ th level in the path $\\mathbf{c}$ down to the parent of $d_k$ , and the second set of terms represents document $d_k$ at level $k$ .\n","The $>$ symbol in Eq. ~ \\ref{eq:abuse} refers to terms representing all ancestors of a particular node , and $\\ge$ refers to the ancestors of a node including itself .\n","NOUN PHRASES:\n"," ['second term', 'distribution', 'levels', 'equation', 'split', 'p', 'd', 'z', '_', 'd', '-n', 'c', '=', 'j=1', '^', '_T', 'z', '_', 'd', '> j ]', 'z', '_', 'd', '_T', 'z', '_', 'd', '= k ]', 'z', '_', 'd', 'split', 'eq', 'abuse', 'equation', 'number', 'elements', 'vector', 'satisfy', 'given', 'condition', 'Eq', '~', 'eq', 'abuse', 'abuses', 'notation', 'product', 'combines terms', 'representing', 'nodes', 'th level', 'c', 'parent', 'second set', 'terms', 'represents', 'symbol', 'Eq', '~', 'eq', 'abuse', 'refers', 'terms', 'representing', 'ancestors', 'particular node', 'refers', 'ancestors', 'node', 'including']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 15, 'text': 'The second term'}, 'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 19, 'end': 47, 'text': 'the distribution over levels'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 456, 'end': 526, 'text': 'the number of elements in the vector which satisfy the given condition'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 470, 'end': 526, 'text': 'elements in the vector which satisfy the given condition'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 667, 'end': 671, 'text': 'path'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 615, 'end': 712, 'text': 'terms representing nodes at the $j$ th level in the path $\\\\mathbf{c}$ down to the parent of $d_k$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 719, 'end': 742, 'text': 'the second set of terms'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 754, 'end': 762, 'text': 'document'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 772, 'end': 777, 'text': 'level'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 792, 'end': 798, 'text': 'symbol'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 833, 'end': 886, 'text': 'terms representing all ancestors of a particular node'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 909, 'end': 949, 'text': 'the ancestors of a node including itself'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $d_t$ is the walkers current position at time $t$ , $\\mathrm{dep}_{T}(d)$ is the depth of $d$ in $T$ , and $\\mathrm{deg}_{T}(d_t)$ is the outdegree of $d_t$ in the hierarchy $T$ .\n","In other words , the probability of landing at $d$ is the product of the emission probabilities from each document in the path through $T$ from $r$ to $d$ .\n","NOUN PHRASES:\n"," ['d_t', 'walkers current position', 'time', 'dep', '_', 'T', 'd', 'depth', 'T', 'deg', '_', 'T', 'd_t', 'outdegree', 'T', 'other words', 'probability', 'landing', 'product', 'emission probabilities', 'document', 'path', 'T']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 26, 'end': 66, 'text': 'the walkers current position at time $t$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 94, 'end': 117, 'text': 'the depth of $d$ in $T$'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 58, 'end': 62, 'text': 'time'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 151, 'end': 194, 'text': 'the outdegree of $d_t$ in the hierarchy $T$'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 181, 'end': 190, 'text': 'hierarchy'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Specifically , the second term represents the probability of drawing the path $c_{d,k}$ to document $d$ at depth $k$ from the RWR process .\n","Recall that each node has an emission probability of $1/\\mathrm{deg}_T(d)$ , and a restart probability of $\\gamma$ .\n","The probability is defined recursively :\n","\\begin{equation}\n","\\begin{split}\n","p(&c_{d,k} | \\mathbf{c}_{-d}, c_{d,1:(k-1)})=\\prod_{k=0}{\\frac{1-\\gamma}{\\mathrm{deg}_T(d_k)}}\n","\\end{split}\n","\\end{equation}\n","In other words , the probability of reaching $d$ is equal to the probability of a random walker with restart probability $\\gamma$ being at document $d$ at time $k$ .\n","NOUN PHRASES:\n"," ['second term', 'represents', 'probability', 'drawing', 'path', 'd', 'k', 'document', 'RWR process', 'Recall', 'node', 'has', 'emission probability', 'deg', '_T', 'd', 'restart probability', 'probability', 'equation', 'split', 'p', 'c_', 'd', 'k', 'c', '_', '-d', 'd,1', 'k-1', 'k=0', '_T', 'd_k', 'split', 'equation', 'other words', 'probability', 'reaching', 'probability', 'random walker', 'restart probability', 'time']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 69, 'end': 77, 'text': 'the path'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 91, 'end': 99, 'text': 'document'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 107, 'end': 112, 'text': 'depth'}, 'T9': {'eid': 'T9', 'label': 'PRIMARY', 'start': 169, 'end': 189, 'text': 'emission probability'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 223, 'end': 242, 'text': 'restart probability'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 257, 'end': 272, 'text': 'The probability'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 42, 'end': 137, 'text': 'the probability of drawing the path $c_{d,k}$ to document $d$ at depth $k$ from the RWR process'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 552, 'end': 571, 'text': 'restart probability'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 590, 'end': 598, 'text': 'document'}, 'T20': {'eid': 'T20', 'label': 'PRIMARY', 'start': 606, 'end': 610, 'text': 'time'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\Globals{Vertices            , Hierarchy            , Restart Prob.            }  \\Input{Messages received containing path probabilities            }            \\ Blank Line\n","NOUN PHRASES:\n"," ['Vertices', 'Hierarchy', 'Restart Prob', 'Messages', 'received containing', 'path probabilities']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 89, 'end': 136, 'text': 'Messages received containing path probabilities'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 54, 'end': 67, 'text': 'Restart Prob.'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 31, 'end': 40, 'text': 'Hierarchy'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 9, 'end': 17, 'text': 'Vertices'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A Web site $G$ can be viewed as a directed graph with Web pages as vertices $V$ and hyperlinks as directed edges $E$ between Web pages $v_x \\rightarrow v_y$ -- excluding inter-site hyperlinks .\n","In most cases , designating the Web site entry page as the root $r$ allows for a Web site to be viewed as a rooted directed graph .\n","Web site creators and curators purposefully organize the hyperlinks between documents in a topically meaningful manner .\n","As a result , Web documents further away from the root document typically contain more specific topics than Web documents graphically close to the root document .\n","NOUN PHRASES:\n"," ['Web site', 'G', 'directed graph', 'Web pages', 'vertices', 'V', 'hyperlinks', 'directed edges', 'E', 'Web pages', 'excluding', 'inter-site hyperlinks', 'cases', 'designating', 'Web site entry page', 'root', 'r', 'allows', 'Web site', 'rooted', 'directed graph', 'Web site creators', 'curators', 'purposefully organize', 'hyperlinks', 'documents', 'meaningful manner', 'result', 'Web documents', 'root document', 'specific topics', 'Web documents', 'root document']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 0, 'end': 10, 'text': 'A Web site'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 32, 'end': 48, 'text': 'a directed graph'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 54, 'end': 63, 'text': 'Web pages'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 67, 'end': 75, 'text': 'vertices'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 84, 'end': 94, 'text': 'hyperlinks'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 98, 'end': 112, 'text': 'directed edges'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 125, 'end': 133, 'text': 'Web page'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 98, 'end': 111, 'text': 'directed edge'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 253, 'end': 257, 'text': 'root'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 222, 'end': 245, 'text': 'the Web site entry page'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\author{Baoxu Shi}  \\email{bshi@nd.edu}  \\author{Tim Weninger}            \\ affiliation { % $^{\\ast\\dag}$ Department of Computer Science and Engineering , University of Notre Dame , Notre Dame , Indiana , USA }\n","% \\date{\\today}\n","NOUN PHRASES:\n"," ['Baoxu Shi', 'bshi @ nd.edu', 'Tim Weninger', '%', 'Department', 'Computer Science', 'Engineering', 'University', 'Notre Dame', 'Notre Dame', 'Indiana', 'USA', '%']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The mechanism behind Gibbs sampling , and other Markov Chain Monte Carlo methods , requires sequential sampling steps , and execution of each step depends on the results of the previous step making Gibbs samplers , and MCMC method in general , difficult to parallelize .\n","Approximate Distributed LDA ( AD - LDA ) is one attempt to find approximate , distributed solutions to the serial inference problem by dividing documents into $P$ parts where $P$ is the number of processors and initializes the topic distribution $z$ globally .\n","Then , for every Gibbs iteration , each processor samples $\\frac{1}{P}^\\textrm{th}$ of the dataset using the $z_P$ from last Gibbs sampling iteration .\n","When all processors are finished , a global synchronization is performed and $z$ is updated ~ \\citep{newman2007distributed} .\n","NOUN PHRASES:\n"," ['mechanism', 'Gibbs sampling', 'other Markov Chain Monte Carlo methods', 'requires sequential sampling', 'steps', 'execution', 'step', 'depends', 'results', 'previous step', 'making', 'Gibbs samplers', 'MCMC method', 'parallelize', 'Approximate Distributed LDA', 'AD', 'LDA', 'attempt', 'find', 'distributed solutions', 'serial inference problem', 'dividing', 'documents', 'P', 'parts', 'P', 'number', 'processors', 'initializes', 'topic distribution', 'Gibbs iteration', 'processor', 'samples', 'P', 'th', 'dataset', 'using', 'last Gibbs', 'sampling', 'iteration', 'processors', 'global synchronization']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 434, 'end': 439, 'text': 'parts'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 453, 'end': 477, 'text': 'the number of processors'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 494, 'end': 516, 'text': 'the topic distribution'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Sampling document paths}\n","The first Gibbs sampling step is to draw a path from each document to the root through the graph .\n","The sampling distribution for a path $c_d$ is\n","\\begin{equation}\n","\\begin{split}\n","p(c_{d} | \\mathbf{c}_{-d}, \\mathbf{z}, \\mathbf{w}, \\eta, \\gamma) & \\propto p(c_d, \\mathbf{w}_d | \\mathbf{c}_{-d}, \\mathbf{z}, \\mathbf{w}_{-d}, \\gamma, \\eta) \\\\\n","& = p(\\mathbf{w}_d | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}_{-d}, \\eta) p(c_d | \\mathbf{c}_{-d}),\n","\\label{eq:pathsample}\n","\\end{split}\n","\\end{equation}\n","\\ noindent where $\\mathbf{w}$ is the count of terms in document $d$ , and $\\mathbf{w}_{-d}$ are the words without document $d$ .\n","This equation is an expression of Bayes ' theorem where the first term represents the probability of data given some choice of path from the root , and the second term represents the probability of selecting some path .\n","NOUN PHRASES:\n"," ['Sampling', 'document paths', 'first Gibbs', 'sampling', 'step', 'draw', 'path', 'document', 'root', 'graph', 'sampling', 'distribution', 'equation', 'split', 'p', 'c', '_', '-d', 'z', 'w', 'p', 'c_d', 'w', 'c', '_', '-d', 'z', 'w', '_', '-d', '= p', 'c', 'z', 'w', '_', '-d', 'p', 'c', '_', '-d', 'eq', 'pathsample', 'split', 'equation', 'w', 'count', 'terms', 'w', '_', '-d', 'words', 'equation', 'expression', 'Bayes', 'theorem', 'first term', 'represents', 'probability', 'data', 'given', 'choice', 'path', 'root', 'second term', 'represents', 'probability', 'selecting', 'path']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 169, 'end': 175, 'text': 'a path'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 139, 'end': 181, 'text': 'The sampling distribution for a path $c_d$'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 552, 'end': 586, 'text': 'the count of terms in document $d$'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 574, 'end': 582, 'text': 'document'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 615, 'end': 645, 'text': 'the words without document $d$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 633, 'end': 641, 'text': 'document'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{enumerate} \\ item\n","Each document $d \\in G$ is assigned a topic $\\beta_d \\sim $ Dir ( $\\eta$ ) :\n","\\ item For each document $d \\in G$ : \\begin{enumerate} \\ item Draw a path $\\mathbf{c}_d \\sim$ RWR ( $\\gamma$ )\n","\\ item\n","Draw an $L$ - dim topic proportion vector $\\theta$ from Dir ( $\\alpha$ ) , where $L = $ len $(\\mathbf{c}_d)$ . \\ item For each word $n \\in \\{1,\\ldots,N\\}$ : \\begin{enumerate} \\ item\n","Choose topic $z_{d,n}|\\theta \\sim$ Mult ( $\\theta_d$ ) .\n","\\ item\n","Choose word $w_{d,n} | \\{z_{d,n}, \\mathbf{c}_d, \\boldsymbol\\beta\\} \\sim$ Mult ( $\\beta_{\\mathbf{c}_d, z_{d,n} }$ ) , where $\\beta_{\\mathbf{c}_d, z_{d,n}}$ is the topic in the $z$ th position in $\\mathbf{c}_d$ .\n","\\end{enumerate}\n","\\end{enumerate}  \\end{enumerate}\n","NOUN PHRASES:\n"," ['enumerate', 'document', 'Dir', 'document', 'enumerate', 'c', 'RWR', 'L', 'dim topic proportion', 'Dir', 'L =', 'c', '_d', 'word', 'enumerate', 'd', 'Mult', 'd', 'z_', 'd', 'c', '_d', 'Mult', 'c', '_d', 'z_', 'd', 'c', '_d', 'z_', 'd', 'topic', 'th position', 'c', '_d', 'enumerate', 'enumerate', 'enumerate']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 30, 'end': 38, 'text': 'document'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 63, 'end': 68, 'text': 'topic'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 118, 'end': 126, 'text': 'document'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 171, 'end': 175, 'text': 'path'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 225, 'end': 261, 'text': 'an $L$ - dim topic proportion vector'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 234, 'end': 237, 'text': 'dim'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 347, 'end': 351, 'text': 'word'}, 'T17': {'eid': 'T17', 'label': 'PRIMARY', 'start': 409, 'end': 414, 'text': 'topic'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 473, 'end': 477, 'text': 'word'}, 'T22': {'eid': 'T22', 'label': 'PRIMARY', 'start': 624, 'end': 674, 'text': 'the topic in the $z$ th position in $\\\\mathbf{c}_d$'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ForPar(\\tcc*[f]{\\small{each document in parallel} }) { vertex $d_k \\in V$ } { \\ForEach{            } { \\Sendmsg(            ) \\tcc*[f]{\\small{Send local            ,            to node            } } \\\\ } \\If{            } { \\ForEach{            } { $d_k.n \\gets u.n$ \\\\ $d_k.z \\gets u.z$ \\\\ } } } \\caption{Path-Global Update}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['document', 'parallel', '[ f ]', 'Send', 'node', 'Path-Global Update', 'algorithm']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 56, 'end': 62, 'text': 'vertex'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 181, 'end': 185, 'text': 'node'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 29, 'end': 37, 'text': 'document'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $C$ is the distribution over all terms in $V$ , $\\mu$ is the smoothing parameter , and the length is modified by the propagation algorithm to be $ |d|^{\\prime} = (1+\\alpha)|d|$ .\n","NOUN PHRASES:\n"," ['C', 'distribution', 'terms', 'V', 'smoothing', 'parameter', 'length', 'propagation algorithm', '=', '|d|']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 24, 'end': 62, 'text': 'the distribution over all terms in $V$'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 74, 'end': 97, 'text': 'the smoothing parameter'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 104, 'end': 114, 'text': 'the length'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As a result of the upward propagation $p_{\\mu}$ function , the root document ( Web site entry page ) will contain all of the words from all of the Web pages in the Web site with different non-zero probabilities .\n","The most probable words are those that occur most frequently and most generally across all documents , and are thus propagated the most .\n","NOUN PHRASES:\n"," ['result', 'upward propagation', 'function', 'root document', 'Web site entry page', 'contain', 'words', 'Web pages', 'Web site', 'different non-zero probabilities', 'probable words', 'occur', 'documents', 'thus propagated']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 15, 'end': 37, 'text': 'the upward propagation'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The distributed HDTM inference algorithm is similar to procedural HD TM .\n","We do not detail the entire distributed HD TM inference algorithm in this paper ; however , the source code is referenced in the Section ~ \\ref{sec:conclusions} .\n","To fit HD TM to the vertex - programming model , changes in sampling sequence and attention to global synchronization were required .\n","Firstly , random walk with restart must be executed during each Gibbs iteration so that every visited node can have a random walk probability .\n","Next , every node gathers $\\mathbf{w}_{-d}$ , $\\mathbf{z}$ , and $\\mathbf{c}$ separately and decides their new path $\\mathbf{c}$ according to Eq ~ \\ref{eq:pathsample} and shown in Alg ~ \\ref{alg:dist_rwr} .\n","After a path is sampled , each node will pick sample assignments $z$ for each word $n$ across all documents / nodes $d$ in parallel according to Eq ~ \\ref{eq:wordsample} .\n","A global synchronization step is required so that each document / node can update the number of words $n$ and the topic assignments $z$ globally ; fortunately , because the topics and words are sampled according to the information provided in the { \\em path} from root            to the each document            (document            at level            ), it suffices to update the nodes on the path from            to            instead of an actual global update to all nodes in            . This vertex-programming based path-global update function is shown in Alg~\\ref{alg:dist_upate} .\n","Furthermore , this update is executed during the { \\em synchronization barrier } , which is built - in to most vertex - programming frameworks , is highly optimized , and does not lock the global system any more than the synchronization barrier already does .\n","NOUN PHRASES:\n"," ['distributed HDTM inference algorithm', 'procedural HD TM', 'do', 'not detail', 'entire distributed HD TM inference algorithm', 'paper', 'source code', 'sec', 'conclusions', 'fit', 'HD TM', 'vertex', 'programming model', 'changes', 'sampling', 'sequence', 'attention', 'global synchronization', 'random walk', 'restart', 'Gibbs iteration', 'visited node', 'have', 'random walk probability', 'node gathers', 'w', '_', '-d', 'z', 'c', 'decides', 'new path', 'c', 'according', 'eq', 'pathsample', 'shown', 'alg', 'dist_rwr', 'path', 'node', 'pick', 'sample assignments', 'word', 'documents', '/', 'parallel according', 'eq', 'wordsample', 'global synchronization step', 'document', 'node', 'update', 'number', 'words', 'topic', 'assignments', 'topics', 'words', 'information', 'provided', 'root', 'document', 'document', 'level', 'suffices', 'update', 'nodes', 'path', 'actual global update', 'nodes', 'vertex-programming based', 'path-global update function', 'alg', 'dist_upate', 'update', 'programming frameworks', 'highly optimized', 'does', 'not lock', 'global system', 'synchronization barrier', 'already does']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 622, 'end': 630, 'text': 'new path'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 775, 'end': 786, 'text': 'assignments'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 800, 'end': 804, 'text': 'word'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 820, 'end': 829, 'text': 'documents'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 832, 'end': 837, 'text': 'nodes'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 1004, 'end': 1025, 'text': 'the topic assignments'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 976, 'end': 995, 'text': 'the number of words'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 1158, 'end': 1162, 'text': 'root'}, 'T16': {'eid': 'T16', 'label': 'PRIMARY', 'start': 1186, 'end': 1194, 'text': 'document'}, 'T18': {'eid': 'T18', 'label': 'PRIMARY', 'start': 1207, 'end': 1246, 'text': 'document 9999999988 at level 9999999987'}, 'T19': {'eid': 'T19', 'label': 'PRIMARY', 'start': 1207, 'end': 1215, 'text': 'document'}, 'T21': {'eid': 'T21', 'label': 'PRIMARY', 'start': 1230, 'end': 1235, 'text': 'level'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Hyperparameters also play an important role in the shape and character of the hierarchy .\n","The $\\alpha$ parameter affects the smoothing on topic distributions , and the $\\eta$ parameter affects the smoothing on word distributions .\n","The $\\gamma$ parameter is perhaps the most important parameter because it affects the depth of the hierarchy .\n","Specifically , if $\\gamma$ is set to be large ( { \\em e.g.},            ) then resulting hierarchy is shallow. Low values ({\\em e.g.} , $\\gamma = 0.05$ ) may result in deep hierarchies , because there is a smaller probabilistic penalty for each step that the random walker takes .\n","NOUN PHRASES:\n"," ['Hyperparameters', 'also play', 'important role', 'shape', 'character', 'hierarchy', 'parameter', 'affects', 'smoothing', 'topic distributions', 'parameter', 'affects', 'smoothing', 'word distributions', 'parameter', 'important parameter', 'affects', 'depth', 'hierarchy', 'e.g', 'then resulting', 'hierarchy', 'Low values', 'e.g', 'result', 'deep hierarchies', 'probabilistic penalty', 'step', 'random walker', 'takes']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 103, 'end': 112, 'text': 'parameter'}, 'T4': {'eid': 'T4', 'label': 'PRIMARY', 'start': 175, 'end': 184, 'text': 'parameter'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 244, 'end': 253, 'text': 'parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\Globals{Vertices            , Hierarchy            , Restart Prob.            }  \\Input{Messages received containing path probabilities            }            \\ Blank Line\n","NOUN PHRASES:\n"," ['Vertices', 'Hierarchy', 'Restart Prob', 'Messages', 'received containing', 'path probabilities']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 9, 'end': 17, 'text': 'Vertices'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 31, 'end': 40, 'text': 'Hierarchy'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 54, 'end': 67, 'text': 'Restart Prob.'}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 89, 'end': 136, 'text': 'Messages received containing path probabilities'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Algorithmically , HD TM infers document hierarchies by drawing paths $\\mathbf{c}_d$ from the $r$ to the document $d$ .\n","Thus , the documents are drawn from the following generative process :\n","NOUN PHRASES:\n"," ['HD TM infers document hierarchies', 'drawing', 'c', '_d', 'Thus', 'documents', 'following', 'generative process']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 63, 'end': 68, 'text': 'paths'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 104, 'end': 112, 'text': 'document'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," notre & computer & computer \\\\ dame & engineering & engineering \\\\ engineering & science & science \\\\ university & notre & notre \\\\ science & dame & dame \\\\ computer & department & department \\\\ \t\\noalign{\\hrule height 1.5pt}  \\end{tabular} } \\caption{Comparison of most probable words in top document (in            ), and in root topic of hLDA and HDTM}  \\label{tab:inh_lm}  \\end{table}\n","NOUN PHRASES:\n"," ['notre', 'computer', 'engineering', 'science', 'notre', 'dame', 'department', 'height', 'Comparison', 'probable words', 'top document', 'root topic', 'hLDA', 'HDTM', 'tab', 'inh_lm', 'table']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table} \\ centering \\ small { \\begin{tabular} { c | c | c } $p_{\\mu}~\\alpha = .5$ & hLDA $\\gamma=1$ & HD TM $\\gamma=0.95$ \\\\ \\noalign{\\hrule height 1.5pt}\n","NOUN PHRASES:\n"," ['centering', 'c | c | c', 'hLDA', 'HD TM', 'height']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This random walker function assigns higher probabilities to parents that are at a shallower depth than those at deeper positions .\n","This is in line with the intuition that flatter hierarchies are easier for human understanding than deep hierarchies ~ \\citep{Ho2012} .\n","Simply put , the restart probability $\\gamma$ controls how much resistance there is to placing a document at successive depths .\n","NOUN PHRASES:\n"," ['random walker function', 'probabilities', 'parents', 'depth', 'positions', 'line', 'intuition', 'flatter hierarchies', 'human', 'understanding', 'deep hierarchies', 'Ho2012', 'Simply', 'put', 'restart probability', 'controls', 'much resistance', 'placing', 'document', 'successive depths']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 280, 'end': 303, 'text': 'the restart probability'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ForPar(\\tcc*[f]{each document in parallel} ) { vertex $d_k \\in V$ } { \\If{            } { $c_{d,k} \\gets c_{d,k} + \\sum(m)$ \\tcc*[f]{\\small{Add path-probs from incoming message            } } \\\\ \\ForEach{            } { \\tcc*[f]{\\small{Send prob message to children of            } } \\\\ \\ Send msg ( $child$ , $c_{d,k} + \\operatorname{log}\\left(\\frac{1-\\gamma}{\\operatorname{len}\\left(\\T.\\Ch(d_k)\\right)}\\right)$ ) \\\\ } } } \\caption{Distributed Random Walk with Restart}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['document', 'c_', 'd', 'k', 'd', 'k', 'm', 'Add path-probs', 'incoming', 'message', 'Send', 'message', 'children', 'child', 'd', 'k', 'log', 'd_k', 'Distributed Random Walk', 'Restart', 'algorithm']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 48, 'end': 54, 'text': 'vertex'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 161, 'end': 177, 'text': 'incoming message'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Recall that HDTM uses the $\\gamma$ parameter to balance the weight of topological links and topical similarities .\n","Hence when specific nodes like \\textsf{Barack Obama} are chosen as the root , higher $\\gamma$ will reduce Jaccard coefficient for high certainty nodes .\n","This is because higher $\\gamma$ values favor topology rather than content during the path and parent selection process .\n","NOUN PHRASES:\n"," ['Recall', 'HDTM', 'uses', 'parameter', 'balance', 'weight', 'topological links', 'topical similarities', 'Hence', 'specific nodes', 'Barack Obama', 'root', 'reduce', 'Jaccard coefficient', 'high certainty nodes', 'values', 'favor', 'topology', 'content', 'path', 'parent selection process']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 35, 'end': 44, 'text': 'parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Recall that the HDTM , in its most basic form , generates a hierarchy by picking the best parent for each node / document ( except the root ) .\n","In the iterative Gibbs sampling process it is an almost certainty that a given node will pick different parents during different iterations .\n","For example , say node $d$ has two parents $x$ and $y$ , it is possible that during iterations 1 -- 5 node $d$ samples node $x$ as its parent , but then in iterations 6 -- 20 node $d$ samples node $y$ to be its parent .\n","Two questions come to mind : 1 ) which parent should be ultimately picked for node $d$ in the final output graph ?\n","and 2 ) what can the distribution of samples say about the certainty of our inferred graph ?\n","NOUN PHRASES:\n"," ['Recall', 'HDTM', 'basic form', 'generates', 'hierarchy', 'picking', 'parent', 'node', 'document', 'root', 'iterative Gibbs', 'sampling', 'process', 'certainty', 'given', 'node', 'pick', 'different parents', 'different iterations', 'example', 'say', 'has', 'parents', 'iterations', 'samples', 'parent', 'iterations', 'samples', 'parent', 'questions', 'come', 'mind', 'parent', 'ultimately picked', 'final output graph', 'distribution', 'samples', 'say', 'certainty', 'inferred graph']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 304, 'end': 308, 'text': 'node'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 321, 'end': 328, 'text': 'parents'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 388, 'end': 392, 'text': 'node'}, 'T8': {'eid': 'T8', 'label': 'PRIMARY', 'start': 405, 'end': 409, 'text': 'node'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 461, 'end': 465, 'text': 'node'}, 'T12': {'eid': 'T12', 'label': 'PRIMARY', 'start': 478, 'end': 482, 'text': 'node'}, 'T14': {'eid': 'T14', 'label': 'PRIMARY', 'start': 584, 'end': 588, 'text': 'node'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Table ~ \\ref{tab:deltacon} shows that the hierarchy inferred by HDTM is indeed most similar to the Wikipedia category graph , followed by the random article hierarchy .\n","Recall that DeltaCon looks at graph topology similarity , thus the HDTM hierarchy is expected to more topologically similar to another hierarchy ( even if random ) than the article graph .\n","These results demonstrate that HDTM can identify and preserve critical topological features when inferring a hierarchy from the graph .\n","As usual , an increase in the number of LDA topics increases the goodness of fit score ( in this case measured by DeltaCon instead of likelihood ) .\n","Interestingly , the $\\gamma$ parameter and the selection of different roots did not significantly influence DeltaCon score .\n","To understand why this is , recall that different root - nodes and $\\gamma$ values result in different classification perspectives , but the different perspectives are still subsets of the same category graph with similar topological properties .\n","Hence they should have similar scores using the DeltaCon metric .\n","NOUN PHRASES:\n"," ['tab', 'deltacon', 'shows', 'hierarchy', 'inferred', 'HDTM', 'Wikipedia category graph', 'followed', 'random article hierarchy', 'Recall', 'DeltaCon', 'looks', 'graph topology similarity', 'HDTM hierarchy', 'hierarchy', 'random', 'article graph', 'results', 'demonstrate', 'HDTM', 'identify', 'preserve', 'critical topological features', 'inferring', 'hierarchy', 'graph', 'increase', 'number', 'LDA topics', 'increases', 'goodness', 'fit score', 'case', 'measured', 'DeltaCon', 'likelihood', 'parameter', 'selection', 'different roots', 'did', 'not significantly influence', 'DeltaCon score', 'understand', 'recall', 'different root', 'nodes', 'values', 'result', 'different classification perspectives', 'different perspectives', 'subsets', 'same category graph', 'similar topological properties', 'Hence', 'have', 'similar scores', 'using', 'DeltaCon']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 672, 'end': 681, 'text': 'parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," for each document $d$ that belongs to a set of categories $C_d$ , and where $C_{pa}$ is the union of the corresponding categories of the nodes in $d$ 's ancestors $\\mathbf{c}_d$ .\n","The Jaccard coefficient is useful here because it gives an quantitative measure describing how well the inferred hierarchical structure matches with the human - annotated categorical structure of Wikipedia .\n","NOUN PHRASES:\n"," ['document', 'belongs', 'set', 'categories', 'C_d', 'C_', 'pa', 'union', 'corresponding categories', 'nodes', 'ancestors', 'c', '_d', 'Jaccard coefficient', 'gives', 'quantitative measure', 'describing', 'inferred hierarchical structure matches', 'annotated categorical structure', 'Wikipedia']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 9, 'end': 17, 'text': 'document'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 38, 'end': 57, 'text': 'a set of categories'}, 'T6': {'eid': 'T6', 'label': 'PRIMARY', 'start': 88, 'end': 177, 'text': \"the union of the corresponding categories of the nodes in $d$ 's ancestors $\\\\mathbf{c}_d$\"}, 'T7': {'eid': 'T7', 'label': 'PRIMARY', 'start': 146, 'end': 162, 'text': \"$d$ 's ancestors\"}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\centering \\ small { \\begin{tabular} { c | c c c } & CompSci Web site & Wikipedia ( Cat ) & Bib. Network\n","\\\\ \\noalign{\\hrule height 1.5pt} HDTM $\\gamma=0.05$ & - 1.8570 & - 148.071 & - 0.4758 \\\\ HDTM $\\gamma=0.95$ & - 9.2412 & - 148.166 & - 0.5183 \\\\ HLDA\n","$\\gamma=1.0$ & - 8.5306 & - 50.6732 & - 8.5448 \\\\ Topic Block $\\gamma=1.0$ & \\textbf{-0.2404} & - 2.9827 &            \\\\\n","TSSB $k=10$ & - 0.5689 & \\textbf{-0.0336} & - 0.4655 \\\\ fsLDA & - 48.9149 & - 149.622 & - 0.6602 \\\\ \\noalign{\\hrule height 1.5pt}  \\end{tabular} } \\caption{Log likelihood results of the best sample from among 5,000 Gibbs iterations. Values are            . Higher values are better. Best results are in bold.}  \\label{tab:LLResults}  \\end{table}\n","NOUN PHRASES:\n"," ['c | c c c', 'CompSci Web site', 'Wikipedia', 'Cat', 'Bib', 'height', 'HDTM', '-0.2404', 'k=10', '-0.0336', 'height', 'Log', 'likelihood', 'results', 'sample', 'Gibbs iterations', 'Values', 'values', 'results', 'tab', 'LLResults']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure ~ \\ref{fig:wiki_certainty_density} shows the probability density functions for the certainties in HD TMs parent sampling process .\n","In the figures on both left and right we find that the probability densities appear to be polynomial distributions with interesting plateaus that end just before the 50 \\%, 66\\%, and 75\\% certainty scores corresponding to the near-certain (but not perfectly-certain) scores for nodes with indegree values of 2, 3, and 4 respectively. In these results, and others not displayed in this paper, we find that when the root is a general document like \\textsf{Science} , then changing $\\gamma$ does not affect the certainty distribution .\n","Given a relatively specific root like \\textsf{Barack Obama} , larger $\\gamma$ values increase certainty overall .\n","NOUN PHRASES:\n"," ['Figure', '~', 'fig', 'wiki_certainty_density', 'shows', 'probability density functions', 'certainties', 'HD TMs parent', 'sampling', 'process', 'figures', 'left', 'find', 'probability', 'densities appear', 'polynomial distributions', 'interesting plateaus', 'end', '%', '%', '% certainty', 'scores corresponding', 'scores', 'nodes', 'indegree values', 'results', 'others', 'not displayed', 'paper', 'find', 'root', 'general document', 'Science', 'then changing', 'does', 'not affect', 'certainty distribution', 'Given', 'specific root', 'Barack Obama', 'values', 'increase', 'certainty']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As discussed earlier , measuring topical similarity in hierarchies can be precarious because , in many cases , documents that are correctly situated under a topically unrelated parent may still have a strong contextual association with the parent document that outweighs the topical / language - oriented similarity as determined by the $\\gamma$ parameter .\n","As an example , consider the Wiki - articles \\textsf{Honest Leadership and Open Government Act} and \\textsf{Alexi Giannoulias} : even though these two articles are topically dissimilar , HDTM parameterized with a high $\\gamma$ value is likely to place both articles as children of the hierarchy 's root \\textsf{Barack Obama} because the high $\\gamma$ values weigh the topological link as more important that the documents ' inferred topicality .\n","If , on the other hand , HDTM was parameterized with a very low $\\gamma$ value , then \\textsf{Alexi Giannoulias} is more likely to be situated with other state senators , and the \\textsf{Honest Leadership and Open Government Act} is more likely to be situated with other legislation .\n","NOUN PHRASES:\n"," ['discussed', 'measuring', 'topical similarity', 'hierarchies', 'many cases', 'documents', 'correctly situated', 'unrelated parent', 'still have', 'strong contextual association', 'parent document', 'outweighs', 'topical / language', 'oriented', 'similarity', 'determined', 'parameter', 'example', 'consider', 'Wiki', 'articles', 'Honest Leadership', 'Open Government Act', 'Alexi Giannoulias', 'articles', 'HDTM', 'parameterized', 'value', 'place', 'articles', 'children', 'hierarchy', 'Barack Obama', 'values', 'weigh', 'topological link', 'documents', 'inferred topicality', 'other hand', 'HDTM', 'value', 'Alexi Giannoulias', 'other state senators', 'Honest Leadership', 'Open Government Act', 'other legislation']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 346, 'end': 355, 'text': 'parameter'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\centering \\begin{tabular} { r | c } & DeltaCon \\\\ \\noalign{\\hrule height 1.5pt} HD TM ( $r$ = \\textsf{Science} , $\\gamma=0.05$ ) & 0.046852 \\\\ HD TM ( $r$ = \\textsf{Science} , $\\gamma=0.95$ ) & 0.046851 \\\\ HD TM ( $r$ = \\textsf{Barack Obama} , $\\gamma=0.05$ ) & 0.046857 \\\\ HD TM ( $r$ = \\textsf{Barack Obama} , $\\gamma=0.95$ ) & 0.046856 \\\\ Random Article Hierarchy & 0.046700 \\\\ Article Graph & 0.044208 \\\\ LDA ( $k=10$ ) & 0.026770 \\\\ LDA ( $k=50$ ) & 0.037949 \\\\ \\end{tabular}  \\caption{Comparison of the Wikipedia category graph to other generated hierarchies. DeltaCon scoring means higher is better. The random article hierarchy is generated by randomly picking one parent among all possible parents, \\ie, references, for each article. The differences in DeltaCon scores show that HDTM can preserve crucial connectivity information when constructing a new hierarchical structure from the original graph.}  \\label{tab:deltacon}  \\end{table}\n","NOUN PHRASES:\n"," ['r | c', 'height', 'HD TM', 'r', 'Science', 'r', 'Science', 'r', 'Barack Obama', 'r', 'Barack Obama', 'k=10', 'k=50', 'Comparison', 'Wikipedia category graph', 'other generated hierarchies', 'DeltaCon', 'scoring', 'means', 'random article hierarchy', 'randomly picking', 'parent', 'possible parents', 'references', 'article', 'differences', 'DeltaCon scores', 'show', 'HDTM', 'preserve', 'crucial connectivity information', 'constructing', 'new hierarchical structure', 'original graph', 'tab', 'deltacon', 'table']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $$\n","\\mathcal{L}^{(t)} = \\log p\\left(\\textbf{c}_{1:D}^{(t)}, \\textbf{z}_{1:D}^{(t)}, \\textbf{w}_{1:D} | \\gamma, \\eta \\right).\n","$$\n","NOUN PHRASES:\n"," ['L', '^', 't', 'c', '_', 'D', '^', 't', 'z', '_', 'D', '^', 't', 'w', '_', 'D']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure} \\ centering \\ subfigure [ CompSci Web site ] { \\includegraphics[width=.65\\textwidth]{fig/csturk}  \\label{fig:csturk} } \\ subfigure [ Wikipedia ] { \\includegraphics[width=.65\\textwidth]{fig/wikiturk}  \\label{fig:wikiturk} } \\ subfigure [ Bib. Network ] { \\includegraphics[width=.65\\textwidth]{fig/dblpturk}  \\label{fig:dblpturk} } \\caption{The model precision for five models on three document-graph collections. Higher is better.            and            represents statistical significance from HDTM            and            respectively.}  \\label{fig:turks}  \\end{figure}\n","NOUN PHRASES:\n"," ['figure', '[', 'fig/csturk', 'fig', 'csturk', '[', 'fig/wikiturk', 'fig', 'wikiturk', 'Network ]', '[', 'fig/dblpturk', 'fig', 'dblpturk', 'model precision', 'models', 'document-graph collections', 'represents', 'statistical significance', 'HDTM', 'fig', 'turks', 'figure']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Applying those lessons to our experiments , recall that HDTM has as many topics as there are documents , and non-root document topics are mixtures of the topics on the path to the root .\n","Also recall that HLDA , TopicBlock and TSSB all generate a large number of latent topics .\n","In HLDA and TopicBlock , there are infinitely many topics / tables in the nCRP ; and practically speaking , the number of topics in the final model is much larger than the number of documents ( conditioned on the $\\gamma$ parameter ) .\n","In TSSB , the topic generation is said to be an interleaving of two stick breaking processes ; practically , this generates even larger topic hierarchies .\n","The fsLDA algorithm has as many topics as there are in hLDA , however , the fsLDA hierarchy is not redrawn during Gibbs iterations to fit the word distributions resulting in a lower likelihood .\n","Simply put , the number of topics in HDTM and fsLDA $=|V|$ $\\ll$ hPAM , hLDA and TopicBlock $\\ll$ TSSB .\n","NOUN PHRASES:\n"," ['Applying', 'lessons', 'experiments', 'recall', 'HDTM', 'has', 'many topics', 'documents', 'non-root document topics', 'mixtures', 'topics', 'path', 'root', 'Also recall', 'HLDA', 'TopicBlock', 'TSSB', 'generate', 'large number', 'latent topics', 'HLDA', 'TopicBlock', 'many topics / tables', 'nCRP', 'speaking', 'number', 'topics', 'final model', 'number', 'documents', 'conditioned', 'parameter', 'TSSB', 'topic generation', 'interleaving', 'stick breaking', 'processes', 'generates', 'topic hierarchies', 'fsLDA algorithm', 'has', 'many topics', 'hLDA', 'fsLDA hierarchy', 'Gibbs iterations', 'fit', 'word distributions', 'resulting', 'likelihood', 'Simply', 'put', 'number', 'topics', 'HDTM', 'hPAM', 'hLDA', 'TopicBlock', 'TSSB']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 878, 'end': 916, 'text': 'the number of topics in HDTM and fsLDA'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 892, 'end': 916, 'text': 'topics in HDTM and fsLDA'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The collected `` samples '' from the Markov chain are full hierarchies constructed from the selection of a path for each node $c_d$ and a word for each document $z_{d,n}$ .\n","Therefore each sampled hierarchy contains one estimation about position of each document in the hierarchy and the position of each word in a document .\n","For a given sampled hierarchy , we can assess the goodness of the hierarchy by measuring the log probability of that hierarchy and the observed words conditioned on the hyperparameters :\n","NOUN PHRASES:\n"," ['collected', 'samples', 'Markov chain', 'full hierarchies', 'constructed', 'selection', 'path', 'word', 'document', 'd', 'Therefore', 'sampled', 'hierarchy', 'contains', 'estimation', 'position', 'document', 'hierarchy', 'position', 'word', 'document', 'given', 'sampled hierarchy', 'assess', 'goodness', 'hierarchy', 'measuring', 'log probability', 'hierarchy', 'observed words', 'conditioned', 'hyperparameters']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 105, 'end': 125, 'text': 'a path for each node'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 136, 'end': 160, 'text': 'a word for each document'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The intruder detection tasks described above were offered on Amazon Mechanical Turk .\n","No specialized training is expected of the judges .\n","50 tasks were created for each data set and model combination ; each user was presented with 5 tasks at a time at a cost of \\$0.07 per task. Each task was evaluated by 15 separate judges. In order to measure the trustworthiness of a judge, 5 easy tasks were selected, {\\em i.e.} , groupings with clear intruders , and gold - standard answers were created .\n","Judges who did not answer 80 \\ % of the gold - standard answers correctly are thrown out and not paid .\n","In total the solicitation attracted 31,494 judgments , across 14 models of 50 tasks each .\n","Of these , 13,165 judgments were found to be from trustworthy judges .\n","NOUN PHRASES:\n"," ['intruder detection tasks', 'described', 'Amazon Mechanical Turk', 'No specialized training', 'judges', 'tasks', 'data set', 'model combination', 'user', 'tasks', 'time', 'cost', 'task', 'task', 'separate judges', 'order', 'measure', 'trustworthiness', 'judge', 'easy tasks', 'groupings', 'clear intruders', 'gold', 'standard answers', 'Judges', 'did', 'not answer', '%', 'gold', 'standard answers', 'not paid', 'solicitation', 'attracted', 'judgments', 'models', 'tasks', 'judgments', 'trustworthy judges']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure} \\ centering \\includegraphics[width=.65\\textwidth]{fig/bestCumLL}  \\caption{Best cumulative log complete likelihood for each tested            value. Lower            values result in deeper hierarchies.}  \\label{fig:bestcumLL}  \\end{figure}\n","NOUN PHRASES:\n"," ['figure', 'fig/bestCumLL', 'Best cumulative log complete likelihood', 'tested', 'value', 'values', 'result', 'hierarchies', 'fig', 'bestcumLL', 'figure']\n","EXPECTED:\n"," {'T2': {'eid': 'T2', 'label': 'PRIMARY', 'start': 157, 'end': 162, 'text': 'value'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The Gibbs sampling algorithm was run on HD TM for various values of $\\gamma$ , and Figure ~ \\ref{fig:bestcumLL} shows the best cumulative log likelihood for each of the tested values of $\\gamma$ .\n","We observe that HD TM with $\\gamma = 0.05$ achieved the best likelihood score .\n","Likelihood scores decreased steadily for increasing $\\gamma$ values , and HD TM with $\\gamma = 0.95$ achieved the worst likelihood score .\n","NOUN PHRASES:\n"," ['Gibbs', 'sampling', 'algorithm', 'HD TM', 'various values', 'fig', 'bestcumLL', 'shows', 'cumulative log likelihood', 'tested values', 'observe', 'HD TM', '=', 'achieved', 'likelihood score', 'Likelihood scores', 'decreased', 'increasing', 'values', 'HD TM', '=', 'achieved', 'likelihood score']\n","EXPECTED:\n"," {}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The { \\em model precision } is measured based on how well the intruders were detected by the judges .\n","Specifically , if the intruder word $w^m_k$ is from model $m$ and task $k$ , and $i^m_{k,j}$ is the intruder selected by the human judge $j$ on task $k$ in model $m$ then\n","NOUN PHRASES:\n"," ['intruders', 'judges', 'intruder word', 'task', 'k', 'k', 'j', 'intruder', 'selected', 'human judge', 'task', 'model']\n","EXPECTED:\n"," {'T1': {'eid': 'T1', 'label': 'PRIMARY', 'start': 120, 'end': 137, 'text': 'the intruder word'}, 'T3': {'eid': 'T3', 'label': 'PRIMARY', 'start': 154, 'end': 159, 'text': 'model'}, 'T5': {'eid': 'T5', 'label': 'PRIMARY', 'start': 168, 'end': 172, 'text': 'task'}, 'T10': {'eid': 'T10', 'label': 'PRIMARY', 'start': 198, 'end': 267, 'text': 'the intruder selected by the human judge $j$ on task $k$ in model $m$'}, 'T11': {'eid': 'T11', 'label': 'PRIMARY', 'start': 223, 'end': 238, 'text': 'the human judge'}, 'T13': {'eid': 'T13', 'label': 'PRIMARY', 'start': 246, 'end': 250, 'text': 'task'}, 'T15': {'eid': 'T15', 'label': 'PRIMARY', 'start': 258, 'end': 263, 'text': 'model'}}\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n"]}]},{"cell_type":"code","source":["def findNounsWithLocs(text):\n","  '''This function takes in a block of text, finds the nouns in it and then returns an array of 1s and 0s representing where those nouns are'''\n","  originalText = text\n","  #modify text here however we please in getNounPhrases\n","  nounList = getNounPhrases(text)\n","  start = 0\n","  predicted_array = np.zeros(len(originalText))\n","\n","  for word in nounList:\n","    nounStartLoc = originalText.find(word, start)\n","    nounEndLoc = nounStartLoc + len(word)\n","\n","    if abs(originalText.find('$', start) - nounStartLoc) < 25:\n","      predicted_array[nounStartLoc : nounEndLoc] = 1\n","\n","    start = nounEndLoc\n","\n","  return predicted_array"],"metadata":{"id":"WreKyj_aG2PL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(findNounsWithLocs(\"The bus is yellow.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylM5UWpPIaHw","executionInfo":{"status":"ok","timestamp":1642284331916,"user_tz":480,"elapsed":6,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"2862fbe6-1e3c-4c72-8a0e-6d7a329d88e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}]},{"cell_type":"code","source":["for column in data:\n","  paragraph = df[column][\"text\"]\n","  print(\"PARAGRAPH:\\n\", paragraph)\n","  print(\"NOUN PHRASES:\\n\", getNounPhrases(paragraph))\n","  print(\"NOUN LOCATIONS:\\n\", findNounsWithLocs(paragraph))\n","  print('*' * 280 + '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luShAjMVOD-H","executionInfo":{"status":"ok","timestamp":1642284340516,"user_tz":480,"elapsed":8605,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"335edd45-d7ce-488f-c135-005d2227487b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","NOUN LOCATIONS:\n"," [0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-rdf-term} {\\ em RDF term } $n$ has {\\ bf value } $v$ for property $p$ in an \\href{https://www.w3.org/TR/rdf11-concepts/#section-rdf-graph} {\\ em RDF graph } $G$ if there is an \\href{https://www.w3.org/TR/rdf11-concepts/#section-triples} {\\ em RDF triple } in $G$ with \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-subject} {\\ em subject } $n$ , \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-predicate} {\\ em predicate } $p$ , and \\href{https://www.w3.org/TR/rdf11-concepts/#dfn-object} {\\ em object } $v$ .\n","NOUN PHRASES:\n"," ['https', 'has', 'bf', 'value', 'property', 'https', 'G', 'https', 'section-triples', 'G', 'https', 'https', 'https']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The { \\bf members} of any ASHACL list except {\\tt rdf:nil} in an RDF graph $G$ consist of its value for { \\tt rdf : first } in $G$ followed by the members in $G$ of its value for { \\tt rdf:rest} in            . The ASHACL list {\\tt rdf:nil} has no {\\bf members } in any RDF graph .\n","NOUN PHRASES:\n"," ['ASHACL list', 'nil', 'RDF graph', 'G', 'consist', 'value', 'G', 'followed', 'members', 'G', 'value', 'rest', 'ASHACL list', 'nil', 'has', 'RDF graph']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An RDF term $n$ is an { \\bf ASHACL instance } of an RDF term $m$ in an RDF graph $G$ if there is a path in $G$ from $n$ to $m$ where the predicate of the first RDF triple in the path is { \\tt rdf : type } and the predicates of any other RDF triples in the path are { \\tt rdfs : subClassOf } .\n","NOUN PHRASES:\n"," ['RDF term', 'RDF term', 'RDF graph', 'G', 'path', 'G', 'm', 'predicate', 'first RDF triple', 'path', 'type', 'predicates', 'other RDF triples', 'path', 'subClassOf']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For a constraint component $C$ with mandatory parameters $p_1$ , ... , $p_n$ , a shape s in a shapes graph $S$ has a { \\ bf constraint } that has kind $C$ with { \\ bf mandatory parameter values } $\\left<p_1,v_1\\right>$ , \\ ldots , $\\left<p_n,v_n\\right>$ in $S$ when $s$ has $v_i$ as a value for $p_i$ in $S$ .\n","If $s$ in $S$ has a constraint that has kind $C$ in $S$ then the { \\bf optional parameter values } of the constraint in $S$ are all the $\\left<o_i,v_i\\right>$ where $o_i$ is an optional parameter of $C$ and $s$ has $v_i$ as a value for $o_i$ in $S$ .\n","The { \\ bf parameter values } of a constraint are its mandatory parameter values plus its optional parameter values .\n","NOUN PHRASES:\n"," ['constraint', 'C', 'mandatory parameters', 'shape s', 'S', 'has', 'has', 'kind', 'C', '< p_n', 'S', 'has', 'value', 'S', 'S', 'has', 'constraint', 'has', 'kind', 'C', 'S', 'constraint', 'S', 'optional parameter', 'C', 'has', 'value', 'S', 'constraint', 'mandatory parameter values', 'optional parameter values']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A blank node is an { \\ bf ill - formed property path } in an RDF graph $G$ if it has a value for more than one of either { \\tt rdf:first} or {\\tt rdf:rest} , % % CHANGE { \\tt sh:alternativePath}, {\\tt sh:inversePath} , { \\tt sh:zeroOrMorePath}, {\\tt sh:oneOrMorePath} , and { \\tt sh : zeroOrOnePath } in $G$ .\n","NOUN PHRASES:\n"," ['blank node', 'formed', 'property path', 'RDF graph', 'G', 'has', 'value', 'rest', '% % CHANGE', 'alternativePath', 'inversePath', 'zeroOrMorePath', 'oneOrMorePath', 'zeroOrOnePath', 'G']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An RDF term $n$ has an RDF term $m$ as { \\bf ASHACL type } in an RDF graph $G$ if $n$ is an ASHACL instance of $m$ in $G$ .\n","NOUN PHRASES:\n"," ['RDF term', 'has', 'RDF term', 'RDF graph', 'G', 'ASHACL instance', 'G']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If $s$ is a shape in an RDF graph $G$ with a value for { \\tt sh : deactivated } in $G$ that is not a literal with datatype { \\tt xsd : boolean } then $s$ is an { \\bf ill - formed shape } in $G$ .\n","NOUN PHRASES:\n"," ['shape', 'RDF graph', 'G', 'value', 'deactivated', 'G', 'boolean', 'ill', 'formed', 'shape', 'G']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent\\fbox{\\begin{minipage}\n","{\\ dimexpr\\ line width - 2\\fboxrule - 2\\fboxsep } {\\footnotesize \\tt \\begin{enumerate}\n","[ itemsep= - 1 mm , label= \\textbf{\\arabic*.} ]\n","\\item Haiku corpus $\\rightarrow$ POS tagger $\\rightarrow$ grammatical skeleton fragments . \\item General text corpus $\\rightarrow$ n - gram model . \\ item General text corpus $\\rightarrow$ topic vectors . \\item Combine skeleton fragments to make a haiku template . \\item Assign syllable counts to slots . \\ item Fill in the template , preferring n - grams and close topic matches .\n","\\end{enumerate} } \\end{minipage} }\n","NOUN PHRASES:\n"," ['minipage', 'enumerate', '[', 'mm', 'corpus', 'POS tagger', 'grammatical skeleton fragments', 'General text corpus', 'gram model', 'topic vectors', 'make', 'haiku template', 'Assign syllable counts', 'slots', 'template', 'preferring', 'grams', 'close topic matches', 'enumerate', 'minipage']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table} [ tp ] \\centering \\begin{tabular} { | l | l | l | l | } \\hline & Inputs & Outputs & Total \\\\ \\hline\n","Number of linked entities & 255, 101 & 4,467 & 259,568 \\\\ \\hline\n","Number of different DBpedia types linked & 8,453 & 3,439 & 10,166 \\\\ \\hline Precision & 96\\% ( $P_{I}$ ) & 98.3 \\% ( $P_{O}$ ) & 96\\% ( $P_{I+O}$ ) \\\\ \\hline \\end{tabular}  \\caption{Results of the DBpedia integration experiment}  \\label{tab:statisticsDBpediaIntegration}  \\end{table}\n","NOUN PHRASES:\n"," ['[ tp ]', '| l | l | l | l |', 'Inputs', 'Outputs', 'linked', 'entities', 'different DBpedia types', 'linked', '%', 'P_', '%', 'P_', 'O', '%', 'P_', 'I+O', 'Results', 'DBpedia integration experiment', 'tab', 'statisticsDBpediaIntegration', 'table']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Total precision of the links & 70.7 \\% & 87.3\\% & 88.1\\% \\\\ \\hline\n","Total number of links & 106,056 & 221,351 & 376,795 \\\\ \\hline\n","Total coverage of the links & 45,999 ( 27.5\\% ) & 84,350 ( 50.4\\% ) & 114,166 ( 53.9\\% ) \\\\ \\hline\n","\\end{tabular}  \\caption{Comparison of the wikiHow community integration (\\textbf{WH-C} ) with the results of our integration of wikiHow ( \\textbf{WH} ) and of both wikiHow and Snapguide ( \\textbf{WH+S} ) } \\label{tab:finalcomparison}  \\end{table}\n","The result of this comparison can be seen in Table \\ref{tab:finalcomparison} .\n","For each of the two types of links generated by the wikiHow community ( \\textbf{WH-C} ) , the precision has been evaluated manually on 200 randomly selected links .\n","The precision of the I/O links generated by our system ( \\textbf{WH+S} ) is defined as the probability that both the input and the output links involved are correct : $P_{I/O}=P_{I}*P_{O}$ .\n","As such , it can be derived from the precision values shown in Table \\ref{tab:statisticsDBpediaIntegration} .\n","The precision of the decomposition links generated by our system ( \\textbf{WH+S} ) is determined by the precision of the classifier used to select them .\n","This precision was evaluated using 10 - fold cross validation .\n","\\par It should be noted that the links generated by the wikiHow community only interlink wikiHow resources. On the contrary, our system integrates procedural knowledge both from the wikiHow and the Snapguide repositories. To make a fair comparison, Table \\ref{tab:finalcomparison} also shows the evaluation of the links generated by our system which only connect wikiHow resources ( \\textbf{WH} ) .\n","\\par\n","The result of our evaluation shows how our automatic approach to human know - how integration significantly outperforms manual community - based integration efforts .\n","This is shown for all the metrics considered and for both types of links .\n","This result demonstrates how our framework can be used to significantly increase the value of human know - how by automatic means .\n","We take this result as strong evidence for the effectiveness of our integration framework .\n","NOUN PHRASES:\n"," ['Total precision', 'links', '%', '%', 'links', 'links', '%', '%', '%', 'Comparison', 'wikiHow community integration', 'results', 'integration', 'wikiHow', 'WH', 'wikiHow', 'Snapguide', 'WH+S', 'tab', 'finalcomparison', 'result', 'comparison', 'tab', 'finalcomparison', 'types', 'links', 'generated', 'wikiHow community', 'precision', 'randomly selected', 'links', 'precision', 'I/O links', 'generated', 'system', 'WH+S', 'probability', 'input', 'output links', 'P_', 'I/O', '=P_', 'O', 'precision values', 'shown', 'tab', 'statisticsDBpediaIntegration', 'precision', 'decomposition links', 'generated', 'system', 'WH+S', 'precision', 'classifier', 'used', 'select', 'precision', 'fold cross validation', 'links', 'generated', 'wikiHow community', 'only interlink', 'wikiHow resources', 'system', 'integrates', 'procedural knowledge', 'wikiHow', 'Snapguide repositories', 'make', 'fair comparison', 'tab', 'finalcomparison', 'also shows', 'evaluation', 'links', 'generated', 'system', 'only connect', 'wikiHow resources', 'WH', 'result', 'evaluation', 'shows', 'automatic approach', 'human know', 'integration', 'significantly outperforms', 'manual community', 'based', 'integration efforts', 'metrics', 'considered', 'types', 'links', 'result', 'demonstrates', 'framework', 'significantly increase', 'value', 'human', 'know', 'automatic means', 'take', 'result', 'strong evidence', 'effectiveness', 'integration framework']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\par After extracting the Linked Data representation of a large number of processes, we applied our Linked Data integration system. Our system followed the method described in section \\ref{sec:DBpediaLinks} to discover the links between DBpedia entities and the inputs and the outputs of the processes .\n","The results of this experiment can be found in Table \\ref{tab:statisticsDBpediaIntegration} .\n","The precision was manually evaluated separately for the inputs ( $P_{I}$ ) and the outputs ( $P_{O}$ ) on 300 randomly selected links for each type .\n","A link was considered wrong ( 1 ) if it linked an entity which was not an input or an output of the process or ( 2 ) if the type of the input or output did not correspond to the linked DBpedia type .\n","NOUN PHRASES:\n"," ['extracting', 'Linked', 'Data representation', 'large number', 'processes', 'applied', 'Linked', 'Data integration system', 'system', 'followed', 'method', 'described', 'sec', 'DBpediaLinks', 'discover', 'links', 'DBpedia entities', 'inputs', 'outputs', 'processes', 'results', 'experiment', 'tab', 'statisticsDBpediaIntegration', 'precision', 'manually evaluated', 'inputs', 'P_', 'outputs', 'P_', 'O', 'randomly selected', 'links', 'type', 'link', 'linked', 'entity', 'input', 'output', 'process', 'type', 'input', 'output', 'did', 'not correspond', 'linked', 'DBpedia type']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In section \\ref{relatedWorkKnowledgeRepresentation} we discussed several issues in reusing existing knowledge representation languages in the human know - how domain .\n","This lead us to the development of a Linked Data vocabulary which is both lightweight and generic \\cite{Pareti2014} .\n","This vocabulary is sufficient to represent the two main concepts that can be reliably extracted from semi-structured human know - how .\n","These concepts , namely dependencies and process decompositions , play a key role in most procedural knowledge representation formalisms .\n","This vocabulary is based on just three properties , as shown in Table \\ref{tab:vocabulary} .\n","\\begin{table}\n","[ tp ] \\centering \\begin{tabular}\n","{ | l | l | } \\hline\n","Prefix & Namespace \\\\ \\hline prohow: & \\url{http://vocab.inf.ed.ac.uk/prohow#} \\\\ \\hline \\hline\n","Term & Definition when $X$ is the subject and $Y$ is the object \\\\ \\hline \\url{prohow:has_step} & $Y$ can help accomplishing / obtaining\n","$X$ \\\\ \\hline\n","\\url{prohow:has_method} & $Y$ can be accomplished / obtained instead of $X$ \\\\ \\hline\n","\\url{prohow:requires} & $Y$ should be accomplished / obtained before doing\n","$X$ \\\\ \\hline\n","\\end{tabular}  \\caption{The vocabulary to represent processes}  \\label{tab:vocabulary}  \\end{table}\n","NOUN PHRASES:\n"," ['relatedWorkKnowledgeRepresentation', 'discussed', 'several issues', 'reusing existing', 'knowledge representation', 'languages', 'human', 'domain', 'development', 'Linked', 'Data vocabulary', 'Pareti2014', 'represent', 'main concepts', 'reliably extracted', 'semi-structured human', 'know', 'concepts', 'dependencies', 'process decompositions', 'play', 'key role', 'procedural knowledge representation formalisms', 'vocabulary', 'properties', 'shown', 'tab', '[ tp ]', '| l | l |', 'http', '//vocab.inf.ed.ac.uk/prohow', 'Definition', 'X', 'subject', 'Y', 'prohow', 'has_step', 'Y', 'help accomplishing / obtaining', 'X', 'prohow', 'has_method', 'Y', '/', 'obtained', 'X', 'prohow', 'requires', 'Y', '/', 'obtained', 'doing', 'X', 'vocabulary', 'represent', 'processes', 'tab']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Conduction Cost}\n","A pure double elimination system with $i$ upper rounds has $2^i$ participants , and there will be $(2^i-1)+(2^{i-1}-1)$ matches conducted .\n","Thus , a double elimination tournament for 8 participants will have 10 matches conducted .\n","NOUN PHRASES:\n"," ['Conduction Cost', 'A pure double elimination system', 'upper rounds', 'has', 'participants', '+', '-1', 'matches', 'conducted', 'double elimination tournament', 'participants', 'have', 'matches', 'conducted']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Even with increased number of matches conducted compared to single elimination , it is still very lacking in the ranking precision .\n","However , double elimination has two starting nodes ( see Figure ~ \\ref{fig:DEDT} ) .\n","It can be argued that double elimination is designed for a multi-stage system , or the players are distributed properly by using a rating system \\cite{elo,5446231,trueskilltm-a-bayesian-skill-rating-system,trueskill-through-time-revisiting-the-history-of-chess,5283063,5671406} .\n","Thus , it is expected to have stronger participants and weaker participants distributed ( seeded ) into upper bracket and lower bracket properly .\n","We perform another experiment this way , as a seeded double elimination .\n","There will be $4! * 4! = 576$ cases this time .\n","NOUN PHRASES:\n"," ['increased number', 'matches', 'conducted compared', 'single elimination', 'still very lacking', 'ranking precision', 'double elimination', 'has', 'starting', 'nodes', 'see', 'fig', 'DEDT', 'double elimination', 'multi-stage system', 'players', 'using', 'elo,5446231', 'trueskilltm-a-bayesian-skill-rating-system', 'have', 'participants', 'participants', 'distributed', 'seeded', 'upper bracket', 'bracket', 'perform', 'experiment', 'way', 'seeded double elimination', '*', '=', 'cases', 'time']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Conduction Cost}\n","A pure single elimination system with $i$ rounds has $n = 2^i$ participants , and there will be $n-1$ matches conducted .\n","For 8 players single elimination , there would be 7 matches with 3 rounds .\n","NOUN PHRASES:\n"," ['Conduction Cost', 'A pure single elimination system', 'rounds has', 'n =', 'participants', 'matches', 'conducted', 'players single elimination', 'matches', 'rounds']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," With the consistent prizes in the same unit , we would be able to evaluate nodes in the progress tree .\n","Since we are only considering the structure of the tournament , we evaluate a node as the average value of its direct child nodes .\n","For example , in Figure ~ \\ref{fig:SEDT} tournament , let $x_1$ , $x_2$ , $x_3$ and $x_4$ be 1st place prize , 2nd prize , 3rd - 4th prize and 5th - 8th prize .\n","Then we have $x1 \\geq x2 \\geq x3 \\geq x4$ .\n","Table ~ \\ref{tab:SEEV} shows the evaluation of other nodes .\n","We call this value \" stability \" value .\n","\\begin{table} [ h ]            }            \\centering            {l c }            } &            }\\\\ \\hline\\\\\n","Final ( $v$ ) &             \\\\\\\\ Semi-final ( $v_1$ ) &             \\\\\\\\\n","Quarter- final ( $v_2$ ) & $\\frac{v_1 + x_4}{2}$\n","\\end{tabular}  \\end{table}\n","NOUN PHRASES:\n"," ['consistent prizes', 'same unit', 'evaluate', 'nodes', 'progress tree', 'only considering', 'structure', 'tournament', 'evaluate', 'node', 'average value', 'direct child nodes', 'example', 'fig', 'SEDT', 'tournament', 'let', 'x_1', 'place prize', 'prize', 'prize', 'prize', 'have', 'tab', 'SEEV', 'shows', 'evaluation', 'other nodes', 'call', 'value', 'stability', 'value', '[ h ]', 'l c', 'v_1 + x_4']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Conducting Cost}\n","A pure round - robin system with $n$ participants has $\\frac{n}{2}(n-1)$ matches conducted .\n","Thus for 8 participants , there would be 28 matches .\n","NOUN PHRASES:\n"," ['Conducting', 'Cost', 'A pure round', 'robin system', 'participants', 'has', 'matches', 'conducted', 'Thus', 'participants', 'matches']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We first show that for any set atom $C$ with arity $n$ ( i.e. , the number of sets involved in the atom ) and set $S$ of atoms , if $C$ is satisfied by $A$ , there is a minimal support for $C$ in $A$ .\n","Let the sets occurring in $C$ be $S_1, \\ldots, S_n$ where $S_i = \\{X_i: condition_i(X_i)\\}$ .\n","For $i \\in 1..n$ , let $T_i = \\{condition_i(t): condition_i(t) \\in A\\}$ .\n","Clearly $\\bar{T}^n$ satisfies the first two conditions in the definition of { \\em minimal support } .\n","By reducing $T_i$ if necessary ( ?? what if $T_i$ is infinite ?? ) , we can always find a minimal support for $C$ in $A$ .\n","NOUN PHRASES:\n"," ['first show', 'set', 'C', 'i.e', 'number', 'sets', 'involved', 'atom', 'set', 'S', 'atoms', 'C', 'minimal support', 'C', 'Let', 'sets', 'occurring', 'C', 'S_1', 'S_n', 'X_i', 'condition_i', 'X_i', 'let', 'condition_i', 't', 'condition_i', 't', 'Clearly', 'T', '^n', 'satisfies', 'conditions', 'definition', 'reducing', 'T_i', 'T_i', 'always find', 'minimal support', 'C']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Let $R_{SS}(R_I(\\Pi, A), A)$ be a Slog + set reduction ( by replacing any set atom $C$ by a minimal support for $C$ in $A$ ) of the set introduction reduct of $\\Pi$ wrt $A$ .\n","NOUN PHRASES:\n"," ['Let', 'R_', 'SS', 'R_I', 'A', 'A', 'Slog +', 'set', 'reduction', 'replacing', 'set', 'C', 'minimal support', 'C', 'A', 'set introduction reduct']\n","NOUN LOCATIONS:\n"," [1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition}\n","[ Rule Satisfaction and Supportedness ]\n","\\label{p1aa}\n","Let $A$ be an $\\mathcal{A}log$ or $\\mathcal{S}log^+$ answer set of a ground program $\\Pi$ .\n","Then \\begin{itemize} \\ item $A$ satisfies every rule $r$ of            . \\ item\n","If $p(\\bar{t}) \\in A$ then there is a rule $r$ from $\\Pi$ such that the body of $r$ is satisfied by $A$ and            \\ item $p(\\bar{t})$ is the only atom in the head of $r$ which is true in $A$ or \\ item the head of $r$ % contains an is of the form $p \\odot \\{\\bar{X} : q(\\bar{X})\\}$ and $q(\\bar{t}) \\in A$ .\n","( It is often said that rule $r$ supports atom $p$ . )\n","\\end{itemize}  \\end{itemize}  \\end{proposition}\n","NOUN PHRASES:\n"," ['proposition', '[ Rule Satisfaction', 'p1aa', 'Let', 'S', 'log^+', 'set', 'ground program', 'itemize', 'item', 'satisfies', 'rule', 'r', 'p', 't', 'rule', 'body', 'A', 'item', 'p', 't', 'only atom', 'head', 'r', 'item', 'head', '%', 'contains', 'X', 'q', 'X', 'q', 't', 'often said', 'rule', 'r', 'supports', 'atom', 'itemize', 'itemize', 'proposition']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","A \\emph{set atom} of $\\mathcal{A}log$ is an expression of the form\n","\\begin{equation}\\label{aggr-atom1}\n","f_1(S_1) \\odot f_2(S_2)\n","\\end{equation}\n","or\n","\\begin{equation}\\label{aggr-atom2}\n","f(S) \\odot k\n","\\end{equation}\n","where $f$ , $f_1,f_2$ are functions from ${\\cal A}$ , $S$ , $S_1$ , $S_2$ are set names , $k$ is a number , and $\\odot$ is an arithmetic relation $>, \\geq, <, \\leq,=$ or ! = , or of the form\n","\\begin{equation}\\label{set-atom}\n","S_1 \\otimes S_2\n","\\end{equation}\n","where $\\otimes$ is $\\subset, \\subseteq$ , or $=$ .\n","We often write $f(\\{\\bar{X}:p(\\bar{X})\\})$ as $f\\{\\bar{X}:p(\\bar{X})\\}$ and $\\{\\bar{X} : p(\\bar{X})\\} \\otimes S$ and $S \\otimes \\{\\bar{X} : p(\\bar{X})\\}$ as $p \\otimes S$ and $S \\otimes p$ respectively .\n","Regular and set atoms are referred to as \\emph{atoms} .\n","A \\emph{rule} of $\\mathcal{A}log$ is an expression of the form\n","\\begin{equation}\\label{rule}\n","head \\leftarrow body%pos,neg,pos\\_set,neg\\_set\n","\\end{equation}\n","where $head$ is a disjunction of regular literals or a set atom of the form $p \\subseteq S$ , $S \\subseteq q$ , or $p = S$ , and $body$ is a collection of regular literals ( possibly preceded by $not$ ) and set atoms .\n","A rule with set atom in the head is called \\emph{set introduction rule} .\n","Note that both head and body of a rule can be infinite .\n","All parts of $\\mathcal{A}log$ rules , including $head$ , can be empty .\n","A \\emph{program} of $\\mathcal{A}log$ is a collection of $\\mathcal{A}log$ 's rules .\n","NOUN PHRASES:\n"," ['set', 'atom', 'expression', 'form', 'equation', 'f_1', 'S_1', 'f_2', 'S_2', 'equation', 'equation', 'f', 'S', 'equation', 'f_1', 'functions', 'S', 'S_1', 'S_2', 'names', 'number', 'arithmetic relation', '<', '=', '=', 'form', 'equation', 'equation', 'often write', 'f', 'p', 'X', 'p', 'X', 'p', 'X', 'S', 'X', 'p', 'X', 'p', 'set atoms', 'atoms', 'rule', 'expression', 'form', 'equation', 'rule', 'head', 'body % pos', 'equation', 'head', 'disjunction', 'regular literals', 'set atom', 'p = S', 'collection', 'regular literals', 'possibly preceded', 'set', 'atoms', 'rule', 'set atom', 'head', 'set', 'introduction rule', 'Note', 'head', 'body', 'rule', 'parts', 'rules', 'including', 'program', 'collection', 'rules']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 1. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Let $\\Sigma$ be a ( possibly sorted ) signature with a finite collection of predicate and function symbols and ( possibly infinite ) collection of object constants , and let $\\mathcal{A}$ be a finite collection of symbols used to denote functions from sets of terms of $\\Sigma$ into integers .\n","Terms and literals over signature $\\Sigma$ are defined as usual and referred to as \\emph{regular} .\n","Regular terms are called \\emph{ground} if they contain no variables and no occurrences of symbols for arithmetic functions .\n","Similarly for literals .\n","We refer to an expression\n","\\begin{equation}\\label{set-name}\n","\\{\\bar{X}:cond\\}\n","\\end{equation}\n","where $cond$ is a finite collection of regular literals and $\\bar{X}$ is the list of variables occurring in $cond$ , as a \\emph{set name} .\n","It is read as \\ emph { the set of all objects of the program believed to satisfy $cond$ } .\n","Variables from $\\bar{X}$ are often referred to as { \\em set variables } .\n","An occurrence of a set variable in ( \\ref{set-name} ) is called \\emph{bound} within ( \\ref{set-name} ) .\n","Since treatment of variables in extended $\\mathcal{A}log$ is the same as in the original language we limit our attention to programs in which every occurrence of a variable is bound .\n","Rules containing non-bound occurrences of variables are considered as shorthands for their ground instantiations ( for details see \\cite{GelfondZ14} ) .\n","NOUN PHRASES:\n"," ['Let', 'possibly sorted', 'signature', 'finite collection', 'predicate', 'function symbols', 'collection', 'object constants', 'let', 'finite collection', 'symbols', 'used', 'denote', 'functions', 'sets', 'terms', 'integers', 'Terms', 'literals', 'referred', 'Regular terms', 'ground', 'contain', 'variables', 'occurrences', 'symbols', 'arithmetic functions', 'literals', 'refer', 'equation', 'X', 'equation', 'cond', 'finite collection', 'regular literals', 'X', 'list', 'variables', 'occurring', 'set', 'name', 'set', 'objects', 'program', 'believed', 'satisfy', 'Variables', 'X', 'often referred', 'occurrence', 'set variable', 'bound', 'treatment', 'variables', 'original language', 'limit', 'attention', 'programs', 'occurrence', 'Rules', 'containing', 'non-bound occurrences', 'variables', 'shorthands', 'ground instantiations', 'details', 'see', 'GelfondZ14']\n","NOUN LOCATIONS:\n"," [1. 1. 1. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section we give some basic properties of $\\mathcal{A}log$ and $\\mathcal{S}log^+$ programs .\n","Propositions \\ref{p1aa} and \\ref{p2aa} ensure that , as in regular ASP , answer sets of $\\mathcal{A}log$ program are formed using the program rules together with the rationality principle .\n","Proposition \\ref{split} is the $\\mathcal{A}log$ / $\\mathcal{S}log^+$ version of the Splitting Set Theorem -- basic technical tool used in theoretical investigations of ASP and its extensions \\cite{GelfondP92,lt94,Turner96} .\n","NOUN PHRASES:\n"," ['section', 'give', 'basic properties', 'S', 'log^+', 'programs', 'Propositions', 'p1aa', 'p2aa', 'ensure', 'regular ASP', 'sets', 'program', 'program rules', 'rationality principle', 'split', 'S', 'log^+', 'version', 'Splitting Set Theorem', 'basic technical tool', 'used', 'theoretical investigations', 'ASP', 'extensions', 'GelfondP92', 'lt94', 'Turner96']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To define the semantics of $\\mathcal{A}log$ programs we first notice that the \\emph{standard definition of answer set from \\cite{gl91} is applicable to programs with infinite rules } .\n","Hence we already have the definition of answer set for $\\mathcal{A}log$ programs not containing occurrences of set atoms .\n","We also need the satisfiability relation for set atoms .\n","Let $A$ be a set of ground regular literals .\n","If $f(\\{\\bar{t}:cond(\\bar{t}) \\subseteq A\\})$ is defined then $f(\\{\\bar{X}:cond\\}) \\geq k$ is satisfied by $A$ ( is \\emph{true} in $A$ ) iff $f(\\{\\bar{t}:cond(\\bar{t}) \\subseteq A\\}) \\geq k$ .\n","Otherwise , $f(\\{\\bar{X}:cond\\}) \\geq k$ is falsified ( is \\emph{false} in $A$ ) .\n","If $f(\\{\\bar{t}:cond(\\bar{t})\n","\\subseteq A\\})$ is not defined then $f(\\{\\bar{X}:cond\\}) \\geq k$ is \\emph{undefined} in $A$ .\n","( For instance , atom $card\\{X:p(X)\\} \\geq 0$ is undefined in $A$ if $A$ contains an infinite collection of atoms formed by $p$ . )\n","Similarly for other set atoms .\n","Finally a rule is \\emph{satisfied} by $S$ if its head is \\emph{true} in $S$ or its body is \\emph{false} or \\emph{undefined} in $S$ .\n","\\subsubsection{Answer Sets for Programs without Set Introduction Rules.}\n","To simplify the presentation we first give the definition of answer sets for programs whose rules contain no set atoms in their heads .\n","First we need the following definition :\n","\\begin{definition} [ Set Reduct of $\\mathcal{A}log$ ] \\label{reduct1}\n","Let $\\Pi$ be a ground program of $\\mathcal{A}log$ .\n","The \\emph{set reduct} of $\\Pi$ with respect to a set of ground regular literals $A$ is obtained from $\\Pi$ by \\begin{enumerate} \\ item removing rules containing set atoms which are \\emph{false} or \\emph{undefined} in $A$ . \\ item replacing every remaining set atom $SA$ by the union of $cond(\\bar{t})$ such that $\\{\\bar{X} : cond(\\bar{X})\\}$ occurs in $SA$ and $cond(\\bar{t})\n","\\subseteq A$ .\n","\\end{enumerate}  \\end{definition}\n","The first clause of the definition removes rules useless because of the truth values of their aggregates in $A$ .\n","The next clause reflects the principle of avoiding vicious circles .\n","Clearly , set reducts do not contain set atoms .\n","\\begin{definition} [ Answer Set ]\n","\\label{ans-set}\n","A set $A$ of ground regular literals over the signature of a ground $\\mathcal{A}log$ program $\\Pi$ is an \\emph{answer set} of $\\Pi$ if $A$ is an answer set of the set reduct of $\\Pi$ with respect to $A$ .\n","\\end{definition}\n","It is easy to see that for programs of the original $\\mathcal{A}log$ our definition coincides with the old one .\n","Next several examples demonstrate the behavior of our semantics for programs not covered by the original syntax .\n","NOUN PHRASES:\n"," ['define', 'semantics', 'programs', 'first notice', 'standard definition', 'answer', 'set', 'gl91', 'programs', 'infinite rules', 'Hence', 'already have', 'definition', 'answer', 'set', 'programs', 'not containing', 'occurrences', 'set atoms', 'also need', 'satisfiability relation', 'set atoms', 'Let', 'set', 'ground regular literals', 'f', 'cond', 't', 'f', 'A', 'iff', 'f', 'cond', 't', 'Otherwise', 'f', 'A', 'f', 'cond', 't', 'not defined', 'f', 'instance', 'X', 'p', 'X', 'contains', 'infinite collection', 'atoms', 'formed', 'other set atoms', 'rule', 'S', 'head', 'S', 'body', 'S', 'Answer Sets', 'Programs', 'Set Introduction Rules', 'simplify', 'presentation', 'first give', 'definition', 'sets', 'programs', 'rules', 'contain', 'set atoms', 'heads', 'First', 'need', 'following definition', 'definition', '[ Set Reduct', 'reduct1', 'Let', 'ground program', 'set', 'reduct', 'respect', 'set', 'ground regular literals', 'enumerate', 'removing', 'rules', 'containing', 'set atoms', 'replacing', 'remaining', 'set', 'SA', 'union', 'cond', 't', 'cond', 'X', 'occurs', 'SA', 'cond', 't', 'enumerate', 'definition', 'first clause', 'definition', 'removes', 'rules', 'truth values', 'aggregates', 'next clause', 'reflects', 'principle', 'avoiding', 'vicious circles', 'set', 'reducts', 'do', 'not contain', 'set atoms', 'definition', 'A', 'set', 'ground regular literals', 'signature', 'ground', 'program', 'answer', 'set', 'answer set', 'set reduct', 'respect', 'definition', 'see', 'programs', 'definition coincides', 'old one', 'Next several examples', 'demonstrate', 'behavior', 'semantics', 'programs', 'not covered', 'original syntax']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st\n","We view this restriction as a possible interpretation of VCP and refer to it as \\emph{Strong VCP} .\n","Let us illustrate the intuition behind $\\mathcal{A}log$ set constructs .\n","\\begin{example}            %\n","[ Example \\ref{e1} revisited ]\n","{ \\rm Let us consider programs from Example \\ref{e1} .\n","$P_0$ clearly has no answer set since $\\emptyset$ does not satisfy its rule and there is no justification for believing in $p(1)$ .\n","$P_1$ is also inconsistent .\n","To see that notice that the first two rules of the program limit our possibilities to $A_1 = \\emptyset$ and $A_2=\\{p(0), p(1)\\}$ .\n","In the first case $\\{X:p(X)\\}$ denotes $\\emptyset$ .\n","But this contradicts the last rule of the program .\n","$A_1$ cannot be an answer set of $P_1$ .\n","In $A_2$ , $\\{X:p(X)\\}$ denotes $S=\\{0, 1\\}$ .\n","But this violates our form of VCP since the reasoner 's beliefs in both , $p(0)$ and $p(1)$ , cannot be established without reference to $S$ .\n","$A_2$ is not an answer set either .\n","Now consider program $P_2$ .\n","There are two candidate answer sets \\\n","footnote\n","{ By a candidate answer set we mean a consistent set of ground regular literals satisfying the rules of the program . } : $A_1 =\n","\\emptyset$ and $A_2 = \\{p(1)\\}$ .\n","In $A_1$ , $S = \\emptyset$ which contradicts the rule .\n","In $A_2$ , $S=\\{1\\}$ but this would contradict the $\\mathcal{A}log$ 's VCP .\n","The program is inconsistent \\\n","footnote\n","{ There is a common argument for the semantics in which $\\{p(1)\\}$ would be the answer set of $P_2$ :\n","`` Since $card\\{X: p(X)\\} \\geq 0$ is always true it can be dropped from the rule without changing the rule 's meaning '' .\n","But the argument assumes existence of the set denoted by $\\{X:p(X)\\}$ which is not always the case in $\\mathcal{A}log$ .}. }\n","\\end{example}\n","We hope that the examples are sufficient to show how the informal semantics of $\\mathcal{A}log$ can give a programmer some guidelines in avoiding formation of sets problematic from the standpoint of VCP .\n","In what follows we \\begin{itemize}\n","\\item\n","Expand $\\mathcal{A}log$ by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching .\n","\\item\n","Propose an alternative formalization of the original VCP and incorporate it into the semantics of new language , $\\mathcal{S}log^+$ , which allows more liberal construction of sets and their use in programming rules .\n","( The name of the new language is explained by its close relationship with language $\\mathcal{S}log$ \\cite{SonP07} -- see Theorem 2 ) .\n","\\item\n","Show that , for programs without disjunction and infinite sets , the formal semantics of aggregates in $\\mathcal{S}log^+$ coincides with that of several other known languages .\n","Their intuitive and formal semantics , however , are based on quite different ideas and seem to be more involved than that of $\\mathcal{S}log^+$ .\n","\\item\n","Prove some basic properties of programs in ( extended ) $\\mathcal{A}log$ and $\\mathcal{S}log^+$ .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['view', 'restriction', 'possible interpretation', 'VCP', 'refer', 'Strong VCP', 'Let', 'illustrate', 'intuition', 'set', 'constructs', 'example', 'e1', 'revisited', 'consider', 'programs', 'e1', 'P_0', 'clearly has', 'answer', 'set', 'does', 'not satisfy', 'rule', 'justification', 'believing', 'p', 'P_1', 'see', 'notice', 'rules', 'program limit', 'possibilities', 'p', 'p', 'first case', 'X', 'p', 'X', 'denotes', 'contradicts', 'last rule', 'program', 'answer set', 'P_1', 'A_2', 'X', 'p', 'X', 'denotes', 'violates', 'form', 'VCP', 'reasoner', 'beliefs', 'p', 'p', 'reference', 'S', 'A_2', 'answer set', 'Now consider', 'program', 'P_2', 'sets', 'candidate answer', 'set', 'mean', 'consistent set', 'ground regular literals', 'satisfying', 'rules', 'program', 'p', 'A_1', 'contradicts', 'rule', 'A_2', 'contradict', 'VCP', 'program', 'common argument', 'semantics', 'p', 'answer set', 'P_2', 'X', 'p', 'X', 'rule', 'changing', 'rule', 'meaning', 'argument', 'assumes', 'existence', 'set', 'denoted', 'X', 'p', 'X', 'case', 'example', 'hope', 'examples', 'show', 'informal semantics', 'give', 'programmer', 'guidelines', 'avoiding', 'formation', 'sets', 'standpoint', 'VCP', 'follows', 'itemize', 'allowing', 'infinite sets', 'several additional set', 'related', 'constructs', 'knowledge representation', 'teaching', 'alternative formalization', 'original VCP', 'incorporate', 'semantics', 'new language', 'S', 'log^+', 'allows', 'liberal construction', 'sets', 'use', 'programming rules', 'name', 'new language', 'close relationship', 'language', 'S', 'log', 'SonP07', 'see', 'Theorem', 'Show', 'programs', 'disjunction', 'infinite sets', 'formal semantics', 'aggregates', 'S', 'log^+', 'coincides', 'several other known', 'languages', 'formal semantics', 'different ideas', 'seem', 'S', 'log^+', 'Prove', 'basic properties', 'programs', 'extended', 'S', 'log^+', 'itemize']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In what follows we retain the name $\\mathcal{A}log$ for the new language and refer to the earlier version as `` original $\\mathcal{A}log$ '' .\n","NOUN PHRASES:\n"," ['follows', 'retain', 'name', 'new language', 'refer', 'version']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\st{\\bf Infinite Universe}\n","\\begin{example} [ Aggregates on infinite sets ]\n","\\label{new1}\n","Consider a program $E_1$ consisting of the following rules :\n","\\begin{verbatim} even ( 0 ) .\n","even ( I +2 ) :- even ( I ) .\n","q :- min { X : even ( X ) } = 0.            \\noindent\n","It is easy to see that the program has one answer set , $S_{E_1} = \\{q,even(0),even(2),\\dots\\}$ .\n","Indeed , the reduct of $E_1$ with respect to $S_{E_1}$ is the infinite collection of rules\n","\\begin{verbatim} even ( 0 ) .\n","even ( 2 ) :\n","- even ( 0 ) . ...\n","q :- even ( 0 ) , even ( 2 ) , even ( 4 ) ...\n","           \\noindent\n","The last rule has the infinite body constructed in the last step of definition \\ref{reduct1} .\n","Clearly , $S_{E_1}$ is a subset minimal collection of ground literals satisfying the rules of the reduct ( i.e. its answer set ) .\n","Hence $S_{E_1}$ is an answer set of $E_1$ .\n","\\end{example}\n","\\begin{example} [ Programs with undefined aggregates ]\n","\\label{new2}\n","Now consider a program $E_2$ consisting of the rules :\n","\\begin{verbatim} even ( 0 ) .\n","even ( I +2 ) :- even ( I ) .\n","q :- card { X : even ( X ) } > 0.           \n","NOUN PHRASES:\n"," ['example', '[ Aggregates', 'infinite sets', ']', 'new1', 'Consider', 'program', 'E_1', 'consisting', 'following rules', 'verbatim', '+2', 'q', 'min', 'X', 'X', 'see', 'program', 'has', 'answer set', 'S_', 'E_1', 'q', 'reduct', 'E_1', 'respect', 'S_', 'E_1', 'infinite collection', 'rules', 'verbatim', 'q', 'last rule', 'has', 'infinite body', 'constructed', 'last step', 'reduct1', 'Clearly', 'S_', 'E_1', 'subset minimal collection', 'ground literals', 'satisfying', 'rules', 'reduct', 'answer', 'set', 'Hence', 'S_', 'E_1', 'answer set', 'E_1', 'example', 'example', '[', 'Programs', 'undefined aggregates', ']', 'Now consider', 'program', 'E_2', 'consisting', 'rules', 'verbatim', '+2', 'q', 'card', 'X', 'X']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st ( Here student is eliminated from the parameters and we are limited to only one required class , $c$ . )\n","Even though in this case the answers are correct , unprincipled use of default negation leads to some potential difficulties .\n","Suppose , for instance , that a student may graduate if given a special permission .\n","This can be naturally added as a rule\n","NOUN PHRASES:\n"," ['student', 'parameters', 'required class', 'case', 'answers', 'unprincipled use', 'default negation', 'leads', 'potential difficulties', 'Suppose', 'instance', 'student', 'graduate', 'given', 'special permission', 'naturally added', 'rule']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st {\\tt ready \\_to\\_graduate(S) :- \\{C: required(C)\\} $\\subseteq$ \\{ C : taken ( S , C ) \\ } . }\n","NOUN PHRASES:\n"," ['S', 'C', 'required', 'C', 'C', 'taken', 'S', 'C']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","This program has one answer set , $S_{E_2}= \\{even(0),even(2),\\dots\\}$ .\n","Since our aggregates range over natural numbers , the aggregate $card$ is not defined on the set $card\\{t :\n","even(t) \\in S_{E_2}\\}$ .\n","This means that the body of the last rule is undefined .\n","According to clause one of definition \\ref{reduct1} this rule is removed .\n","The reduct of $E_2$ with respect to $S_{E_2}$ is \\begin{verbatim} even ( 0 ) .\n","even ( 2 ) :- even ( 0 ) .\n","even ( 4 ) :- even ( 2 ) . ... \\end{verbatim}\n","Hence $S_{E_2}$ is the answer set of $E_2$ .\n","\\ footnote\n","{ Of course this is true only because of our ( somewhat arbitrary ) decision to limit aggregates of $\\mathcal{A}log$ to those ranging over natural numbers .\n","We could , of course , allow aggregates mapping sets into ordinals .\n","In this case the body of the last rule of $E_2$ will be defined and the only answer set of $E_2$ will be $S_{E_1}$ . }\n","It is easy to check that , since every set $A$ satisfying the rules of $E_2$ must contain all even numbers , $S_{E_1}$ is the only answer set .\n","NOUN PHRASES:\n"," ['program', 'has', 'answer set', 'S_', 'E_2', 'aggregates', 'range', 'natural numbers', 'not defined', 'set', 't', 't', 'S_', 'E_2', 'means', 'body', 'last rule', 'According', 'clause', 'reduct1', 'rule', 'reduct', 'E_2', 'respect', 'S_', 'E_2', 'verbatim', 'verbatim', 'Hence', 'S_', 'E_2', 'answer set', 'E_2', 'course', 'decision', 'limit', 'aggregates', 'ranging', 'natural numbers', 'course', 'allow', 'aggregates', 'mapping', 'sets', 'ordinals', 'case', 'body', 'last rule', 'E_2', 'only answer set', 'E_2', 'S_', 'E_1', 'check', 'set', 'satisfying', 'rules', 'E_2', 'contain', 'numbers', 'S_', 'E_1', 'only answer', 'set']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," To make weak VCP based semantics precise we need the following notation and definitions :\n","By $\\bar{W}^n, \\bar{V}^n$ we denote n - ary vectors of sets of ground regular literals and by $W_i$ , $V_i$ their $i$ - th coordinates .\n","$\\bar{W}^n \\leq \\bar{V}^n$ if for every $i$ , $W_i \\subseteq V_i$ .\n","$\\bar{W}^n < \\bar{V}^n$ if $\\bar{W}^n \\leq \\bar{V}^n$ and $\\bar{W}^n \\not= \\bar{V}^n$ .\n","A set atom $C(\\{X : p_1(X)\\},\\dots,\\{X : p_1(X)\\})$ is \\emph{satisfied} by $\\bar{W}^n$ if $C(\\{t : p_1(t) \\in W_1\\}, \\dots, \\{t : p_n(t) \\in W_n\\})$ is true .\n","NOUN PHRASES:\n"," ['make', 'weak VCP', 'based', 'semantics', 'need', 'following notation', 'definitions', 'W', '^n', 'V', '^n', 'denote', 'ary vectors', 'sets', 'ground regular literals', 'W_i', 'V_i', 'i', 'th coordinates', 'W', 'V', '^n', 'W', 'V', '^n', 'W', 'V', '^n', 'W', 'V', '^n', 'A', 'set', 'C', 'X', 'p_1', 'X', 'X', 'p_1', 'X', 'W', '^n', 'C', 't', 'p_1', 't', 't', 'p_n', 't']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{definition} [ Minimal Support ]\n","\\label{d1}\n","Let $A$ be a set of ground regular literals of $\\Pi$ , and $C$ be a set atom with $n$ parameters .\n","$\\bar{W}^n$ is a \\emph{minimal support} for $C$ in $A$ if            \\ item %\n","For every $i$ , $W^0_i \\subseteq \\{\\bar{t} : p_i(\\bar{t}) \\in A$ For ever $1 \\leq i \\leq n$ ,            . \\ item Every $\\bar{V}^n$ % $W=\\langle W_1,\\dots,W_n \\rangle$ where $W_i =\\{\\bar{t} : p_i(\\bar{t})$ such that for every $1 \\leq i \\leq n$ , $W_i \\subseteq V_i \\subseteq A$ satisfies $C$ .\n","NOUN PHRASES:\n"," ['definition', 'd1', 'Let', 'set', 'ground regular literals', 'C', 'set atom', 'parameters', 'W', '^n', 'minimal support', 'C', 't', 'p_i', 't', 'V', '^n', '%', 't', 'p_i', 't', 'satisfies', 'C']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent { \\bf Programs with Set Atoms in the Bodies of Rules }\n","\\begin{example} [ Set atoms in the rule body ]\n","Consider a knowledge base containing two complete lists of atoms :\n","\\begin{verbatim}\n","taken ( mike , cs1 ) . taken ( mike , cs2 ) . taken ( john , cs2 ) . required ( cs1 ) . required ( cs2 ) .\n","\\end{verbatim}\n","Set atoms allow for a natural definition of the new relation , $ready\\_to\\_graduate(S)$ , which holds if student $S$ has taken all the required classes from the second list :\n","NOUN PHRASES:\n"," ['Set Atoms', 'Bodies', 'Rules', 'example', '[ Set atoms', 'rule body ] Consider', 'knowledge base', 'containing', 'complete lists', 'atoms', 'verbatim', 'taken', 'cs1', 'taken', 'cs2', 'taken', 'john', 'cs2', 'required', 'cs1', 'required', 'cs2', 'verbatim', 'Set atoms', 'allow', 'natural definition', 'new relation', 'S', 'holds', 'S', 'has taken', 'required classes', 'second list']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\end{example}\n","The next example shows how the semantics deals with vicious circles .\n","\\begin{example}\n","[ Set atoms in the rule body ]\n","Consider a program\n","$P_4$\n","NOUN PHRASES:\n"," ['example', 'next example', 'shows', 'semantics deals', 'vicious circles', 'example', '[ Set atoms', 'rule body ] Consider', 'program', 'P_4']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ item No $\\bar{U}^n < \\bar{W}^n$ satisfies the first two conditions .\n","NOUN PHRASES:\n"," ['U', 'W', '^n', 'satisfies', 'conditions']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st\n","Our last example shows how subset introduction rule with equality can be used to represent synonyms :\n","\\begin{example} [ Synonyms ]\n","\\label{e26} %\n","Introducing\n","Suppose we have a set of cars represented by atoms formed by a predicate symbol $car$ , e.g. , $\\{car(a).\\ car(b).\\}$\n","The following rule \\begin{verbatim} carro = { X : car ( X ) } :\n","- spanish .\n","\\end{verbatim} allows to introduce a new name of this set for Spanish speaking people .\n","Clearly , $car$ and $carro$ are synonyms .\n","Hence , program $P_9 \\cup\n","\\{spanish.\\}$ has one answer set :\n","$\\{spanish, car(a), car(b), carro(a),carro(b)\\}$ .           \n","NOUN PHRASES:\n"," ['st', 'last example', 'shows', 'subset introduction rule', 'equality', 'represent', 'synonyms', 'example', 'e26', '%', 'Introducing', 'Suppose', 'have', 'set', 'cars', 'represented', 'atoms', 'formed', 'predicate symbol', 'car', 'e.g', 'car', 'b', 'verbatim', 'carro =', 'X', 'car', 'X', 'spanish', 'verbatim', 'allows', 'introduce', 'new name', 'set', 'Spanish speaking people', 'Clearly', 'car', 'Hence', 'program', 'has', 'answer set', 'car', 'car', 'b', 'carro', 'carro', 'b']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{example}  \\label{e1} %\n","[ Self - reference and the set notation ]\n","$P_0$ consisting of a rule :\n","\\begin{verbatim} p ( 1 ) :\n","- card { X : p ( X ) } ! = 1.           \n","NOUN PHRASES:\n"," ['example', 'e1', '% [ Self', 'reference', 'set notation ]', 'P_0', 'consisting', 'rule', 'verbatim', 'p', 'card', 'X', 'p', 'X']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent $P_1$ consisting of rules :\n","\\begin{verbatim}\n","p ( 1 ) : - p ( 0 ) .\n","p ( 0 ) :- p ( 1 ) .\n","p ( 1 ) :\n","- card { X : p ( X ) } ! = 1.           \n","NOUN PHRASES:\n"," ['P_1', 'consisting', 'rules', 'verbatim', 'p', 'p', 'p', 'p', 'p', 'card', 'X', 'p', 'X']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section we introduce alternative interpretation of VCP ( referred to as \\emph{weak VCP} ) and incorporate it in the semantics of a new logic programming language with set , called $\\mathcal{S}log^+$ .\n","The syntax of $\\mathcal{S}log^+$ coincides with that of $\\mathcal{A}log$ .\n","Its informal semantics is based on weak VCP .\n","By $C(T)$ we denote a set atom containing an occurrence of set term $T$ .\n","The { \\em instantiation } of $C(\\{X:p(X)\\})$ in a set $A$ of regular literals obtained from $C(\\{X:p(X)\\})$ by replacing $\\{X:p(X)\\}$ by $\\{t:p(t) \\in A\\}$ .\n","The weak VCP is : { \\em belief in p ( t ) ( i.e. inclusion of p ( t ) in an answer set $A$ ) must be established without reference to the instantiation of a set atom $C$ in $A$ unless the truth of this instantiation can be demonstrated without reference to $p(t)$ . }\n","NOUN PHRASES:\n"," ['section', 'introduce', 'alternative interpretation', 'VCP', 'referred', 'weak VCP', 'incorporate', 'semantics', 'new logic programming language', 'set', 'called', 'S', 'log^+', 'syntax', 'S', 'log^+', 'coincides', 'informal semantics', 'weak VCP', 'C', 'T', 'denote', 'set', 'containing', 'occurrence', 'set term', 'T', 'C', 'X', 'p', 'X', 'set', 'regular literals', 'obtained', 'C', 'X', 'p', 'X', 'replacing', 'X', 'p', 'X', 't', 'p', 't', 'weak VCP', 'p', 't', 'i.e', 'inclusion', 'p', 't', 'answer', 'set', 'A', 'reference', 'instantiation', 'set', 'C', 'truth', 'instantiation', 'reference', 'p', 't']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\end{definition}\n","Intuitively , the weak VCP says that set atom $C$ can be safely used to support the reasoner 's beliefs iff the existence of a minimal support of $C$ can be established without reference to those beliefs .\n","Precise definition of answer sets of $\\mathcal{S}log^+$ is obtained by replacing definition \\ref{reduct1} of set reduct of $\\mathcal{A}log$ by definition \\ref{vcp2} below and combining it with definition \\ref{reduct2} .\n","\\begin{definition}\n","[ Set - reduct of $\\mathcal{S}log^+$ ]\n","\\label{vcp2}\n","A \\emph{set reduct} of $\\mathcal{S}log^+$ program $\\Pi$ with respect to a set $A$ of ground regular literals is obtained from $\\Pi$ by \\begin{enumerate} \\ item\n","Removing rules containing set atoms which are \\emph{false} or \\emph{undefined} in            . \\ item\n","Replacing every remaining set atom $C$ in the body of the rule by the union of coordinates of one of its minimal supports .\n","\\end{enumerate}\n","Clearly such a reduct is a regular ASP program without sets .\n","$A$ is an \\emph{answer set} of a $\\mathcal{S}log^+$ program\n","$\\Pi$ if $A$ is an answer set of a weak set reduct of $\\Pi$ with respect to $A$ .\n","\\end{definition}\n","NOUN PHRASES:\n"," ['definition', 'weak VCP', 'says', 'set', 'C', 'safely used', 'support', 'reasoner', 'beliefs', 'iff', 'existence', 'minimal support', 'C', 'reference', 'beliefs', 'Precise definition', 'sets', 'S', 'replacing', 'reduct1', 'set reduct', 'vcp2', 'combining', 'reduct2', 'definition', '[ Set', 'reduct', 'S', 'log^+', 'vcp2', 'set', 'reduct', 'S', 'program', 'respect', 'set', 'ground regular literals', 'enumerate', 'Removing', 'rules', 'containing', 'set atoms', 'Replacing', 'remaining', 'set', 'C', 'body', 'rule', 'union', 'coordinates', 'minimal supports', 'enumerate', 'Clearly', 'reduct', 'regular ASP program', 'sets', 'answer', 'set', 'S', 'program', 'answer set', 'weak set reduct', 'respect', 'definition']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\st {\\tt p ( a ) :- p            \\{X : q ( X ) \\} . \\\\ q ( a ) . }\n","NOUN PHRASES:\n"," ['p', 'X', 'q', 'X']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ hide\n","{ \\begin{example}            %\n","[ Example \\ref{e1} revisited ]\n","{\\rm\n","To better understand the weak VCP let us consider programs in Example \\ref{e1} from the standpoint of $\\mathcal{S}log^+$ .\n","Inconsistency of $P_0$ is not caused by self - reference and the program remains inconsistent under the new semantics .\n","As before , there are two candidate answer sets of $P_1$ : $A_1=\\emptyset$ and $A_2=\\{p(0),p(1)\\}$ .\n","$A_1$ is ruled out by the satisfiability requirement .\n","$A_2$ was shown to violate the original version of VCP but one can see that it does not satisfy weak VCP either .\n","Beliefs in $p(0)$ and $p(1)$ depend on the existence of the set $S =\n","\\{t:p(t) \\in A_2\\} = \\{0,1\\}$ satisfying condition $card(S) \\not= 1$ .\n","The truth of this condition cannot be however established without reference to these beliefs .\n","So $P_1$ is still inconsistent .\n","For $P_2$ the situation changes .\n","Consider a candidate answer set $A=\\{p(1)\\}$ .\n","In this case $S = \\{t:p(t) \\in A\\} = \\{1\\}$ .\n","Truth of condition $card(S) \\geq 0$ does not depend on $p(1)$ .\n","Hence the set can be formed without violating the weak VCP and $\\{p(1)\\}$ is the only answer set of $P_2$ . } \\end{example} }\n","NOUN PHRASES:\n"," ['example', 'e1', 'revisited', 'understand', 'weak VCP', 'let', 'consider', 'programs', 'e1', 'standpoint', 'S', 'log^+', 'Inconsistency', 'P_0', 'not caused', 'reference', 'program', 'remains', 'new semantics', 'sets', 'P_1', 'p', 'p', 'A_1', 'satisfiability requirement', 'A_2', 'violate', 'original version', 'VCP', 'see', 'does', 'not satisfy', 'weak VCP', 'Beliefs', 'p', 'p', 'depend', 'existence', 'set', 't', 'p', 't', 'satisfying', 'condition', 'card', 'S', 'truth', 'condition', 'however established', 'reference', 'beliefs', 'P_1', 'P_2', 'situation changes', 'Consider', 'candidate answer', 'set', 'p', 'case', 't', 'p', 't', 'Truth', 'condition', 'card', 'S', 'does', 'not depend', 'p', 'Hence', 'set', 'violating', 'weak VCP', 'p', 'only answer set', 'P_2', 'example']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\hide{For program            , consider candidate answer set            .            is a minimal support for {\\tt card\\{X:p(X)\\} ! = 1 } in $A$ .\n","The set - redutt of $P_0$ wrt $A$ is            \\} for which $A$ is not the answer set .\n","So , $A$ is not an answer set of $P_0$ .\n","Similarly , there is no answer set for $P_1$ .\n","For $P_2$ , consider the candidate answer set $A = \\{p(1)\\}$ .\n","$\\{\\}$ is a minimal support for { \\tt card\\{X:p(X)\\} $\\ge$ 0 } in $A$ .\n","The set - reduct of $P_2$ wrt $A$ is \\{\\noindent {\\tt p(1)} \\} .\n","Hence , $A$ is an answer set of $P_2$ . }\n","NOUN PHRASES:\n"," ['program', 'consider', 'candidate answer set', 'minimal support', 'X', 'p', 'X', 'set', 'redutt', 'P_0', 'wrt', 'answer set', 'So', 'answer set', 'P_0', 'answer', 'set', 'P_1', 'P_2', 'consider', 'candidate answer', 'set', 'p', 'minimal support', 'X', 'p', 'X', 'set', 'reduct', 'P_2', 'wrt', 'Hence', 'answer set', 'P_2']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This paper is the continuation of work started in \\cite{GelfondZ14} with introduction of $\\mathcal{A}log$ -- a version of Answer Set Prolog ( ASP ) with aggregates .\n","The semantics of $\\mathcal{A}log$ combines the Rationality Principle of ASP \\cite{GelK14} with the adaptation of the Vicious Circle Principle ( VCP ) introduced by Poincare and Russel \\cite{poin1906,Russell} in their attempt to resolve paradoxes of set theory .\n","In $\\mathcal{A}log$ , the latter is used to deal with formation of sets and their legitimate use in program rules .\n","To understand the difficulty addressed by $\\mathcal{A}log$ consider the following programs :\n","NOUN PHRASES:\n"," ['paper', 'continuation', 'work', 'started', 'GelfondZ14', 'introduction', 'version', 'Answer Set Prolog', 'ASP', 'aggregates', 'semantics', 'combines', 'Rationality Principle', 'GelK14', 'adaptation', 'Vicious Circle Principle', 'VCP', 'introduced', 'Poincare', 'poin1906', 'Russell', 'attempt', 'resolve', 'paradoxes', 'set theory', 'latter', 'deal', 'formation', 'sets', 'legitimate use', 'program rules', 'understand', 'difficulty', 'addressed', 'consider', 'following programs']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ make title \\begin{abstract}\n","The paper continues the investigation of Poincare and Russel 's Vicious Circle Principle ( VCP ) in the context of the design of logic programming languages with sets .\n","We expand previously introduced language $\\mathcal{A}log$ with aggregates by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching .\n","In addition , we propose an alternative formalization of the original VCP and incorporate it into the semantics of new language , $\\mathcal{S}log^+$ , which allows more liberal construction of sets and their use in programming rules .\n","We show that , for programs without disjunction and infinite sets , the formal semantics of aggregates in $\\mathcal{S}log^+$ coincides with that of several other known languages .\n","Their intuitive and formal semantics , however , are based on quite different ideas and seem to be more involved than that of $\\mathcal{S}log^+$ .\n","NOUN PHRASES:\n"," ['make', 'abstract', 'paper', 'continues', 'investigation', 'Poincare', 'Russel', 'Vicious Circle Principle', 'VCP', 'context', 'design', 'logic programming languages', 'sets', 'expand', 'previously introduced', 'language', 'aggregates', 'allowing', 'infinite sets', 'several additional set', 'related', 'constructs', 'knowledge representation', 'teaching', 'addition', 'propose', 'alternative formalization', 'original VCP', 'incorporate', 'semantics', 'new language', 'S', 'log^+', 'allows', 'liberal construction', 'sets', 'use', 'programming rules', 'show', 'programs', 'disjunction', 'infinite sets', 'formal semantics', 'aggregates', 'S', 'log^+', 'coincides', 'several other known', 'languages', 'formal semantics', 'different ideas', 'seem', 'S', 'log^+']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{example}\n","\\label{e4} {\\rm\n","Consider now an $\\mathcal{S}log^+$ program $P_3$ \\begin{verbatim} p ( 3 ) :\n","- card { X : p ( X ) } >= 2. p ( 2 ) :\n","- card { X : p ( X ) } >= 2. p ( 1 ) .\n","\\end{verbatim}\n","It has two candidate answer sets : $A_1=\\{p(1)\\}$ and $A_2=\\{p(1),p(2),p(3)\\}$ .\n","In $A_1$ the corresponding condition is not satisfied and , hence , the weak set reduct of the program with respect to $A_1$ is $p(1).$\n","Consequently , $A_1$ is an answer set of $P_3$ .\n","In $A_2$ the condition has three minimal supports : $M_1 = \\{p(1),p(2)\\}$ , $M_2\n","= \\{p(1),p(3)\\}$ , and $M_3 = \\{p(2),p(3)\\}$ .\n","Hence , the program has nine weak set reducts of $P_3$ with respect to $A_2$ .\n","Each reduct is of the form \\begin{verbatim} p ( 3 ) :\n","- Mi. p ( 2 ) :\n","- Mj. p ( 1 ) .\n","\\end{verbatim} where $M_i$ and $M_j$ are minimal supports of the condition .\n","Clearly , the first two rules of such a reduct are useless and hence $A_2$ is not an answer set of this reduct .\n","Consequently $A_2$ is not an answer set of $P_3$ . }\n","\\end{example}\n","NOUN PHRASES:\n"," ['example', 'e4', 'S', 'program', 'P_3', 'verbatim', 'p', 'card', 'X', 'p', 'X', '>', 'p', 'card', 'X', 'p', 'X', '>', 'p', 'verbatim', 'has', 'candidate answer sets', 'p', 'p', 'p', 'p', 'A_1', 'corresponding condition', 'weak set reduct', 'program', 'respect', 'A_1', 'p', 'A_1', 'answer set', 'P_3', 'A_2', 'condition', 'has', 'minimal supports', 'p', 'p', 'p', 'p', 'p', 'p', 'Hence', 'program', 'has', 'weak set', 'reducts', 'P_3', 'respect', 'A_2', 'reduct', 'form', 'verbatim', 'p', 'Mi', 'p', 'Mj', 'p', 'verbatim', 'M_i', 'M_j', 'minimal supports', 'condition', 'rules', 'reduct', 'A_2', 'answer set', 'reduct', 'A_2', 'answer set', 'P_3', 'example']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent\n","First we assume $C$ be $card\\{X:p(X)\\} > 0$ .\n","There is only one candidate answer set $A = \\{p(0)\\}$ for this program .\n","Belief in $p(0)$ ( i.e. its membership in answer set $A$ ) can only be established by checking if instantiation $card\\{t : p(t) \\in A\\} > 0$ of $C$ in $A$ holds .\n","This is prohibited by weak VCP unless the truth of this instantiation can be demonstrated without reference to $p(0)$ .\n","But this cannot be so demonstrated because $card\\{t : p(t) \\in A\\} > 0$ holds only when $p(0)$ is in $A$ .\n","Hence , $A$ is not an answer set .\n","Now let $C$ be $card\\{X:p(X)\\} \\ge 0$ .\n","This time the truth of instantiation $card\\{t : p(t) \\in A\\} \\ge 0$ of $C$ can be demonstrated without reference to $p(0)$ -- the instantiation would be true even if $A$ were empty .\n","Hence $p(0)$ must be believed and thus the program has one answer set , $\\{p(0)\\}$ .\n","\\end{example}\n","NOUN PHRASES:\n"," ['noindent First', 'assume', 'C', 'X', 'p', 'X', 'candidate answer', 'set', 'p', 'program', 'Belief', 'p', 'i.e', 'membership', 'answer', 'set', 'A', 'checking', 't', 'p', 't', '>', 'C', 'holds', 'weak VCP', 'truth', 'instantiation', 'reference', 'p', 't', 'p', 't', '>', 'holds', 'p', 'Hence', 'answer set', 'Now let', 'C', 'X', 'p', 'X', 'time', 'truth', 'instantiation', 't', 'p', 't', 'C', 'reference', 'p', 'instantiation', 'Hence', 'p', 'program', 'has', 'answer set', 'p', 'example']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st has answer sets $A_1 = \\{q(a)\\}$ where the set $p$ is empty and $A_2 = \\{q(a),p(a)\\}$ where $p = \\{a\\}$ .           \n","The formal definition of answer sets of programs with set introduction rules is given via a notion of \\emph{set introduction reduct} .\n","( The definition is similar to that presented in \\cite{gel02} ) .\n","NOUN PHRASES:\n"," ['has', 'sets', 'q', 'set', 'q', 'p', 'formal definition', 'sets', 'programs', 'set', 'introduction rules', 'notion', 'set', 'introduction reduct', 'definition', 'presented', 'gel02']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As shown in \\cite{SonPT07} $\\mathcal{S}log$ has sufficient expressive power to formalize complex forms of recursion , including that used in the Company Control Problem \\cite{FaberPL11} .\n","Theorem \\ref{th2} guarantees that the same representations will work in $\\mathcal{S}log^+$ .\n","Of course , in many respects $\\mathcal{S}log^+$ substantially increases the expressive power of $\\mathcal{S}log$ .\n","Most importantly it expands the $\\mathcal{S}log$ semantics to programs with epistemic disjunction -- something which does not seem to be easy to do using the original definition of $\\mathcal{S}log$ answer sets .\n","Of course , new set constructs and rules with infinite number of literals are available in $\\mathcal{S}log^+$ but not in $\\mathcal{S}log$ .\n","On another hand , $\\mathcal{S}log$ allows multisets -- a feature we were not trying to include in our language .\n","The usefulness of multisets and the analysis of its cost in terms of growing complexity of the language due to its introduction is still under investigation .\n","NOUN PHRASES:\n"," ['shown', 'SonPT07', 'S', 'log', 'has', 'sufficient expressive power', 'formalize', 'complex forms', 'recursion', 'including', 'used', 'FaberPL11', 'th2', 'guarantees', 'same representations', 'work', 'S', 'log^+', 'course', 'many respects', 'S', 'log^+', 'substantially increases', 'expressive power', 'S', 'log', 'expands', 'S', 'log', 'semantics', 'programs', 'epistemic disjunction', 'something', 'does', 'not seem', 'do using', 'original definition', 'S', 'log', 'sets', 'course', 'new set', 'constructs', 'rules', 'infinite number', 'literals', 'S', 'log^+', 'S', 'log', 'hand', 'S', 'log', 'allows', 'multisets', 'feature', 'not trying', 'include', 'language', 'usefulness', 'multisets', 'analysis', 'cost', 'terms', 'growing', 'complexity', 'language', 'introduction', 'investigation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent $P_2$ consisting of rules :\n","NOUN PHRASES:\n"," ['P_2', 'consisting', 'rules']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{verbatim} p ( 1 ) :- card { X : p ( X ) } >= 0.           \n","Even for these seemingly simple programs , there are different opinions about their meaning .\n","To the best of our knowledge all ASP based semantics , including that of \\cite{FaberPL11,SonP07,GelfondZ14} ) view $P_0$ as a bad specification .\n","It is inconsistent , i.e. , has no answer sets .\n","Opinions differ , however , about the meaning of the other two programs .\n","\\cite{FaberPL11} views $P_1$ as a reasonable specification having one answer set -- $\\{p(0),p(1) \\}$ .\n","According to \\cite{SonP07,GelfondZ14} $P_1$ is inconsistent .\n","According to most semantics $P_2$ has one answer set , $\\{p(1)\\}$ .\n","$\\mathcal{A}log$ , however , views it as inconsistent .\n","\\end{example}\n","NOUN PHRASES:\n"," ['verbatim', 'p', 'card', 'X', 'p', 'X', '>', 'simple programs', 'different opinions', 'meaning', 'knowledge', 'ASP', 'based', 'semantics', 'including', 'FaberPL11', 'SonP07', 'GelfondZ14', 'view', 'P_0', 'bad specification', 'i.e', 'has', 'answer', 'sets', 'Opinions', 'differ', 'meaning', 'programs', 'FaberPL11', 'views', 'P_1', 'reasonable specification', 'having', 'answer set', 'p', 'p', 'According', 'SonP07', 'GelfondZ14', 'P_1', 'According', 'semantics', 'P_2', 'has', 'answer set', 'p', 'views', 'inconsistent', 'example']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n"," 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ medskip\n","Unfortunately , the additional power of $\\mathcal{S}log^+$ as compared with $\\mathcal{A}log$ comes at a price .\n","Part of it is a comparative complexity of the definition of $\\mathcal{S}log^+$ set reduct .\n","But , more importantly , the formalization of the weak VCP does not eliminate all the known paradoxes of reasoning with sets .\n","Consider , for instance the following example :\n","\\begin{example}\n","\\label{e5} { \\rm\n","Recall program $P_2$ :            p ( 1 ) :\n","- card { X : p ( X ) } >= 0.            from Example \\ref{e1} and assume , for simplicity , that parameters of $p$ are restricted to $\\{0,1\\}$ .\n","Viewed as a program of $\\mathcal{A}log$ , $P_2$ is inconsistent .\n","In $\\mathcal{S}log^+$ ( and hence in $\\mathcal{S}log$ and $\\mathcal{F}log$ ( the language defined in \\cite{FaberPL11} ) ) it has an answer set $\\{p(1)\\}$ .\n","The latter languages therefore admit existence of set $\\{X:p(X)\\}$ .\n","Now let us look at program $P_5$ : \\begin{verbatim} p ( 1 ) :\n","- card { X : p ( X ) } = Y , Y >=0.            and its grounding\n","$P_6$ : \\begin{verbatim} p ( 1 ) :\n","- card { X : p ( X ) } = 1, 1 >=0. p ( 1 ) :\n","- card { X : p ( X ) } = 0, 0 >=0.           \n","They seem to express the same thought as $P_2$ , and it is natural to expect all these programs to be equivalent .\n","It is indeed true in $\\mathcal{A}log$ -- none of the programs is consistent .\n","According to the semantics of $\\mathcal{S}log^+$ ( and $\\mathcal{S}log$ and $\\mathcal{F}log$ ) , however , $P_5$ and $P_6$ are inconsistent .\n","To see that notice that there are two candidate answer sets for $P_6$ : $A_1=\\emptyset$ and $A_2 = \\{p(1)\\}$ .\n","The minimal support of $card\\{X:p(X)\\}\n","= 0$ in $A_1$ is $\\emptyset$ and hence the only weak set reduct of $P_6$ with respect to $A_1$ is \\{{\\tt p(1) :- 0>=0} \\} .\n","$A_1$ is not an answer set of $P_6$ .\n","The minimal support of $card\\{X:p(X)\\}=1$ in $A_2$ is $\\{p(1)\\}$ .\n","The only weak set reduct is \\{ {\\tt p ( 1 ) : - p ( 1 ) , 1 >=0 } \\} .\n","$A_2$ is not an answer set of $P_6$ either .\n","It could be that this paradoxical behavior will be in the future explained from some basic principles but currently authors are not aware of such an explanation .\n","NOUN PHRASES:\n"," ['additional power', 'S', 'compared', 'comes', 'price', 'Part', 'comparative complexity', 'definition', 'S', 'log^+', 'set', 'reduct', 'formalization', 'weak VCP', 'does', 'not eliminate', 'known', 'paradoxes', 'reasoning', 'sets', 'Consider', 'instance', 'following example', 'example', 'e5', 'P_2', 'p', 'card', 'X', 'p', 'X', '>', 'e1', 'assume', 'simplicity', 'parameters', 'Viewed', 'program', 'P_2', 'S', 'log^+', 'S', 'log', 'F', 'log', 'language', 'defined', 'FaberPL11', 'has', 'answer', 'set', 'p', 'latter languages', 'therefore', 'admit existence', 'X', 'p', 'X', 'Now let', 'look', 'program', 'P_5', 'verbatim', 'p', 'card', 'X', 'p', 'X', '= Y', 'Y > =0', 'grounding', 'P_6', 'verbatim', 'p', 'card', 'X', 'p', 'X', '> =0', 'p', 'card', 'X', 'p', 'X', '> =0', 'seem', 'express', 'same thought', 'P_2', 'expect', 'programs', 'none', 'programs', 'According', 'semantics', 'S', 'log^+', 'S', 'log', 'F', 'log', 'P_5', 'P_6', 'see', 'notice', 'sets', 'P_6', 'p', 'minimal support', 'X', 'p', 'X', 'A_1', 'hence', 'only weak set', 'reduct', 'P_6', 'respect', 'A_1', 'p', '> =0', 'A_1', 'answer set', 'P_6', 'minimal support', 'X', 'p', 'X', 'A_2', 'p', 'only weak set', 'reduct', 'p', 'p', '> =0', 'A_2', 'answer set', 'P_6', 'paradoxical behavior', 'future', 'explained', 'basic principles', 'authors', 'explanation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{definition}\n","[ Set Introduction Reduct ] \\label{reduct2}\n","The \\emph{set introduction reduct} of a ground $\\mathcal{A}log$ program $\\Pi$ with respect to a set of ground regular literals $A$ is obtained from $\\Pi$ by            \\ item replacing every set introduction rule of $\\Pi$ whose head is not true in $A$ by            \\ item replacing every set introduction rule of $\\Pi$ whose head $p\n","\\subseteq \\{\\bar{X}:q(\\bar{X})\\}$ ( or $p = \\{\\bar{X}:q(\\bar{X})\\}$ or $\\{\\bar{X}:q(\\bar{X})\\} \\subseteq p$ ) is true in $A$ by\n","$$p(\\bar{t}) \\leftarrow body$$\n","for each $p(\\bar{t}) \\in A$ .\n","\\end{enumerate}\n","Set $A$ is an \\emph{answer set} of $\\Pi$ if it is an answer set of the set introduction reduct of $\\Pi$ with respect to $A$ .\n","\\end{definition}\n","\\begin{example}\n","[ Set introduction rule ] \\label{e25}\n","Consider a program $P_9$ from Example \\ref{e20} .\n","The reduct of this program with respect to $A_1 = \\{q(a)\\}$ is $\\{q(a).\\}$ and hence $A_1$ is an answer set of $P_9$ .\n","The reduct of $P_9$ with respect to $A_2 = \\{q(a),p(a)\\}$ is $\\{q(a). \\ p(a).\\}$ and hence $A_2$ is also an answer set of $P_9$ .\n","There are no other answer sets .\n","\\end{example}\n","The use of a set introduction rule $p \\subseteq S \\leftarrow body$ is very similar to that of choice rule $\\{p(\\bar{X}) : q(\\bar{X})\\} \\leftarrow body$ of \\cite{nss02} implemented in Clingo and other similar systems .\n","In fact , if $p$ from the set introduction rule does not occur in the head of any other rule of the program , the two rules have the same meaning .\n","However if this condition does not hold the meaning is different .\n","An $\\mathcal{A}log$ program consisting of rules $p \\subseteq \\{X:q_1(X)\\}$ and $p \\subseteq \\{X:q_2(X)\\}$ defines an arbitrary set $p$ from the intersection of $q_1$ and $q_2$ .\n","With choice rules it is not the case .\n","We prefer the set introduction rule because of its more intuitive reading ( after all everyone is familiar with the statement `` $p$ is an arbitrary subset of $q$ ' ' ) and relative simplicity of the definition of its formal semantics as compared with that of the choice rule .\n","NOUN PHRASES:\n"," ['definition', 'reduct2', 'set', 'introduction reduct', 'ground', 'program', 'respect', 'set', 'ground regular literals', 'replacing', 'set introduction rule', 'head', 'replacing', 'set introduction rule', 'X', 'q', 'X', 'X', 'q', 'X', 'q', 'X', 'p', 't', 'p', 't', 'enumerate', 'Set', 'answer', 'set', 'answer set', 'set introduction reduct', 'respect', 'definition', 'example', 'e25', 'Consider', 'program', 'P_9', 'e20', 'reduct', 'program', 'respect', 'q', 'q', 'A_1', 'answer set', 'P_9', 'reduct', 'P_9', 'respect', 'q', 'q', 'A_2', 'answer set', 'P_9', 'sets', 'example', 'use', 'set introduction rule', 'choice rule', 'p', 'X', 'q', 'X', 'implemented', 'Clingo', 'other similar systems', 'fact', 'p', 'set introduction rule', 'does', 'not occur', 'head', 'other rule', 'program', 'rules', 'have', 'same meaning', 'condition', 'does', 'not hold', 'meaning', 'program consisting', 'rules', 'p', 'X', 'q_1', 'X', 'X', 'q_2', 'X', 'defines', 'arbitrary set', 'intersection', 'choice rules', 'case', 'prefer', 'set introduction rule', 'intuitive reading', 'everyone', 'statement', 'arbitrary subset', 'relative simplicity', 'definition', 'formal semantics', 'compared', 'choice rule']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{theorem}  \\label{th2}\n","Let $\\Pi$\n","be a program which , syntactically , belongs to both $\\mathcal{S}log$ and $\\mathcal{S}log^+$ .\n","A set $A$ is an $\\mathcal{S}log$ answer set of $\\Pi$ iff it is an $\\mathcal{S}log^+$ answer set of            .\n","\\end{theorem}\n","NOUN PHRASES:\n"," ['theorem', 'th2', 'Let', 'program', 'belongs', 'S', 'log', 'S', 'log^+', 'A', 'set', 'S', 'log', 'set', 'iff', 'S', 'log^+', 'set', 'theorem']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\st {\\tt q ( a ) . \\\\ p            \\{X:q ( X ) \\}.}\n","NOUN PHRASES:\n"," ['X', 'q', 'X']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st \\emph { An expression $\\{X:p(X)\\}$ denotes a set $S$ only if for every $t$ rational belief in $p(t)$ can be established without a reference to $S$ } , or equivalently , \\ emph { the reasoner 's belief in $p(t)$ can not depend on existence of a set denoted by $\\{X:p(X)\\}$ } .\n","NOUN PHRASES:\n"," ['expression', 'X', 'p', 'X', 'denotes', 'S', 'rational belief', 'p', 't', 'reference', 'S', 'emph', 'reasoner', 'belief', 'p', 't', 'not depend', 'existence', 'set', 'denoted', 'X', 'p', 'X']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent\n","As in the naive set theory , the difficulty in interpretations seems to be caused by self - reference .\n","In both $P_1$ and $P_2$ , the definition of $p(1)$ references the set described in terms of $p$ .\n","It is , of course , not entirely clear how this type of differences can be resolved .\n","Sometimes , further analysis can find convincing arguments in favor of one of the proposals .\n","Sometimes , the analysis discovers that different approaches really model different language or world phenomena and are , hence , all useful in different contexts .\n","We believe that the difficulty can be greatly alleviated if the designers of the language provide its users with as clear intuitive meaning of the new constructs as possible .\n","Accordingly , the \\emph{set name} construct $\\{X:p(X)\\}$ of $\\mathcal{A}log$ denotes \\ emph { the set of all objects believed by the rational agent associated with the program to satisfy property $p$ } .\n","( This reading is in line with the epistemic view of ASP connectives shared by the authors . )\n","The difficulties with self - reference in $\\mathcal{A}log$ are resolved by putting the following intuitive restriction on the formation of sets\n","\\ footnote\n","{ It is again similar to set theory where the difficulty is normally avoided by restricting comprehension axioms guaranteeing existence of sets denoted by expressions of the form $\\{X:p(X)\\}$ .\n","In ASP such restrictions are encoded in the definition of answer sets . } :\n","NOUN PHRASES:\n"," ['naive set theory', 'difficulty', 'interpretations', 'seems', 'reference', 'P_1', 'P_2', 'definition', 'p', 'references', 'set', 'described', 'terms', 'course', 'type', 'differences', 'further analysis', 'find', 'convincing arguments', 'favor', 'proposals', 'analysis', 'discovers', 'different approaches', 'really model', 'different language', 'world phenomena', 'different contexts', 'believe', 'difficulty', 'greatly alleviated', 'designers', 'language', 'provide', 'users', 'clear intuitive meaning', 'new constructs', 'set', 'name', 'construct', 'X', 'p', 'X', 'set', 'objects', 'believed', 'rational agent', 'associated', 'program', 'satisfy', 'property', 'reading', 'line', 'epistemic view', 'ASP connectives', 'shared', 'authors', 'difficulties', 'reference', 'putting', 'following intuitive restriction', 'formation', 'footnote', 'set', 'theory', 'difficulty', 'normally avoided', 'restricting', 'comprehension axioms', 'guaranteeing', 'existence', 'sets', 'denoted', 'expressions', 'X', 'p', 'X', 'ASP such restrictions', 'definition', 'sets']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st in which definition of $p(a)$ depends on the existence of the set denoted by $\\{X: p(X)\\}$ .\n","In accordance with the vicious circle principle no answer set of this program can contain $p(a)$ .\n","There are only two candidates for answer sets of $P_4$ : $S_1 = \\{q(a)\\}$ and $S_2 =\n","\\{q(a),p(a)\\}$ .\n","The set atom reduct of $P_4$ with respect to $S_1$ is \\begin{verbatim} p ( a ) : - q ( a ) .\n","q ( a ) .\n","\\end{verbatim} while set atom reduct of $P_4$ with respect to $S_2$ is \\begin{verbatim} p ( a ) : - p ( a ) , q ( a ) . q ( a ) .\n","\\end{verbatim}\n","Clearly , neither $S_1$ nor $S_2$ is an answer set of $P_4$ .\n","As expected , the program is inconsistent .\n","\\end{example}\n","\\subsubsection{Programs with Set Introduction Rules.}\n","A set introduction rule with head $p \\subseteq S$ ( where $p$ is a predicate symbol and $S$ is a set name ) defines set $p$ as an arbitrary subset of $S$ ; rule with head $p = S$ simply gives $S$ a different name ; $S \\subseteq p$ defines $p$ as an arbitrary superset of $S$ .\n","\\begin{example}\n","[ Set introduction rule ]\n","\\label{e20}\n","According to this intuitive reading the program $P_9$ :\n","NOUN PHRASES:\n"," ['st', 'definition', 'p', 'depends', 'existence', 'set', 'denoted', 'X', 'p', 'X', 'accordance', 'vicious circle principle', 'answer set', 'program', 'contain', 'p', 'candidates', 'sets', 'P_4', 'q', 'q', 'set atom reduct', 'P_4', 'respect', 'S_1', 'verbatim', 'p', 'q', 'q', 'verbatim', 'set', 'atom reduct', 'P_4', 'respect', 'S_2', 'verbatim', 'p', 'p', 'q', 'verbatim', 'Clearly', 'S_1', 'S_2', 'answer set', 'P_4', 'expected', 'program', 'example', 'Programs', 'Set Introduction Rules', 'set introduction rule', 'head', 'predicate symbol', 'S', 'set name', 'defines set', 'arbitrary subset', 'S', 'rule', 'head', 'p = S', 'simply gives', 'S', 'different name', 'p', 'defines', 'arbitrary superset', 'S', 'example', 'e20', 'According', 'intuitive reading', 'program', 'P_9']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ st\n","The following two results help to better understand the semantics of $\\mathcal{S}log^+$ .\n","\\begin{theorem}\n","\\label{th1}\n","If a set $A$ is an $\\mathcal{A}log$ answer set of $\\Pi$ then $A$ is an $\\mathcal{S}log^+$ answer set of $\\Pi$ .\n","\\end{theorem}\n","As an $\\mathcal{S}log^+$ program , $P_2$ has an answer set of $\\{p(1)\\}$ , but it has no answer set as an $\\mathcal{A}log$ program .\n","The following result shows that there are many such programs and justifies our name for the new language .\n","NOUN PHRASES:\n"," ['st', 'results', 'help', 'better understand', 'semantics', 'S', 'log^+', 'theorem', 'th1', 'set', 'set', 'S', 'log^+', 'set', 'theorem', 'S', 'program', 'P_2', 'has', 'answer set', 'p', 'has', 'answer', 'set', 'program', 'following result', 'shows', 'many such programs', 'justifies', 'name', 'new language']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Let $R_{I}(\\Pi, A)$ , $R_{AS}(\\Pi, A)$ and $R_{SS}(\\Pi, A)$ to denote the set introduction reduct , set reduct and Slog + set reduct of $\\Pi$ wrt $A$ respectively .\n","NOUN PHRASES:\n"," ['Let', 'R_', 'A', 'R_', 'A', 'R_', 'SS', 'A', 'denote', 'set introduction reduct', 'set', 'reduct', 'Slog +', 'set', 'reduct']\n","NOUN LOCATIONS:\n"," [1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ item Proved a number of basic properties of programs of $\\mathcal{A}log$ and $\\mathcal{S}log^+$ .\n","\\end{itemize}\n","NOUN PHRASES:\n"," ['Proved', 'number', 'basic properties', 'programs', 'S', 'log^+', 'itemize']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Therefore $A$ is an answer set of $R_{SS}(R_I(\\Pi, A), A)^A$ , i.e. , $A$ is an Slog + answer set of            . \\ hfill $\\Box$\n","NOUN PHRASES:\n"," ['answer set', 'R_', 'SS', 'R_I', 'A', 'A', '^A', 'i.e', 'Slog + answer set']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The paper belongs to the series of works aimed at the development of an answer set based knowledge representation language .\n","Even though we want to have a language suitable for serious applications our main emphasis is on teaching .\n","This puts additional premium on clarity and simplicity of the language design .\n","In particular we believe that the constructs of the language should have a simple syntax and a clear intuitive semantics based on understandable informal principles .\n","In our earlier paper \\cite{GelfondZ14} we concentrated on a language $\\mathcal{A}log$ expanding standard Answer Set Prolog by aggregates .\n","We argued that the syntax of the language is simpler than that of the most popular aggregate language $\\mathcal{F}log$ implemented in Clingo and other similar systems .\n","In particular , $\\mathcal{A}log$ 's notion of grounding \\ emph { allows to define the intuitive ( and formal ) meaning of a set name independently from its occurrence in a rule } .\n","As the result , set name $\\{X : p(X)\\}$ can be always equivalently replaced by $\\{Y : p(Y)\\}$ .\n","In $\\mathcal{F}log$ , it is not the case .\n","A semantics of aggregates in $\\mathcal{A}log$ was based on a particularly simple and restrictive formalization of VCP .\n","In this paper we :            \\item\n","Expanded syntax and semantics of the original $\\mathcal{A}log$ by allowing            \\ item rules with an infinite number of literals -- a feature of theoretical interest also useful for defining aggregates on infinite sets ; \\ item subset relation between sets in the bodies of rules concisely expressing a specific form of universal quantification ; \\ item set introduction -- a feature with functionality somewhat similar to that of the choice rule of clingo but with different intuitive semantics . \\end{itemize}\n","Our additional set constructs are aimed at showing that our original languages can be expanded in a natural and technically simple ways .\n","Other constructs such as set operations and rules with variables ranging over sets ( in the style of \\cite{DovierPR03} ) , etc. are not discussed .\n","Partly this is due to space limitations -- we do not want to introduce any new constructs without convincing examples of their use .\n","The future will show if such extensions are justified .\n","NOUN PHRASES:\n"," ['paper', 'belongs', 'series', 'works', 'aimed', 'development', 'answer', 'set based', 'knowledge representation language', 'want', 'have', 'language', 'serious applications', 'main emphasis', 'teaching', 'puts', 'additional premium', 'clarity', 'simplicity', 'language design', 'believe', 'constructs', 'language', 'have', 'simple syntax', 'clear intuitive semantics', 'based', 'understandable informal principles', 'GelfondZ14', 'concentrated', 'language', 'expanding', 'standard Answer Set Prolog', 'aggregates', 'argued', 'syntax', 'language', 'popular aggregate language', 'F', 'log', 'implemented', 'Clingo', 'other similar systems', 'notion', 'grounding', 'allows', 'define', 'intuitive', 'meaning', 'set name', 'occurrence', 'rule', 'result', 'set', 'X', 'p', 'X', 'always equivalently replaced', 'Y', 'p', 'Y', 'F', 'log', 'case', 'semantics', 'aggregates', 'restrictive formalization', 'VCP', 'paper', 'Expanded', 'syntax', 'semantics', 'allowing', 'infinite number', 'literals', 'feature', 'theoretical interest', 'defining', 'aggregates', 'infinite sets', 'subset', 'relation', 'sets', 'bodies', 'rules', 'concisely expressing', 'specific form', 'universal quantification', 'set', 'introduction', 'feature', 'functionality', 'choice rule', 'clingo', 'different intuitive semantics', 'itemize', 'additional set constructs', 'showing', 'original languages', 'simple ways', 'Other constructs', 'set', 'operations', 'rules', 'variables', 'ranging', 'sets', 'style', 'DovierPR03', 'not discussed', 'space limitations', 'do', 'not want', 'introduce', 'new constructs', 'convincing', 'examples', 'use', 'future', 'show', 'such extensions']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Since $A$ is an answer set of $R_{AS}(R_I(\\Pi, A), A)$ , $A$ is a minimal model of $R_{AS}(R_I(\\Pi, A), A)^A$ .\n","We will show $A$ is a model of $R_{SS}(R_I(\\Pi, A), A)^A$ .\n","Consider any $r$ of $R_{SS}(R_I(\\Pi, A), A)^A$ such that $body(r)$ is satisfied by $A$ .\n","There is $r' \\in R_{AS}(R_I(\\Pi, A), A)$ such that both $r'$ and $r$ are obtained from the same rule in $R_I(\\Pi, A)$ during ( Slog + ) set reduct .\n","By definition of set reduct , $body(r) \\subseteq body(r')$ and all atoms of $body(r') - body(r)$ are from $A$ .\n","Hence , $A$ satisfies $body(r)$ implies that $A$ satisfies $body(r')$ .\n","Therefore $A$ satisfies the head of $r'$ because $A$ satisfies $r'$ .\n","Since both $r$ and $r'$ have the same head , $A$ satisfies the head of $r$ .\n","Hence $A$ is a model of $R_{SS}(R_I(\\Pi, A), A)^A$ .\n","NOUN PHRASES:\n"," ['answer set', 'R_', 'R_I', 'A', 'A', 'minimal model', 'R_', 'R_I', 'A', 'A', '^A', 'show', 'model', 'R_', 'SS', 'R_I', 'A', 'A', '^A', 'Consider', 'R_', 'SS', 'R_I', 'A', 'A', 'body', 'r', 'r', 'R_I', 'A', 'A', 'r', 'r', 'same rule', 'R_I', 'A', 'Slog +', 'set', 'reduct', 'definition', 'set reduct', 'body', 'r', 'body', 'r', 'atoms', 'body', 'r', 'body', 'r', 'Hence', 'satisfies', 'body', 'r', 'implies', 'satisfies', 'body', 'r', 'Therefore', 'A', 'satisfies', 'head', 'r', 'satisfies', 'r', 'r', 'r', 'have', 'same head', 'satisfies', 'head', 'Hence', 'model', 'R_', 'SS', 'R_I', 'A', 'A', '^A']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n"," 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n"," 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We next show $A$ is minimal by contradiction .\n","Assume there exists $B \\subset A$ such that $B$ is a model of $R_{SS}(R_I(\\Pi, A), A)^A$ .\n","By set reduct definition , for every $r' \\in R_{AS}(R_I(\\Pi, A), A)^A$ , there is a rule $r \\in R_{SS}(R_I(\\Pi, A), A)^A$ such that they are obtained from the same rule of $R_I(\\Pi, A)$ .\n","Hence they have the same head and $body(r') \\supseteq body(r)$ .\n","Therefore , $B$ is also a model of $R_{AS}(R_I(\\Pi, A), A)^A$ contradicting that $B \\subset A$ and $A$ is a minimal model of $R_{AS}(R_I(\\Pi, A), A)^A$ .\n","NOUN PHRASES:\n"," ['next show', 'contradiction', 'Assume', 'exists', 'B', 'model', 'R_', 'SS', 'R_I', 'A', 'A', '^A', 'set reduct definition', 'r', 'R_I', 'A', 'A', '^A', 'rule', 'SS', 'R_I', 'A', 'A', 'same rule', 'R_I', 'A', 'Hence', 'have', 'same head', 'body', 'r', 'body', 'r', 'Therefore', 'B', 'model', 'R_', 'R_I', 'A', 'A', '^A', 'contradicting', 'minimal model', 'R_', 'R_I', 'A', 'A', '^A']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ item\n","Introduced a new KR language , $\\mathcal{S}log^+$ , with the same syntax as $\\mathcal{A}log$ but different semantics for the set related constructs .\n","The new language is less restrictive and allows formation of substantially larger collection of sets .\n","Its semantics is based on the alternative , weaker formalization of VCP .\n","\\item\n","Proved that ( with the exception of multisets ) $\\mathcal{S}log^+$ is an extension of a well known aggregate language $\\mathcal{S}log$ .\n","The semantics of the new language is based on the intuitive idea quite different from that of $\\mathcal{S}log$ and the definition of its semantics is simpler .\n","We point out some paradoxes of $\\mathcal{S}log^+$ ( and $\\mathcal{F}log$ ) which prevent us from advocating them as standard ASP language with aggregates .\n","NOUN PHRASES:\n"," ['Introduced', 'new KR language', 'S', 'log^+', 'same syntax', 'different semantics', 'set related constructs', 'new language', 'allows', 'formation', 'collection', 'sets', 'semantics', 'alternative', 'formalization', 'VCP', 'Proved', 'exception', 'multisets', 'S', 'extension', 'well', 'known', 'aggregate language', 'S', 'log', 'semantics', 'new language', 'intuitive idea', 'S', 'log', 'definition', 'semantics', 'point', 'paradoxes', 'S', 'log^+', 'F', 'log', 'prevent', 'advocating', 'standard ASP language', 'aggregates']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{definition}\n","[ Occurrences of Regular Literals in Aggregate Atoms ]\n","\\label{occur}\n","We say that a ground literal $l$ \\emph{occurs in a set atom} $C$ if there is a set name $\\{X:cond(X)\\}$ occurring in $C$ and $l$ is a ground instance of some literal in $cond$ .\n","If $B$ is a set of ground literals possibly preceded by default negation $not$ then $l$ occurs in $B$ if $l \\in B$ , or $not\\ l \\in B$ , or $l$ occurs in some set atom from $B$ .\n","\\end{definition}  \\begin{definition}\n","[ Splitting Set ]\n","\\label{split-set}\n","Let $\\Pi$ be a program with signature $\\Sigma$ .\n","A set $S$ of ground regular literals of $\\Sigma$ is called a \\emph{splitting set} of $\\Pi$ if , for every rule $r$ of $\\Pi$ , if $l$ occurs in the head of $r$ then every literal occurring in the body of $r$ belongs to $S$ .\n","The set of rules of $\\Pi$ constructed from literals of $S$ is called \\emph{the bottom} of $\\Pi$ relative to $S$ ; the remaining rules are referred to as \\emph{the top} of $\\Pi$ relative to $S$ .\n","\\end{definition}\n","Note that the definition implies that no literal occurring in the bottom of $\\Pi$ relative to $S$ can occur in the heads of rules from the top of $\\Pi$ relative to $S$ .\n","NOUN PHRASES:\n"," ['definition', '[ Occurrences', 'Regular Literals', 'say', 'ground', 'occurs', 'set atom', 'C', 'set name', 'X', 'cond', 'X', 'occurring', 'C', 'ground instance', 'B', 'set', 'ground literals', 'possibly preceded', 'default negation', 'occurs', 'B', 'occurs', 'set atom', 'B', 'definition', 'definition', 'Let', 'program', 'A', 'set', 'S', 'ground regular literals', 'splitting set', 'rule', 'r', 'l', 'occurs', 'head', 'literal occurring', 'body', 'belongs', 'S', 'set', 'rules', 'constructed', 'literals', 'S', 'bottom', 'S', 'remaining', 'rules', 'S', 'definition', 'Note', 'definition', 'implies', 'literal occurring', 'bottom', 'S', 'occur', 'heads', 'rules', 'top', 'S']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition} [ Anti-chain Property ]\n","\\label{p2aa}\n","If $\\Pi$ is a program without set atoms in the heads of its rules then there are no $\\mathcal{A}log$ answer sets $A_1$ , $A_2$ of $\\Pi$ such that $A_1 \\subset A_2$ .\n","Similarly for its $\\mathcal{S}log^+$ answer sets .\n","NOUN PHRASES:\n"," ['proposition', '[', 'p2aa', 'program', 'set atoms', 'heads', 'rules', 'sets', 'A_1', 'A_2', 'Similarly', 'S', 'log^+', 'sets']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{proposition}\n","[ Splitting Set Theorem ]\n","\\label{split}\n","Let $\\Pi$ be a ground program , $S$ be its splitting set , and $\\Pi_1$ and $\\Pi_2$ be the bottom and the top of $\\Pi$ relative to $S$ respectively .\n","Then a set $A$ is an answer set of $\\Pi$ iff $A \\cap S$ is an answer set of $\\Pi_1$ and $A$ is an answer set of $(A \\cap S) \\cup \\Pi_2$ .\n","\\end{proposition}\n","Note that this formulation differs from the original one in two respects .\n","First , rules of the program can be infinite .\n","Second , the definition of occurrence of a regular literal in a rule changes to accommodate the presence of set atoms .\n","NOUN PHRASES:\n"," ['proposition', 'split', 'Let', 'ground program', 'S', 'splitting', 'set', 'bottom', 'top', 'S', 'set', 'answer set', 'answer set', 'answer set', 'proposition', 'Note', 'formulation differs', 'respects', 'First', 'rules', 'program', 'Second', 'definition', 'occurrence', 'regular literal', 'rule changes', 'accommodate', 'presence', 'set atoms']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," There are multiple approaches to introducing aggregates in logic programming languages under the answer sets semantics \\cite{KempS91,gel02,nss02,Marek04,Marek2004set,Pelov04,PelovDB04,PelovT04,Ferraris05,FerrarisL05,pdb07,SonP07,LeeLP08,ShenYY09,LiuPST10,FaberPL11,PontelliST11,liu2011strong,WangLZY12,HarrisonLY14,shen2014flp,GelfondZ14,gebser2015abstract,alviano2015complexity} .\n","In addition to this work our paper was significantly influenced by the original work on VCP in set theory and principles of language design advocated by Dijkstra , Hoare , Wirth and others .\n","Harrison et al 's work \\cite{HarrisonLY14} explaining the semantics of some constructs of gringo in terms of infinitary formulas of Truszczynski \\cite{truszczynski2012connecting} led to their inclusion in $\\mathcal{A}log$ and $\\mathcal{S}log^+$ .\n","The notion of set reduct of $\\mathcal{A}log$ was influenced by the reduct introduced for defining the semantics of Epistemic Specification in \\cite{Gelfond2011new} .\n","Recent work by Alviano and Faber \\cite{alviano2015stable} helped us to realize the close relationship between $\\mathcal{A}log$ and $\\mathcal{S}log$ and Argumentation theory \\cite{dung1995acceptability,brewka2013abstract,strass2013approximating} which certainly deserves further investigation , as well as provided us with additional knowledge about $\\mathcal{A}log$ .\n","More information about $\\mathcal{S}log$ and $\\mathcal{S}log^+$ can be found in Section 3 .\n","\\\n","hide\n","{\n","We start with a short discussion of our semantics for ASP with infinite rules .\n","The idea of expending the syntax of ASP to allow infinitary formulas is not new .\n","Stable model semantics for such formulas was first introduced in \\cite{Truszczynski12} .\n","It is based on the notion of stable models for finite propositional formulas from \\cite{Ferraris05,Ferraris11} .\n","Proposition 28 from \\cite{fl05} shows that , for finite ASP rules , the latter definition coincides with the original definition of answer sets .\n","The proof of this proposition can be easily adopted to the infinite case to show that a semantics of ASP program with infinite rules coincides with that from \\cite{Truszczynski12} . }\n","Shen et al. \\cite{ShenYY09} and Liu et al. \\cite{liu2011strong} propose equivalent semantics for disjunctive constraint programs ( i.e. , programs with rules whose bodies are built from constraint atoms and whose heads are epistemic disjunctions of such atoms ) .\n","This generalizes the standard ASP semantics for disjunctive programs .\n","We conjecture that when we adapt our definition of $\\mathcal{S}log^+$ semantics to disjunctive constraint programs , it will coincide with that of \\cite{ShenYY09,liu2011strong} .\n","However , our definition seems to be simpler and is based on clear , VCP related intuition .\n","NOUN PHRASES:\n"," ['multiple approaches', 'introducing', 'aggregates', 'logic programming languages', 'answer', 'sets', 'semantics', 'KempS91', 'gel02', 'Marek04', 'Marek2004set', 'Pelov04', 'PelovDB04', 'PelovT04', 'Ferraris05', 'FerrarisL05', 'pdb07', 'SonP07', 'LeeLP08', 'ShenYY09', 'LiuPST10', 'FaberPL11', 'PontelliST11', 'WangLZY12', 'HarrisonLY14', 'shen2014flp', 'GelfondZ14', 'gebser2015abstract', 'alviano2015complexity', 'addition', 'work', 'paper', 'significantly influenced', 'original work', 'VCP', 'set theory', 'principles', 'language design', 'advocated', 'Dijkstra', 'Hoare', 'Wirth', 'others', 'Harrison', 'et', 'al', 'work', 'HarrisonLY14', 'explaining', 'semantics', 'constructs', 'gringo', 'terms', 'infinitary formulas', 'truszczynski2012connecting', 'led', 'inclusion', 'S', 'log^+', 'notion', 'set reduct', 'reduct', 'introduced', 'defining', 'semantics', 'Epistemic Specification', 'Gelfond2011new', 'Recent work', 'Alviano', 'helped', 'realize', 'close relationship', 'S', 'log', 'dung1995acceptability', 'brewka2013abstract', 'strass2013approximating', 'certainly deserves', 'further investigation', 'provided', 'additional knowledge', 'information', 'S', 'log', 'S', 'log^+', 'Section', 'start', 'short discussion', 'semantics', 'ASP', 'infinite rules', 'idea', 'expending', 'syntax', 'ASP', 'allow', 'infinitary formulas', 'Stable model semantics', 'such formulas', 'first introduced', 'Truszczynski12', 'notion', 'stable models', 'finite propositional formulas', 'Ferraris05', 'Ferraris11', 'Proposition', 'fl05', 'shows', 'finite ASP rules', 'latter definition coincides', 'original definition', 'sets', 'proof', 'proposition', 'easily adopted', 'infinite case', 'show', 'semantics', 'ASP program', 'infinite rules coincides', 'Truszczynski12', 'Shen', 'al', 'ShenYY09', 'Liu', 'et', 'al', 'propose equivalent semantics', 'disjunctive constraint programs', 'i.e', 'programs', 'rules', 'bodies', 'constraint atoms', 'heads', 'epistemic disjunctions', 'such atoms', 'generalizes', 'standard ASP semantics', 'disjunctive programs', 'conjecture', 'adapt', 'definition', 'S', 'log^+', 'semantics', 'disjunctive', 'constraint programs', 'coincide', 'ShenYY09', 'definition', 'seems', 'VCP', 'related', 'intuition']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{Hierarchical Q-value iteration (HQI)}  \\begin{algorithmic}  \\label{alg:hqi} \\ REQUIRE $O$ , $D$ \\\n","STATE $Train \\leftarrow O_i \\in O$ with only primitive children \\ STATE $Done \\leftarrow \\{A\\}$ \\WHILE{            }  \\FOR{            }  \\STATE{SQI            }  \\STATE{            } \\ ENDFOR \\\n","STATE $Train \\leftarrow O_i \\in (O-Done)$ AND $U_i \\in Done$ \\ ENDWHILE \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'Hierarchical Q-value iteration', 'HQI', 'alg', 'hqi', 'O', 'D', 'primitive children', 'STATE', 'SQI', 'O-Done', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The HQI algorithm is summarized in Algorithm ~ \\ref{alg:hqi} and SQI is summarized in Algorithm ~ \\ref{alg:sqi} .\n","Any dataset $D$ can be used at every iteration of SQI .\n","If the initial data is sufficient to cover important state - action space , the same dataset is able to train all subtasks of the \\textit{DAG} .\n","NOUN PHRASES:\n"," ['HQI algorithm', 'alg', 'hqi', 'SQI', 'alg', 'sqi', 'D', 'iteration', 'SQI', 'initial data', 'cover', 'important state', 'action space', 'same dataset', 'train', 'subtasks', 'DAG']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{Subtask Q-value Iteration (SQI)}  \\begin{algorithmic}            \\\n","REQUIRE $O_i,D$ \\WHILE{            }  \\FOR{            }  \\FOR{            }            \\\n","STATE            \\\n","STATE            \\ELSE            \\\n","STATE            \\\n","STATE            \\\n","STATE            \\ ENDIF \\ ENDI F \\ END FOR \\ END FOR \\ END WHILE \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'Subtask Q-value Iteration', 'SQI', 'O_i', 'D', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Therefore , instead of using the above Bellman equation that updates the Q table of the parent when a child exits , we use the intra-option Bellman equation proposed in the $option$ framework ~ \\cite{sutton1999between} : \\begin{align}\n","\\label{eq:optionq}\n","Q_i ( s , u )\n","&= \\sum_{a\\in A}\n","{ \\ pi_u ( a | s ) E [ r ( s , a ) + \\gamma V_i ( s' , u ) ] } \\\\ &= \\sum_{a\\in A}\n","{ \\pi_u(a|s)\\bigg [r(s,a) + \\gamma \\sum_{s'} P ( s' | s , a ) V_ i ( s' , u ) \\ bigg ] }\n","\\end{align} Where\n","\\begin{equation}\n","V_i(s,u) = (1-\\beta_i(s))Q_i(s, u) + \\beta_i(s)\\max_{u'\\in U_i} Q_i(s, u')\n","\\end{equation}\n","NOUN PHRASES:\n"," ['using', 'above Bellman equation', 'updates', 'Q table', 'parent', 'child exits', 'use', 'intra-option Bellman equation', 'proposed', 'option', 'align', 'eq', 'optionq', 'Q_i', '| s', 'E [ r', 'V_i', 's', ']', 'a|s', '[ r', 's', 'P', '| s', 'V_ i', 's', 'bigg ]', 'align', 'equation', 'V_i', '=', 'Q_i', 'Q_i', 'equation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Equation \\eqref{eq:optionq} also yeilds a contraction in the max norm and is able to learn the Q table after observing every new reward , which eliminates the need to estimate $P_i^{\\pi}(s', N | s, u)$ .\n","Another key benefit is that we can use flat samples to estimate the one step transition probability and rewards in equation \\eqref{eq:optionq} , which makes the algorithm independent of the hierarchical decomposition and is able to learn optimal polices for different structures from the same dataset .\n","Specifically , we can estimate the above two terms by $\\sum_{s'}P(s'|s,a)V(i, s',u) \\approx \\frac{1}{c}\\sum_{m=1}^{c}V(i, s'_{m}=s', u)$ and $r(s,a) \\approx \\frac{1}{c}\\sum_{m=1}^{c}r(s_m=s, a_m=a)$ , where $c$ is the number of experiences that has $s'$ and ( $s$ , $a$ ) , respectively .\n","At last , since we assume converged subtasks follow deterministic greedy policy , $\\pi_u(a|s) = 1$ if $a$ is the greedy primitive action that subtask $u$ would take at state $s$ , and $\\pi_u(a|s) = 0$ otherwise .\n","This step is in fact crucial for HQI to learn the optimal policy because it allows a subtask to discard those samples that are not following the optimal behavior of its children .\n","NOUN PHRASES:\n"," ['eq', 'optionq', 'also yeilds', 'contraction', 'max norm', 'learn', 'Q table', 'observing', 'new reward', 'eliminates', 'need', 'estimate', 'P_i^', 'N | s', 'u', 'key benefit', 'use', 'flat samples', 'estimate', 'step transition probability', 'rewards', 'eq', 'optionq', 'makes', 'hierarchical decomposition', 'learn', 'optimal polices', 'different structures', 'same dataset', 'estimate', 'terms', 's', 'P', \"s'|s\", 'V', 'i', 's', 'm=1', '^', 'c', 'V', 'i', 'm', '=s', 'u', 'r', 'm=1', '^', 'c', 'r', 's_m=s', 'a_m=a', 'c', 'number', 'experiences', 'has', 's', 'assume', 'converged subtasks', 'follow', 'deterministic greedy policy', 'a|s', '=', 'greedy primitive action', 'subtask', 'take', 'state', 'a|s', '=', 'step', 'fact', 'HQI', 'learn', 'optimal policy', 'allows', 'subtask', 'discard', 'samples', 'not following', 'optimal behavior', 'children']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{Fitted Subtask Q-value Iteration (Fitted SQI)}  \\begin{algorithmic}  \\label{alg:fsqi} \\\n","REQUIRE $O_i,D$ \\WHILE{            }  \\STATE{            ,            }  \\FOR{            }  \\FOR{            }  \\IF{            } \\\n","STATE $y \\leftarrow r$ \\ELSE \\IF{GreedyPolicy(            ,            )            } \\\n","STATE $y \\leftarrow r + \\gamma((1-\\beta_u(s')) Q_i^{k-1}(s', u)$ \\\n","STATE $\\quad \\quad \\quad + \\beta_u(s')max_{u'\\in U_i}Q_i^{k-1}(s', u'))$ \\ ENDIF \\ ENDIF \\STATE{            ,            } \\ ENDFOR \\ ENDFOR \\STATE{            } \\ ENDWHILE \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'Fitted Subtask Q-value Iteration', 'Fitted SQI', 'alg', 'fsqi', 'O_i', 'D', 'STATE', 'r', 'GreedyPolicy', 'STATE', 's', 'Q_i^', 'Q_i^', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{enumerate} \\ item\n","We first prove that : For a subtask $O_i$ , with all of its children converged to their recursive optimal policies and infinity amount of batch data , algorithm SQI converge to the optimal Q - value function after infinity number of iterations , $\\lim_{k->\\infty}Q^k_i=Q^*_i$\n","NOUN PHRASES:\n"," ['enumerate', 'first prove', 'O_i', 'children', 'converged', 'recursive optimal policies', 'infinity amount', 'batch data', 'algorithm SQI converge', 'optimal Q', 'value function', 'infinity number', 'iterations', 'Q^k_i=Q^*_i']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We want to prove that for an MDP $M=(S,A,P,R,P_0, \\gamma)$ with hierarchical decomposition $O=\\{O_0,..O_n\\}$ , HQI converges to recursive optimal policy for the hierarchical policy of $M$ , $\\pi^*_r$ .\n","NOUN PHRASES:\n"," ['want', 'prove', 'MDP', 'M=', 'S', 'A', 'P', 'R', 'P_0', 'hierarchical decomposition', 'O_0', 'HQI', 'converges', 'recursive', 'optimal policy', 'hierarchical policy', 'M']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{algorithm}  \\caption{GreedyPolicy}            \\\n","REQUIRE $u, s$ \\IF{            } \\ RETURN u \\ELSE \\ STATE            \\\n","RETURN Greedy Policy ( $u^*$ , $s$ ) \\ENDIF \\end{algorithmic}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['algorithm', 'GreedyPolicy', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In this section , we prove that our HQI ( in the tabular case ) converges to the recursive optimal policy .\n","Assume that the policy at each subtask $M_i$ is ordered , such that it break ties deterministically ( e.g favor left to right ) , it defines a unique recursive optimal hierarchical policy , $\\pi^*_r$ , and a corresponding recursive optimal Q function $Q^*_r$ .\n","We then show that HQI converge to $\\pi^*_r$ and $Q^*_r$ .\n","The $r$ subscript refers to recursive optimality .\n","NOUN PHRASES:\n"," ['section', 'prove', 'HQI', 'tabular case', 'converges', 'recursive optimal policy', 'Assume', 'policy', 'M_i', 'break', 'ties', 'e.g', 'favor', 'left', 'right', 'defines', 'unique recursive optimal hierarchical policy', 'corresponding recursive optimal Q function', 'Q^*_r', 'then show', 'HQI converge', 'Q^*_r', 'subscript refers', 'recursive', 'optimality']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\item We then show that HQI provides an order of training all the subtasks in the \\textit{DAG} graph , such that when training a subtask , $O_i$ , all of its children , $U_i$ already converged to their optimal recursive policies .\n","NOUN PHRASES:\n"," ['then show', 'HQI', 'provides', 'order', 'training', 'subtasks', 'DAG', 'graph', 'training', 'subtask', 'O_i', 'children', 'U_i', 'already converged', 'optimal recursive policies']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Results show that both HQI with and without state abstraction consistently outperforms the FQI when there is limited training data .\n","When the dataset is large enough , they all converge to the same optimal performance , which is around $1.0$ .\n","We also notice that , occasionally , HQI with state abstraction can learn the optimal performance state abstraction with very limited samples , i.e $5000$ samples .\n","This demonstrates that with proper hierarchy constraints and good behavioral policy , HQI can generalize much faster than FQI .\n","Moreover , even the HQI without state abstraction consistently outperforms FQI in terms of sample efficiency .\n","This is different from the behavior of the on - policy MAXQ - Q algorithm reported in ~ \\cite{dietterich2000hierarchical} , which needs state abstraction in order to learn faster than Q - learning .\n","We argue that HQI without state abstraction is more sample efficient than FQI for the following reasons :\n","1 ) HQI uses all applicable primitive samples to update the Q - table for every subtask while MAXQ - Q only updates for the subtask that executes that particular action .\n","2 ) Upper level subtask in MAXQ - Q needs to wait for its children gradually converges to their greedy optimal policy before it can have have a good estimate of $P(s', N|s, u)$ while HQI does not have this limitation .\n","NOUN PHRASES:\n"," ['Results', 'show', 'HQI', 'state abstraction', 'consistently outperforms', 'FQI', 'data', 'dataset', 'converge', 'same optimal performance', 'also notice', 'HQI', 'state abstraction', 'learn', 'optimal performance state abstraction', 'limited samples', 'samples', 'demonstrates', 'proper hierarchy constraints', 'good behavioral policy', 'HQI', 'generalize', 'FQI', 'HQI', 'state abstraction', 'consistently outperforms', 'FQI', 'terms', 'sample efficiency', 'behavior', 'policy MAXQ', 'Q algorithm', 'reported', 'needs', 'state abstraction', 'order', 'learn', 'Q', 'learning', 'argue', 'HQI', 'state abstraction', 'sample efficient', 'FQI', 'following reasons', 'HQI', 'uses', 'applicable primitive samples', 'update', 'Q', 'table', 'subtask', 'MAXQ', 'Q', 'only updates', 'subtask', 'executes', 'particular action', 'Upper level subtask', 'MAXQ', 'Q', 'needs', 'wait', 'children', 'converges', 'greedy optimal policy', 'have have', 'good estimate', 'P', 's', 'N|s', 'u', 'HQI', 'does', 'not have', 'limitation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An MDP , $M$ , can be decomposed into a finite set of subtasks $O=\\{O_0, O_1...O_n\\}$ with the convention that $O_0$ is the root subtask , i.e. solving $O_0$ solves the entire original MDP , $M$ .\n","$O_i$ is then a Semi-Markov Decision Process ( SMDP ) that shares the same $S$ , $R$ , $P$ with $M$ , and has an extra tuple $<\\beta_i, U_i>$ , where :\n","\\begin{enumerate} \\ item $\\beta_i(s)$ is the termination predicate of subtask $O_i$ that partition $S$ into a set of active states , $S_i$ and a set of terminal states $T_i$ .\n","If $O_i$ enters a state in $T_i$ , $O_i$ and its subtasks exit immediately , i.e. $\\beta_i(s)=1$ if $s\\in T_i$ , otherwise $\\beta_i(s)=0$ .\n","\\ item $U_i$ is a nonempty set of actions that can be performed by $O_i$ .\n","The actions can be either primitive actions from $A$ or other subtask , $O_j$ , where $i\\neq j$ .\n","We will refer to $U_i$ as the children of subtask $O_i$ .\n","\\end{enumerate}\n","It is evident that a valid hierarchical decomposition forms a direct acyclic graph ( DAG ) where each non-terminal node corresponds to a subtask , and each terminal node corresponds to a primitive action .\n","For later discussion , we will use \\textit{hierarchical decomposition} and \\textit{DAG} interchangeably .\n","NOUN PHRASES:\n"," ['MDP', 'M', 'finite set', 'O_0', 'O_1', 'convention', 'O_0', 'root subtask', 'i.e', 'solving', 'O_0', 'solves', 'entire original MDP', 'M', 'O_i', 'Semi-Markov Decision Process', 'SMDP', 'shares', 'S', 'R', 'P', 'M', 'has', 'U_i >', 'enumerate', 'termination predicate', 'O_i', 'partition', 'S', 'set', 'active states', 'S_i', 'set', 'terminal states', 'T_i', 'O_i', 'enters', 'state', 'T_i', 'O_i', 'subtasks exit', 'i.e', 's', '=1', 'T_i', '=0', 'U_i', 'nonempty set', 'actions', 'O_i', 'actions', 'primitive actions', 'other subtask', 'O_j', 'refer', 'U_i', 'children', 'O_i', 'enumerate', 'valid hierarchical decomposition', 'forms', 'direct acyclic graph', 'DAG', 'non-terminal node corresponds', 'subtask', 'terminal node corresponds', 'primitive action', 'later discussion', 'use', 'hierarchical decomposition', 'DAG']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A hierarchical policy , $\\pi$ , is a set of policies for each subtask , $O_i$ , $\\pi=\\{\\pi_0, \\pi_1...\\pi_n\\}$ .\n","In the terminology of $option$ framework , a subtask policy is a deterministic $option$ , with $\\beta_i(s)=1$ for $s\\in T_i$ , and $0$ otherwise .\n","NOUN PHRASES:\n"," ['hierarchical policy', 'set', 'policies', 'subtask', 'O_i', 'terminology', 'option', 'framework', 'subtask policy', 'option', '=1']\n","NOUN LOCATIONS:\n"," [0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The \\textbf{second} experiment is running HQI on different variations of hierarchical decomposition of the original MDP .\n","Figure ~ \\ref{fig:DAG2} and Figure ~ \\ref{fig:DAG3} show two different valid DAGs that could also solve the original MDP .\n","Figure ~ \\ref{fig:res2} demonstrates that with sufficient data all three DAG converge to their recursive optimal solution , which confirms that HQI is able converge for different hierarchies .\n","In terms of sample efficiency , three structures demonstrate slight different behavior .\n","We can notice that DAG $2$ learns particularly slower than the other two .\n","We argue that this is because of poor decomposition of the original MDP .\n","Based on the problem settings , \\textit{pick} and \\textit{drop} are all risky actions ( illegal execution lead to $-10$ reward ) , while in DAG $2$ these two actions are mixed with low - cost \\textit{move} actions while the other two DAGs isolated them in a higher level of decision making .\n","Therefore , designing good hierarchy is crucial to obtain performance gain versus flat RL approaches .\n","This emphasizes the importance of the off - policy nature of HQI , which allows developers to experiment with different DAG structures without collecting new samples .\n","How to effectively evaluate the performance of particular hierarchical decomposition without using a simulator is a part of our future research .\n","NOUN PHRASES:\n"," ['experiment', 'HQI', 'different variations', 'hierarchical decomposition', 'original MDP', 'fig', 'DAG2', 'fig', 'DAG3', 'show', 'different valid DAGs', 'also solve', 'original MDP', 'fig', 'res2', 'demonstrates', 'sufficient data', 'DAG converge', 'recursive optimal solution', 'confirms', 'HQI', 'able converge', 'different hierarchies', 'terms', 'sample efficiency', 'structures', 'demonstrate', 'slight different behavior', 'notice', 'DAG', 'learns', 'argue', 'poor decomposition', 'original MDP', 'Based', 'problem settings', 'pick', 'drop', 'risky actions', 'illegal execution lead', 'reward', 'DAG', 'actions', 'cost', 'move', 'actions', 'DAGs', 'isolated', 'level', 'decision making', 'designing', 'good hierarchy', 'obtain', 'performance gain', 'flat RL approaches', 'emphasizes', 'importance', 'policy nature', 'HQI', 'allows', 'developers', 'experiment', 'different DAG structures', 'collecting', 'new samples', 'effectively evaluate', 'performance', 'particular hierarchical decomposition', 'using', 'simulator', 'part', 'future research']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Then for subtasks with other subtask children , by definition , when we run SQI , the children of $O_i$ ( $U_i$ ) have converged to their unique deterministic optimal recursive policy .\n","This means that every action $u \\in U_i$ , is a deterministic \\textit{deterministic Markov option} as defined in the $option$ framework \\cite{sutton1999between} .\n","\\cite{sutton1999between} proved that \" for any set of \\textit{deterministic Markov options} one step intra-option Q - learning converges w. p. 1 to the optimal Q - values , for every option regardless of what options are executed during learning provided every primitive action gets executed in every state infinitely often \" .\n","Refer to the \\cite{sutton1999between} , for the detailed proof .\n","NOUN PHRASES:\n"," ['subtasks', 'other subtask children', 'definition', 'run', 'children', 'O_i', 'U_i', 'have converged', 'unique deterministic optimal recursive policy', 'means', 'action', 'deterministic Markov option', 'defined', 'option', 'proved', 'set', 'deterministic Markov options', 'step intra-option Q', 'learning converges', 'w.', 'optimal Q', 'values', 'option regardless', 'options', 'learning provided', 'primitive action', 'gets executed', 'state', 'Refer', 'detailed proof']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," An MDP $M$ is described by $(S, A, P, R, P_0)$            \\ item $S$ is the state space of            \\ item $A$ is a set of primitive actions that are available \\\n","item $P(s'|s,a)$ defines the transition probability of executing primitive action $a$ in state ,            \\ item $R(s'|s,a)$ is the reward function defined over $S$ and $A$ \\end{enumerate}\n","NOUN PHRASES:\n"," ['MDP', 'M', 'S', 'A', 'P', 'R', 'P_0', 'S', 'state space', 'item', 'set', 'primitive actions', 'P', \"s'|s\", 'defines', 'transition probability', 'executing', 'primitive action', 'state', 'item', 'R', \"s'|s\", 'reward function', 'defined', 'S', 'enumerate']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Mostly , we follow the definitions in the MAXQ framework .\n","However , for notation simplicity , we also borrow some notations from the $option$ framework .\n","NOUN PHRASES:\n"," ['follow', 'definitions', 'MAXQ framework', 'notation simplicity', 'also borrow', 'notations', 'option', 'framework']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The \\textbf{last} experiment utilizes Random Forests as the function approximator to model the Q - value function in DAG 1 .\n","The main purpose is to demonstrate the convergence of Fitted HQI .\n","For each subtask $O_i$ the Q - value function $Q_i(s, u)$ is modelled by a random forest with $[dest, pass, x, y]$ as the input feature .\n","Since $dest$ and $pass$ are categorical variables , we represent them as a one - hot vector , which transforms the state variable into a $11$ dimension vector ( 4 d for destination , 5 d for passenger and 2 d for the $x,y$ coordinate ) .\n","We report the mean average discounted rewards over 5 independent runs with different random samples of different sizes .\n","Figure ~ \\ref{fig:res3} shows that Fitted - HQI achieves similar performance compared to Tabular HQI .\n","NOUN PHRASES:\n"," ['experiment utilizes Random Forests', 'function approximator', 'model', 'Q', 'value function', 'DAG', 'main purpose', 'demonstrate', 'convergence', 'Fitted HQI', 'O_i', 'Q', 'value function', 'Q_i', 'random forest', '[ dest', 'pass', 'x', 'input feature', 'categorical variables', 'represent', 'hot vector', 'transforms', 'state', 'dimension vector', 'd', 'destination', 'd', 'passenger', 'd', 'coordinate', 'report', 'mean average', 'discounted', 'rewards', 'independent runs', 'different random samples', 'different sizes', 'fig', 'res3', 'shows', 'Fitted', 'HQI', 'achieves', 'similar performance', 'compared', 'Tabular HQI']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The dataset for each run were collected in advance by choosing actions uniformly at random with different sizes .\n","We evaluate the performance of algorithms by running greedy execution for $100$ times to obtain average discounted return at every $5000$ new samples and up to $60,000$ samples .\n","We repeat the experiments for $5$ times to evaluate the influence of different sample distribution .\n","The discounting factor is set to be $0.99$ .\n","NOUN PHRASES:\n"," ['dataset', 'run', 'advance', 'choosing', 'actions', 'random', 'different sizes', 'evaluate', 'performance', 'algorithms', 'running', 'greedy execution', 'times', 'obtain average discounted', 'return', 'new samples', 'samples', 'repeat', 'experiments', 'times', 'evaluate', 'influence', 'different sample distribution', 'discounting factor']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A \\textit{recursive optimal} policy for MDP $M$ with hierarchical decomposition is a hierarchical policy $\\pi=\\{\\pi_0...\\pi_n\\}$ , such that for each subtask , $O_i$ , the corresponding policy $\\pi_i$ is optimal for the SMDP defined by the set of states , $S_i$ , the set of actions $U_i$ , the state transition probability $P^{\\pi}(s', N|s,a)$ , and the rewards function $R(s'|s,a)$ .\n","NOUN PHRASES:\n"," ['policy', 'MDP', 'M', 'hierarchical decomposition', 'hierarchical policy', 'subtask', 'O_i', 'corresponding policy', 'SMDP', 'defined', 'set', 'states', 'S_i', 'set', 'actions', 'U_i', 'state transition probability', 'P^', 'N|s', 'rewards', 'function', 'R', \"s'|s\"]\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The problem formulation is as following : given any finite set of samples , $D=\\big\\{ (s_m, a_m, r_m, s^{'}_m) |m=1,2,...,M \\big\\}$ and any valid hierarchical decomposition $O$ of the original MDP $M$ , we wish to learn the recursive optimal hierarchical policy $\\pi^{*}$ .\n","NOUN PHRASES:\n"," ['problem formulation', 'following', 'given', 'finite set', 'samples', 's_m', 'a_m', 'r_m', '_m', '|m=1,2', 'valid hierarchical decomposition', 'O', 'original MDP', 'M', 'wish', 'learn', 'recursive optimal hierarchical policy', '*']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The \\textbf{first} experiment compares HQI against flat Q - value Iteration ( FQI ) .\n","Also , as pointed out in ~ \\cite{dietterich2000hierarchical} , state abstraction is essential for MAXQ to have fast learning speed compared to flat Q learning .\n","As a result , we manually conduct state abstraction for each subtask in DAG $1$ .\n","However , different from the aggressive state abstraction described in ~ \\cite{dietterich2000hierarchical} , where every subtask and child pair has a different set of state variables , we only conduct a simple state abstraction at subtask level , i.e. all children of a subtask has the same state abstraction .\n","The final state abstraction is listed in Table ~ \\ref{tbl:DAG1} .\n","As described above , we run $5$ independent runs with different random samples of different sizes , we report the mean average discounted return over five runs in Figure ~ \\ref{fig:res0} , as well as the best average discounted reward of the five runs in Figure ~ \\ref{fig:res1} .\n","NOUN PHRASES:\n"," ['experiment compares', 'HQI', 'flat Q', 'value Iteration', 'FQI', 'pointed', 'state abstraction', 'MAXQ', 'have', 'fast learning', 'speed', 'compared', 'flat Q learning', 'result', 'manually conduct', 'state abstraction', 'subtask', 'DAG', 'aggressive state abstraction', 'described', 'subtask', 'child pair', 'has', 'different set', 'state variables', 'only conduct', 'simple state abstraction', 'subtask level', 'i.e', 'children', 'subtask', 'has', 'same state abstraction', 'final state abstraction', 'tbl', 'DAG1', 'described', 'run', 'independent runs', 'different random samples', 'different sizes', 'report', 'mean average', 'discounted', 'return', 'runs', 'fig', 'res0', 'average', 'discounted', 'reward', 'runs', 'fig', 'res1']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We applied our algorithm to the Taxi domain described in \\cite{sutton1999between} .\n","This is a simple grid world that contains a taxi , a passenger , and four specially - designated locations labeled R , G , B , and Y .\n","In the starting state , the taxi is in a randomly - chosen cell of the grid , and the passenger is at one of the four special locations .\n","The passenger has a desired destination that he / she wishes to reach , and the job of the taxi is to go to the passenger , pick him / her up , go to the passenger 's destination , and drop the passenger .\n","The taxi has six primitive actions available to it : move one step to one of the four directions ( north , south , east and west ) , pick up the passenger and put down the passenger .\n","To make the task more difficult , the move actions are not deterministic , so that it has $20\\%$ chance of moving in one of the other directions .\n","Also , every move in the grid will cost $-1$ reward .\n","Attempting to pick up or drop passenger at wrong location will cause $-10$ reward .\n","At last , successfully finish the task has $20$ reward .\n","The grid is described in figure \\ref{fig:taxi0} .\n","Therefore , there are 4 possible state for the destination , 5 possible state for the passenger ( 4 location and 5 is on the car ) , 25 possible locations , which results into $500*6=3000$ parameters in the Q - table that needs to be learned .\n","We denote the state variable as $[dest, pass, x, y]$ for later discussion .\n","NOUN PHRASES:\n"," ['applied', 'algorithm', 'Taxi domain', 'described', 'simple grid world', 'contains', 'taxi', 'passenger', 'designated locations', 'labeled', 'R', 'G', 'B', 'Y', 'starting state', 'taxi', 'chosen cell', 'grid', 'passenger', 'special locations', 'passenger', 'has', 'desired', 'destination', '/', 'wishes', 'reach', 'job', 'taxi', 'go', 'passenger', 'pick', '/', 'go', 'passenger', 'destination', 'drop', 'passenger', 'taxi', 'has', 'primitive actions', 'move', 'step', 'directions', 'pick', 'passenger', 'put', 'passenger', 'make', 'task', 'move actions', 'has', '%', 'chance', 'moving', 'other directions', 'move', 'grid', 'cost', 'reward', 'Attempting', 'pick', 'drop', 'passenger', 'wrong location', 'cause', 'reward', 'successfully finish', 'task', 'has', 'reward', 'grid', 'fig', 'taxi0', 'possible state', 'destination', 'possible state', 'passenger', 'location', 'car', 'possible locations', 'results', 'parameters', 'Q', 'table', 'needs', 'denote', 'state', 'pass', 'x', 'discussion']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 1. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," One challenge of training a subtask with subtask children is that we cannot use the optimal SMDP Bellman equation described in the MAXQ framework ~ \\cite{dietterich2000overview} , $Q_i(s, u)$ , which is the Q - value function for subtask , $O_i$ , at state , $s$ and action $u$ :\n","\\begin{equation}\n","\\label{eq:maxqq}\n","Q_i(s, u) = V(s, u) + \\sum_{s', N} P_i^{\\pi}(s', N | s, u)\\gamma^N Q_i^{\\pi}(s', \\pi_i(s')) \\\\\n","\\end{equation}\n","\\begin{equation}\n","V(s, u) = \\begin{cases}\n","max_{u'}(Q_u(s, u')) &\\text{            is subtask}\\\\\n","\\sum_{s'} P(s'|s,u)R(s'|s,u) &\\text{            is primitive}\n","\\end{cases}\n","\\end{equation}\n","The main problem of this equation \\eqref{eq:maxqq} is that in order to estimate the Q - value for a subtask children , $u$ , the parent $O_i$ needs to have an estimate about the transition probability $P_i^{\\pi}(s', N | s, u)$ , which is the distribution of $u$ 's exit state and number of primitive steps needed to reach its termination .\n","Although the termination states of the child $u$ are given by $T_u$ , it is difficult to estimate the joint distribution of termination steps $N$ and            ' if $u$ follows an policy that is different from the behavior policy without recollecting new samples .\n","This is because since the behavior policy is usually random and poor in performance , the collected samples do not provide information about how many steps the subtask $u$ would take to terminate if following a different ( optimal ) policy .\n","NOUN PHRASES:\n"," ['challenge', 'training', 'subtask', 'subtask children', 'not use', 'optimal SMDP Bellman equation', 'described', 'dietterich2000overview', 'Q_i', 'Q', 'value function', 'subtask', 'O_i', 'state', 'action', 'equation', 'eq', 'maxqq', 'Q_i', '= V', '+', 's', 'N', 'P_i^', 'N | s', 'u', 'Q_i^', 'equation', 'equation', 'V', '=', 'cases', 'max_', 'Q_u', 's', 'P', \"s'|s\", 'u', 'R', \"s'|s\", 'u', 'cases', 'equation', 'main problem', 'eq', 'maxqq', 'order', 'estimate', 'Q', 'value', 'subtask children', 'parent', 'O_i', 'needs', 'have', 'estimate', 'transition probability', 'P_i^', 'N | s', 'u', 'distribution', 'exit state', 'number', 'primitive steps', 'needed', 'reach', 'termination', 'termination states', 'T_u', 'estimate', 'joint distribution', 'termination steps', 'N', 'follows', 'policy', 'behavior policy', 'recollecting', 'new samples', 'behavior policy', 'performance', 'collected samples', 'do', 'not provide', 'information', 'many steps', 'take', 'terminate', 'following', 'policy']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We now propose Hierarchical Q - value Iteration , HQI , and we prove that it converges to the recursive optimal solution for any hierarchical decomposition given that the batch sample distribution has sufficient state action exploration .\n","The basic idea is to train every subtask using Subtask Q - value Iteration ( SQI ) in a bottom up fashion .\n","The training prerequisite of SQI for a specific subtask $O_i$ is that all of its children $U_i$ have converged to their greedy optimal policies .\n","In order to fulfil this constraint , HQI first topologically sorts the \\textit{DAG} and running SQI from subtasks whose children have only primitive actions .\n","After those subtasks converge to their optimal policy , the algorithm continues to other subtasks whose children are either converged or primitive actions .\n","We will show that there always exist an ordering of training every subtask in a valid \\textit{DAG} that fulfills the prerequisite of SQI .\n","NOUN PHRASES:\n"," ['now propose', 'Hierarchical Q', 'value Iteration', 'HQI', 'prove', 'converges', 'recursive optimal solution', 'hierarchical decomposition', 'given', 'batch sample distribution', 'has', 'sufficient state action exploration', 'basic idea', 'train', 'subtask', 'using', 'Subtask Q', 'value Iteration', 'SQI', 'bottom', 'fashion', 'training prerequisite', 'SQI', 'O_i', 'children', 'U_i', 'have converged', 'greedy optimal policies', 'order', 'fulfil', 'constraint', 'HQI', 'first topologically sorts', 'DAG', 'running', 'SQI', 'subtasks', 'children', 'have', 'primitive actions', 'subtasks', 'converge', 'optimal policy', 'algorithm', 'continues', 'other subtasks', 'children', 'converged', 'primitive actions', 'show', 'always exist', 'ordering', 'training', 'subtask', 'DAG', 'fulfills', 'prerequisite', 'SQI']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\textbf{Step 2}\n","By definition , a hierarchical decomposition is a Directed Acyclic Graph ( DAG ) with edges from parents to their children .\n","In this proof , we first reverse the edges so that they are from children to their parents .\n","Also we know from Graph Theory that any Directed Acyclic Graph has at least one topological sort , such that every edge $uv$ , $u$ comes before $v$ in the ordering ~ \\cite{cormen2001section} .\n","Therefore , we can to topologically sort the hierarchical decomposition with reversed edges such that SQI can always train the children before parents .\n","NOUN PHRASES:\n"," ['Step', 'definition', 'hierarchical decomposition', 'Directed Acyclic Graph', 'DAG', 'edges', 'parents', 'children', 'proof', 'first reverse', 'edges', 'children', 'parents', 'know', 'Graph Theory', 'Directed Acyclic Graph', 'has', 'topological sort', 'edge', 'comes', 'ordering', 'cormen2001section', 'topologically sort', 'hierarchical decomposition', 'reversed edges', 'SQI', 'always train', 'children', 'parents']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ noindent \\textbf{Hint:}\n","Predicate $\\mathtt{road}$ was used with arity $1$ which is unexpected .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Hint', 'Predicate', 'road', 'quotation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example with $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{preprocessing_err_2}$ we obtain $\\mi{wrongarity}(P_U,P_R) = \\{ (\\mt{road},1) \\}$ and we can give the following hint accordingly .\n","NOUN PHRASES:\n"," ['example', 'eqSymmetry', 'preprocessing_err_2', 'obtain', 'wrongarity', 'P_U', 'P_R', '=', 'road', ',1', 'give', 'following hint']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Note that if the student is unable to correct the problem after this hint , the next hint can reveal that the arity of $\\mt{road}$ in the sample solution is $2$ , without revealing the full solution .\n","NOUN PHRASES:\n"," ['Note', 'student', 'correct', 'problem', 'hint', 'next hint', 'reveal', 'arity', 'road', 'sample solution', 'revealing', 'full solution']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example if $P_R = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{eqExpected}$ and $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{preprocessing_err_1}$ then we obtain $\\mi{wrongpred}(P_U,P_R) = \\{ \\mt{obstacle} \\}$ .\n","This allows us to easily produce the following hint .\n","NOUN PHRASES:\n"," ['example', 'eqSymmetry', 'eqExpected', 'eqSymmetry', 'preprocessing_err_1', 'obtain', 'wrongpred', 'P_U', 'P_R', '=', 'obstacle', 'allows', 'easily produce', 'following hint']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ noindent \\textbf{Hint:}\n","Predicate $\\mathtt{obstacle}$ should not be used .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Hint', 'Predicate', 'obstacle', 'quotation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example with $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{preprocessing_err_3}$ we obtain $\\mi{wrongcons}(P_U,P_R) = \\{ \\mt{x}, \\mt{y} \\}$ and we can give the following hint accordingly .\n","NOUN PHRASES:\n"," ['example', 'eqSymmetry', 'preprocessing_err_3', 'obtain', 'wrongcons', 'P_U', 'P_R', '=', 'y', 'give', 'following hint']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If the student is not able to fix the problem , we can also mention the predicate .\n","\\begin{quotation} \\ noindent \\textbf{Hint:}\n","The answer set contains more true atoms of predicate $\\mt{open\\_road}$ than it should .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['student', 'fix', 'problem', 'also mention', 'predicate', 'quotation', 'Hint', 'answer set', 'contains', 'true atoms', 'quotation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For example the answer \\ref{semantic_err_1} works correctly only for obstacles between DÃƒÂ¼zce and Bolu , but not between DÃƒÂ¼zce and Zonguldak .\n","Given $P_R$ as above and $P_U = \\eqref{eqExGiven} \\cup \\eqref{eqSymmetry} \\cup \\eqref{semantic_err_1}$ we obtain one answer set $I_R \\ins \\AS(P_R)$ and one answer set $I_U \\ins \\AS(P_U)$ such that $I_R \\setminus I_U = \\emptyset$ because the student 's solution reproduces all atoms that are true in the reference solution , moreover $I_U \\setminus I_R = \\{\n","\\mt{open\\_road(duzce,zonguldak)},\\allowbreak\n","\\mt{open\\_road(zonguldak,duzce)} \\}$ because the student 's solution additionally produces true atoms that should not be true .\n","NOUN PHRASES:\n"," ['example', 'semantic_err_1', 'works', 'obstacles', 'DÃƒÂ¼zce', 'Bolu', 'DÃƒÂ¼zce', 'Zonguldak', 'Given', 'P_R', 'above', 'eqSymmetry', 'semantic_err_1', 'obtain', 'answer', 'set', 'P_R', 'answer', 'set', 'P_U', 'student', 'solution', 'reproduces', 'atoms', 'reference solution', 'duzce', 'zonguldak', 'zonguldak', 'duzce', 'student', 'solution', 'additionally produces', 'true atoms']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\noindent\n","\\textbf{Hint:}\n","The program contains the following unexpected constants which are not required in the solution :\n","$\\mt{x}$ ,            .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Hint', 'program', 'contains', 'following unexpected constants', 'not required', 'solution', 'x', 'quotation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," {\\bf Further Hints .~~}\n","Using $\\mi{bpreds}$ , $\\mi{hpreds}$ , $\\mi{bpredarities}$ , and $\\mi{hpredarities}$ , further hints that are more specific , can be produced , for example that a certain predicate should be used in the body of a rule in the solution .\n","NOUN PHRASES:\n"," ['Further Hints', '.~~', 'Using', 'bpreds', 'hpreds', 'bpredarities', 'hpredarities', 'further hints', 'example', 'certain predicate', 'body', 'rule', 'solution']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," If even this does not help , we can concretely say which true atom should not be true .\n","\\begin{quotation} \\ noindent \\textbf{Hint:}\n","The answer set contains true atoms which should be false :\n","$\\mt{open\\_road(duzce,zonguldak)}$ and $\\mt{open\\_road(zonguldak,duzce)}$ .\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['does', 'not help', 'concretely say', 'true atom', 'quotation', 'Hint', 'answer set', 'contains', 'true atoms', 'duzce', 'zonguldak', 'zonguldak', 'duzce', 'quotation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Answer Set Programming ( ASP ) is a declarative logic programming paradigm \\cite{Gelfond1988,Lifschitz2008,Gebser2012aspbook} .\n","An atom is form $p(x_1,\\ldots,x_l)$ with $0 \\les l$ , and if $l\\eqs0$ we write the atom short as $p$ .\n","A program $P$ consists of a set of rules , a rule $r$ is of the form\n","\\begin{equation}\n","\\label{eqRule}\n","\\mathtt{\n","\\alpha_1 \\lor \\cdots \\lor \\alpha_k\n","\\leftimpl \\beta_1, \\ldots, \\beta_n,\n","\\naf \\beta_{n+1}, \\ldots, \\naf \\beta_m\n","}\n","\\end{equation}\n","where $\\alpha_i$ and $\\beta_i$ are atoms , called head and body atoms of $r$ , respectively .\n","We say that $H(r) = \\{ \\alpha_1,\\ldots, \\alpha_k \\}$ is the \\emph{head} of $r$ , and $B^+(r) = \\{ \\beta_1,\\ldots,\\beta_n\\}$ , respectively $B^-(r) = \\{ \\beta_{n+1},\\ldots,\\beta_m\\}$ are the positive , respectively negative \\emph{body} of $r$ .\n","We call a rule a \\emph{fact} if $m \\eqs 0$ , \\emph{disjunctive} if $k \\gts 1$ , and a \\emph{constraint} if $k \\eqs 0$ .\n","Atoms can contain constants , variables , and function terms , and a program must allow for a finite instantiation .\n","Semantics of an ASP program $P$ is defined based on the ground instantiation $\\grnd(P)$ and Herbrand base $\\HB_P$ of $P$ : an interpretation $I \\ins \\HB_P$ satisfies a rule $r$ iff $H(r) \\caps I \\neqs \\emptyset$ or $B^+(r) \\nsubseteqs I$ or $B^-(r) \\caps I \\neqs \\emptyset$ ; $I$ is a model of $P$ if it satisfies all rules in $P$ .\n","The reduct $P^I$ of $P$ wrt .\n","\\ $I$ is the set of rules $P^I \\eqs \\{ H(r) \\leftimpl B^+(r) \\mids B^-(r) \\caps I \\eqs \\emptyset \\}$ and an interpretation\n","$I$ is an answer set iff it is a $\\subseteq$ - minimal model of $P^I$ .\n","Details of syntactic restrictions , additional syntactic elements , and semantics of ASP are described in the ASP - Core - 2 standard ~% \\cite{Calimeri2012} .\n","NOUN PHRASES:\n"," ['Answer Set Programming', 'ASP', 'declarative logic', 'programming', 'Gelfond1988', 'Lifschitz2008', 'Gebser2012aspbook', 'atom', 'p', 'x_l', 'write', 'atom', 'program', 'P', 'consists', 'set', 'rules', 'rule', 'r', 'form', 'equation', 'eqRule', 'equation', 'called', 'head', 'body atoms', 'say', 'H', 'r', '=', 'head', 'B^+', 'r', '=', 'B^-', 'r', '=', 'body', 'call', 'rule', 'fact', 'm', 'k', 'constraint', 'k', 'Atoms', 'contain', 'constants', 'variables', 'function terms', 'program', 'allow', 'finite instantiation', 'Semantics', 'ASP program', 'P', 'ground instantiation', 'P', 'Herbrand', 'base', 'P', 'interpretation', 'satisfies', 'rule', 'r', 'H', 'r', 'B^+', 'r', 'B^-', 'r', 'model', 'P', 'satisfies', 'rules', 'P', 'reduct', 'P^I', 'P', 'wrt', 'set', 'rules', 'H', 'r', 'B^+', 'r', 'B^-', 'r', 'interpretation', 'answer set iff', 'minimal model', 'P^I', 'Details', 'syntactic restrictions', 'additional syntactic elements', 'semantics', 'ASP', 'ASP', 'Core', '%', 'Calimeri2012']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," While writing programs students are making different kind of mistakes .\n","Our methodology for giving hints is shown in Figure ~ \\ref{figFlowchart} .\n","The input consists of the student 's answer program $P_U$ and the reference solution $P_R$ , where we assume that all parts of the example are combined into a single program .\n","Analyzing the input and giving hints is based on three phases which address different kinds of mistakes .\n","NOUN PHRASES:\n"," ['writing', 'programs students', 'different kind', 'mistakes', 'methodology', 'giving', 'hints', 'figFlowchart', 'input', 'consists', 'student', 'program', 'P_U', 'reference solution', 'P_R', 'assume', 'parts', 'example', 'single program', 'Analyzing', 'input', 'giving', 'hints', 'phases', 'address', 'different kinds', 'mistakes']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\\n","noindent \\textbf{Question:} {\\ it\n","Write a rule which defines predicate $\\mathtt{open\\_road(From, To)}$ that is true for all pairs of cities where the direct road connection from $\\mathtt{From}$ to $\\mathtt{To}$ is not blocked . }\n","\\end{quotation}\n","NOUN PHRASES:\n"," ['Question', 'Write', 'rule', 'defines', 'From', 'pairs', 'cities', 'direct road connection', 'not blocked', 'quotation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{quotation} {\\ it\n","We have a graph which represents the map of cities connected by roads .\n","Cities are nodes , roads are edges , all roads are bidirectional , and roads might be blocked .\n","The task is is to find a route from city $X$ to city $Y$ in the road network such that the path uses only roads that are not blocked .\n","The graph is shown in Figure ~ \\ref{figCities} and is represented in the following set of facts . }\n","\\end{quotation}\n","\\begin{equation}\n","\\label{eqExGiven}\n","\\begin{split}\n","\\begin{array}{@{\\!\\!}l@{\\ }l@{}}\n","\\mathtt{road(istanbul,kocaeli).}\n","&\\mathtt{road(karabuk,bolu).}\\\\\n","\\mathtt{road(kocaeli,sakarya).}\n","&\\mathtt{road(duzce,karabuk).}\\\\\n","\\mathtt{blocked(duzce,zonguldak).}\n","&\\mathtt{road(bolu,zonguldak).}\\\\\n","\\mathtt{road(duzce,zonguldak).}\n","&\\mathtt{road(sakarya,duzce).}\n","\\end{array}\n","\\end{split}\n","\\end{equation}\n","\\begin{quotation} {\\ it Moreover the fact that roads are bidirectional is represented in the following rule . }\n","\\end{quotation}\n","\\begin{equation}\n","\\label{eqSymmetry}\n","\\mathtt{road(X,Y)} \\ \\leftimpl \\ \\mathtt{road(Y,X).}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['quotation', 'have', 'graph', 'represents', 'map', 'cities', 'connected', 'roads', 'Cities', 'nodes', 'roads', 'edges', 'roads', 'roads', 'task', 'find', 'route', 'city', 'X', 'city', 'Y', 'road network', 'path', 'uses', 'roads', 'not blocked', 'graph', 'figCities', 'following set', 'facts', 'quotation', 'equation', 'split', 'array', 'l @', 'road', 'road', 'bolu', 'road', 'sakarya', 'road', 'duzce', 'karabuk', 'blocked', 'duzce', 'zonguldak', 'road', 'bolu', 'zonguldak', 'road', 'duzce', 'zonguldak', 'road', 'duzce', 'array', 'split', 'equation', 'quotation', 'fact', 'roads', 'following rule', 'quotation', 'equation', 'eqSymmetry', 'road', 'X', 'Y', 'road', 'Y', 'X', 'equation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This indicates the location of the mistake in the source code ( line $3$ , characters $8{-}9$ ) and allows us to display the error with additional visual support to the student , for example as the following hint .\n","\\begin{align*}\n","\\begin{array}\n","{@{}l@{}}\n","           \\\\\n","           \\\\\n","           \\up arrow }\\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","           \\\\\n","\\text{or similar.}  \\end{array}  \\end{align*}\n","NOUN PHRASES:\n"," ['indicates', 'location', 'mistake', 'source code', 'line', 'characters', 'allows', 'display', 'error', 'additional visual support', 'student', 'example', 'following hint', 'align*', 'array', 'l @', 'array', 'align*']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Given a set of atoms $A \\eqs \\{ p_1(t_1,\\ldots,t_{k_1}), \\ldots, p_m(t_1,\\ldots,t_{k_m}) \\}$ we define function $\\mi{preds}(A)$ which returns the set of predicates in $A$ , the function $\\mi{predarities}(A)$ which returns a set of tuples of predicates with their arity as occuring in $A$ , and the function $\\mi{constants}(A)$ which returns the constants used in $A$ .\n","\\begin{align} & & \\mi{preds} ( A ) \\eqs& \\{ {p} &\\ mids & p ( t_1,\\ldots , t_k ) \\ins A \\} && &\\\\ &&\n","\\quad\\mi{predarities} ( A ) \\eqs& \\{ {(p,k)}            &\\ mids & p ( t_1 ,\\ldots , t_k ) \\ins A \\} && &\\\\ && \\mi{constants} ( A ) \\eqs& \\{ {t_i} & \\mids&p(t_1,\\ldots,t_k) \\ins A,\\ 1 \\les i \\les k, \\text{and } t_i \\text{ is a constant term}  \\}. && \\hspace*{2em} & \\end{align}\n","NOUN PHRASES:\n"," ['Given', 'set', 'atoms', 'p_1', 't_1', 'k_1', 't_1', 'k_m', 'define', 'preds', 'returns', 'set', 'predicates', 'function', 'predarities', 'returns', 'set', 'tuples', 'predicates', 'arity', 'occuring', 'function', 'constants', 'returns', 'constants', 'used', 'align', 'preds', 'p', 'p', 't_1', 't_k', 'predarities', 'p', 'k', 'p', 't_1', 't_k', 'constants', 't_i', 'p', 't_1', 't_k', 'A', 'k', 't_i', 'constant term', 'align']\n","NOUN LOCATIONS:\n"," [1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Given a reference solution $P_R$ and student input $P_U$ we can use the above definitions to give hints without revewaling $P_R$ .\n","NOUN PHRASES:\n"," ['Given', 'reference solution', 'P_R', 'student', 'P_U', 'use', 'above definitions', 'give', 'hints', 'revewaling', 'P_R']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," We next define functions for obtaining information about predicates and their arities in heads and bodies of rules and programs .\n","\\begin{align*}\n","\\mi{bpreds} ( r ) \\eqs& \\mi{preds} ( B ^{+ } ( r ) \\cups B^{-} ( r ) ) \\\\ \\mi{hpreds} ( r ) \\eqs& \\mi{preds} ( H ( r ) ) \\\\\n","\\mi{bpredarities} ( r ) \\eqs& \\mi{predarities} ( B ^{+ } ( r ) \\cups B^{-} ( r ) ) \\\\ \\mi{hpredarities} ( r ) \\eqs& \\mi{predarities} ( H ( r ) ) \\\\ \\intertext{where            is a rule of form \\eqref{eqRule} , moreover }\n","\\mi{bpreds} ( P ) \\eqs& \\bigcup_{r \\ins P}  \\mi{bpreds} ( r ) \\\\\n","\\mi{hpreds} ( P ) \\eqs& \\bigcup_{r \\ins P}\n","\\mi{hpreds} ( r ) \\\\\n","\\mi{bpredarities} ( P ) \\eqs& \\bigcup_{r \\ins P}  \\mi{bpredarities} ( r ) \\\\\n","\\mi{hpredarities} ( P ) \\eqs& \\bigcup_{r \\ins P}  \\mi{hpredarities} ( r ) \\\\\n","\\mi{preds} ( P ) \\eqs& \\mi{bpreds} ( P ) \\cups \\mi{hpreds} ( P ) \\\\\n","\\mi{predarities} ( P ) \\eqs& \\mi{bpredarities} ( P ) \\cups \\mi{hpredarities} ( P ) \\end{align*} where $P$ is a program --- a set of rules of form \\eqref{eqRule} .\n","NOUN PHRASES:\n"," ['next define functions', 'obtaining', 'information', 'predicates', 'arities', 'heads', 'bodies', 'rules', 'programs', 'align*', 'bpreds', 'r', 'preds', 'B ^', '+', 'r', 'B^', 'r', 'hpreds', 'r', 'preds', 'H', 'r', 'bpredarities', 'r', 'predarities', 'B ^', '+', 'r', 'B^', 'r', 'hpredarities', 'r', 'predarities', 'H', 'r', 'rule', 'eqRule', 'bpreds', 'P', 'r', 'P', 'bpreds', 'r', 'hpreds', 'P', 'r', 'P', 'hpreds', 'r', 'bpredarities', 'P', 'r', 'P', 'bpredarities', 'r', 'hpredarities', 'P', 'r', 'P', 'hpredarities', 'r', 'preds', 'P', 'bpreds', 'P', 'hpreds', 'P', 'predarities', 'P', 'bpredarities', 'P', 'hpredarities', 'P', 'align*', 'P', 'program', 'set', 'rules', 'eqRule']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure ~ \\ref{fig:turks} shows box plots of the precision for the four models on three corpora .\n","In most cases , HD TM performs the best .\n","As in ~ \\cite{Chang2009} , the likelihood scores do not necessarily correspond to human judgments .\n","Paired , two - tailed t - tests of statistical significants ( $p<0.05$ ) performed between HD TM $\\gamma=0.95$ and $\\gamma=0.05$ and the other models are represented by $\\ast$ and $\\circ$ in Figure ~ \\ref{fig:turks} respectively .\n","NOUN PHRASES:\n"," ['Figure', '~', 'fig', 'turks', 'shows', 'box plots', 'precision', 'models', 'corpora', 'cases', 'HD TM', 'performs', 'Chang2009', 'likelihood scores', 'do', 'not necessarily correspond', 'human judgments', 'Paired', 'tailed', 'tests', 'statistical significants', 'p', 'performed', 'HD TM', 'other models', 'fig', 'turks']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $\\mathbbm{1}(\\cdot)$ is the indicator function and $J$ is the number of judges .\n","The model precision is basically the fraction of judges agreeing with the model .\n","NOUN PHRASES:\n"," ['indicator function', 'J', 'number', 'judges', 'model precision', 'fraction', 'judges', 'agreeing', 'model']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ centering \\includegraphics[width=.85\\textwidth]{./fig/dblphierarchy.pdf}  \\caption{Constructed hierarchy of bibliographic network with HDTM            . Words at the root document represent the most probable words in the root topic. Most probable words for other documents are not shown due to space constraints.}\n","\\label{fig:dblphierarchy}  \\end{figure}\n","NOUN PHRASES:\n"," ['centering', './fig/dblphierarchy.pdf', 'Constructed hierarchy', 'bibliographic network', 'HDTM', 'Words', 'root document', 'represent', 'probable words', 'root topic', 'probable words', 'other documents', 'not shown', 'space constraints', 'fig', 'dblphierarchy', 'figure']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In classic Gibbs sampling , maximum a posteriori estimation is determined by the the mode of the samples .\n","For some node $d$ , this translates to choosing the parent that appeared in the most samples as the final parent ; in the above case node $d$ would chose node $y$ as its parent because node $y$ was sampled 15 times compared to only 5 samples of node $x$ .\n","To answer the second question , and therefore model the certainty of an inferred hierarchy , we calculate the number of times some node $d$ picks its final parent over the total number of samples normalized by the number of possible choices , { \\em i.e. } , parents , node $d$ has .\n","This results in a certainty score for node $d$ :\n","NOUN PHRASES:\n"," ['classic Gibbs sampling', 'maximum', 'posteriori estimation', 'mode', 'samples', 'translates', 'choosing', 'parent', 'appeared', 'samples', 'final parent', 'above case', 'chose', 'parent', 'times', 'compared', 'samples', 'answer', 'second question', 'therefore model', 'certainty', 'inferred hierarchy', 'calculate', 'number', 'times', 'picks', 'final parent', 'total number', 'samples', 'normalized', 'number', 'possible choices', 'parents', 'has', 'results', 'certainty score']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," where $n$ represents the total number of samples , $n_{p}$ is the number of times the final parent $p$ is sampled , and $deg^{-}(d)$ is the indegree of node $d$ representing the total number of parents that node $d$ could pick from .\n","In the certainty score the outside fraction is used to measure the normalized difference between the raw probability and probability of random guess .\n","Applying this function to the above example we would calculate that the certainty score of node $d$ would be $(\\frac{15}{20}-\\frac{1}{2}) / \\frac{15}{20} = .33$ .\n","NOUN PHRASES:\n"," ['represents', 'total number', 'samples', 'p', 'number', 'times', 'final parent', 'deg^', 'd', 'indegree', 'representing', 'total number', 'parents', 'node', 'pick', 'certainty', 'score', 'outside fraction', 'measure', 'normalized difference', 'raw probability', 'probability', 'random guess', 'Applying', 'function', 'above example', 'calculate', 'certainty score', '/']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The bibliography network data had relatively low precision scores .\n","This is almost certainly because it was more difficult for the judges , who were probably not computer scientists , to differentiate between the topics in research paper titles .\n","Figure ~ \\ref{fig:dblphierarchy} shows a small portion of the document hierarchy for the bibliographic network data set constructed with HDTM $\\gamma=.95$ .\n","The root document has 20 children in the hierarchy despite having 145 in - collection links .\n","The remaining 120 documents live deeper in the hierarchy because HDTM has determined that they are too specific to warrant a first level position , and have a better fit in one of the subtrees .\n","NOUN PHRASES:\n"," ['bibliography network data', 'had', 'low precision scores', 'judges', 'computer scientists', 'differentiate', 'topics', 'research paper titles', 'fig', 'dblphierarchy', 'shows', 'small portion', 'document hierarchy', 'bibliographic network data', 'set constructed', 'HDTM', 'root document', 'has', 'children', 'hierarchy', 'having', 'collection links', 'remaining', 'documents live deeper', 'hierarchy', 'HDTM', 'has determined', 'warrant', 'first level position', 'have', 'fit', 'subtrees']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{thebibliography} { 55 } % \\ makeat letter \\ providecommand \\@ifxundefined [ 1 ]\n","{%            }% \\providecommand \\@ifnum [ 1 ]\n","{% \\ifnum #1\\ expandafter \\@firstoftwo \\else \\ expandafter \\@secondoftwo \\fi }% \\\n","providecommand \\@ifx [ 1 ]\n","{% \\ifx #1\\ expandafter \\@firstoftwo \\else \\ expandafter \\@secondoftwo \\fi } %            %            %            %            %            %            %            %            \\@@href }%            % \\\n","providecommand \\@sanitize@url [ 0 ]\n","{\\ catcode `\\\\12\\catcode `\\            \\alpha\n","$, however, this would be ambiguous with the Dirichlet hyper-parameter\n","$            %            \\ \\emph {et~al.} ( 2010 ) \\\n","citename font { Malewicz } , \\citenamefont {Austern} , \\citenamefont {Bik} , \\ citename font { Dehnert } , \\citenamefont {Horn} , \\citenamefont {Leiser} ,\\ and\\ \\ citename font { Czajkowski } } ]\n","{ Malewicz 2010 } % \\Bibitem Open\n","\\bibfield {author} { \\bibinfo {author} {            ~\\ bibname font { Malewicz } } , \\bibinfo {author} { \\bibfnamefont {M.~H.} \\ \\ bibname font { Austern } } , \\bibinfo {author} { \\bibfnamefont {A.~J.}  \\ \\bibnamefont {Bik} } , \\bibinfo {author} { \\bibfnamefont {J.~C.}  \\ \\bibnamefont {Dehnert} } , \\ bibinfo { author } {            ~ \\bibnamefont {Horn} } , \\bibinfo {author} {            ~ \\bibnamefont {Leiser} } , \\ and\\ \\bibinfo {author} {            ~ \\bibnamefont {Czajkowski} } , \\ } in \\ \\href@noop {} {\\ emph { \\bibinfo {booktitle} { Proceedings of the 2010 ACM SIGMOD International Conference on Management of data } } } \\ (\\bibinfo {organization} { ACM } , \\ \\bibinfo {year} { 2010 } ) \\ pp.\\ \\bibinfo {pages} { 135-- 146 } \\\n","Bibitem Shut { NoStop } % \\bibitem [{\\citenamefont {Zaharia}  \\ \\emph {et~al.} ( 2010 ) \\ citename font { Zaharia } , \\citenamefont {Chowdhury} , \\citenamefont {Franklin} , \\ citename font { Shenker } , \\ and\\ \\citenamefont {Stoica} } ]\n","{ zaharia2010spark } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} {            ~\\ bibname font { Zaharia } } , \\bibinfo {author} { \\bibfnamefont {M.} ~ \\bibnamefont {Chowdhury} } , \\bibinfo {author} { \\bibfnamefont {M.~J.}  \\ \\bibnamefont {Franklin} } , \\ bibinfo { author } {            ~            } , \\ and \\ \\ bibinfo { author } {            ~            } , \\ } in \\ \\href@noop {}\n","{ \\emph {\\bibinfo {booktitle} { Proceedings of the 2nd USENIX conference on Hot topics in cloud computing } } } , \\ Vol.~\\bibinfo {volume} { 10 } \\ (\\bibinfo {year} { 2010 } ) \\ p.~\\bibinfo {pages} { 10 } \\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {Lee}  \\ \\emph {et~al.} ( 2012 ) \\citenamefont {Lee} , \\citenamefont {Lee} , \\citenamefont {Choi} , \\citenamefont {Chung} , \\ and \\ \\citenamefont {Moon} } ]\n","{ Lee : 2012 : PDP : 2094114.2094118 }\n","% \\ Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {K.-H.} \\ \\ bibname font { Lee } } , \\bibinfo {author} { \\bibfnamefont {Y.-J.}  \\ \\bibnamefont {Lee} } , \\bibinfo {author} { \\bibfnamefont {H.} ~ \\bibnamefont {Choi} } , \\bibinfo {author} { \\bibfnamefont {Y.~D.}  \\ \\bibnamefont {Chung} } , \\ and\\ \\bibinfo {author} { \\bibfnamefont {B.} ~ \\bibnamefont {Moon} } , \\ } \\ href {\\ doibase 10.1145 / 2094114.2094118 } { \\bibfield {journal} { \\bibinfo {journal} { SIGMOD Record } \\ }\\textbf {\\bibinfo {volume} { 40 } } , \\ \\bibinfo {pages} { 11 } (\\ bibinfo { year } { 2012 } ) } \\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {Zou}  \\ \\emph {et~al.} ( 2013 ) \\citenamefont {Zou} , \\citenamefont {Li} , \\citenamefont {Jiang} , \\citenamefont {Lin} , \\ citename font { Li } , \\ and\\ \\citenamefont {Chen} } ]\n","{ zou2013survey } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {Q.} ~\\ bibname font { Zou } } , \\bibinfo {author} { \\bibfnamefont {X.-B.}  \\ \\bibnamefont {Li} } , \\bibinfo {author} { \\bibfnamefont {W.-R.}  \\ \\bibnamefont {Jiang} } , \\ bibinfo { author } { \\bibfnamefont {Z.-Y.}  \\ \\bibnamefont {Lin} } , \\bibinfo {author} { \\bibfnamefont {G.-L.}  \\ \\bibnamefont {Li} } , \\ and\\ \\bibinfo {author} { 9999999884 ~ \\bibnamefont {Chen} } , \\ } \\ href@ noop {} {\\ bibfield { journal } { \\bibinfo {journal} { Briefings in bioinformatics } \\ , \\ \\ bibinfo { pages } { bbs088 } } ( \\bibinfo {year} { 2013 } ) } \\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {McCune}  \\ \\emph {et~al.} ( 2015 ) \\citenamefont {McCune} , \\citenamefont {Weninger} , \\ and\\ \\citenamefont {Madey} }]\n","{ McCune 2015 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999872 \\ \\ bibname font { McCune } } , \\bibinfo {author} { 9999999870 ~ \\bibnamefont {Weninger} } , \\ and \\ \\bibinfo {author} { 9999999867 ~ \\bibnamefont {Madey} } , \\ } \\href@noop {} {\\bibfield {journal} { \\bibinfo {journal} { ACM Computing Surveys } 9999999863 { 2015 } ) }\n","\\BibitemShut {NoStop} % 9999999861 \\ \\emph {et~al.} ( 2009 )\\\n","citename font { Mccallum } , \\citenamefont {Mimno} ,\\ and \\ \\ citename font { Wallach } } ]\n","{ Mccallum 2009} % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999856 ~\\ bibname font { Mccallum } } , \\bibinfo {author} { \\bibfnamefont {D.~M.}  \\ \\bibnamefont {Mimno} } , \\ and\\ \\bibinfo {author} { \\bibfnamefont {H.~M.}  \\ \\bibnamefont {Wallach} } ,\\ } in \\ \\href@noop {} {\\emph {\\bibinfo {booktitle} { Advances in neural information processing systems } }} \\ (\\bibinfo {year} { 2009 } )\\ pp. \\ \\ bibinfo { pages } { 1973 - - 1981 }\n","9999999847 % 9999999846 \\ \\emph {et~al.} ( 1998 ) \\citenamefont {Giles} , \\citenamefont {Bollacker} ,\\ and \\ \\ citename font { Lawrence } } ]\n","{ giles1998citeseer} % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {C.~L.} \\ \\ bibname font { Giles } } , \\bibinfo {author} { \\bibfnamefont {K.~D.} \\ \\ bibname font { Bollacker } } , \\ and\\ \\bibinfo {author} { 9999999836 ~\\ bibname font { Lawrence } } , \\ }in\\ \\href@noop {} {\\emph {\\bibinfo {booktitle} { Proceedings of the third ACM conference on Digital libraries } }} \\ (\\bibinfo {organization} { ACM } , \\ \\bibinfo {year} { 1998 } ) \\ pp.\\ \\bibinfo {pages} { 89-- 98 } \\\n","Bibitem Shut { No Stop } % \\bibitem [{\\citenamefont {Ley} ( 2002 ) }]\n","{ ley2002dblp } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999828 ~\\ bibname font { Ley } } , \\ }in\\ \\href@noop {} {\\emph {\\bibinfo {booktitle} { String Processing and Information Retrieval } }} \\ (\\bibinfo {organization} { Springer } , \\ \\ bibinfo { year } { 2002 } ) \\ pp.\\ \\bibinfo {pages} { 1 -- 10 }\n","\\BibitemShut {NoStop} % \\bibitem [{\\citenamefont {Tang}\n","\\ \\emph {et~al.} ( 2008 ) \\citenamefont {Tang} , \\citenamefont {Zhang} , \\citenamefont {Yao} , \\citenamefont {Li} , \\ citename font { Zhang } , \\ and\\ \\citenamefont {Su} }]\n","{ Tang 2008 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {J.} ~\\ bibname font { Tang } } , \\bibinfo {author} { \\bibfnamefont {J.} ~ \\bibnamefont {Zhang} } , \\bibinfo {author} { \\bibfnamefont {L.} ~ \\bibnamefont {Yao} } , \\bibinfo {author} { \\bibfnamefont {J.} ~ \\bibnamefont {Li} } , \\bibinfo {author} {\\ bibfname font { L. } ~ \\bibnamefont {Zhang} } , \\ and\\ \\bibinfo {author} {\\ bibfname font { Z. } ~ \\bibnamefont {Su} } , \\ }in\\ \\href {\\doibase 10.1145/1401890.1402008} { \\emph {\\bibinfo {booktitle} { SIGKDD } }} \\ (\\bibinfo {address} { New York , New York , USA } , \\ \\bibinfo {year} { 2008 } ) \\ p.\\ \\bibinfo {pages} { 990 }\\ Bibitem Shut { No Stop } % 9999999795 \\ and \\ \\ citename font { Croft } ( 1998 ) }]\n","{ Ponte 1998 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {J.~M.} \\ \\ bibname font { Ponte } } \\ and\\ \\bibinfo {author} { \\bibfnamefont {W.~B.} \\ \\ bibname font { Croft } } , \\ }in\\ \\href {\\doibase 10.1145/290941.291008}\n","{\\ emph {\\ bibinfo { booktitle } { SIGIR } } } \\ (\\bibinfo {address} { New York , New York , USA } , \\ \\bibinfo {year} { 1998 } ) \\ pp.\\ \\bibinfo {pages} { 275-- 281 }\\ Bibitem Shut { NoStop } % \\bibitem [{\\citenamefont {Faloutsos}  \\ \\emph {et~al.} ( 2013 )\\ citename font { Faloutsos } , \\citenamefont {Koutra} ,\\ and \\ \\ citename font { Vogelstein } }]\n","{ Faloutsos : 2013 dn } % \\\n","Bibitem Open \\bibfield {author} { 9999999781 { 9999999780 ~\\ bibname font { Faloutsos } } , \\bibinfo {author} { 9999999778 ~ \\bibnamefont {Koutra} } , \\ and \\ \\bibinfo {author} { \\bibfnamefont {J.~T.}  \\ \\bibnamefont {Vogelstein} } ,\\ } \\href@noop {} {\\bibfield {journal} { \\bibinfo {journal} { SDM } \\ , \\ \\ bibinfo { pages } { 162 } } ( \\bibinfo {year} { 2013 } ) } 9999999770 % 9999999769 ( 2002 ) }]\n","{ Mccallum : 2002fe } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { \\bibfnamefont {A.~K.} \\ \\ bibname font { McCallum } } , \\ }\\href@noop {} {\\bibfield {journal} { \\bibinfo {journal} { ( 2002 ) } \\ } (\\bibinfo {year} { 2002 } ) } 9999999762 % \\bibitem [{\\citenamefont {Chang}  \\ \\emph {et~al.} ( 2009 ) \\citenamefont {Chang} , \\citenamefont {Gerrish} , \\citenamefont {Wang} , \\citenamefont {Boyd-graber} ,\\ and \\ \\citenamefont {Blei} }] { Chang 2009 } % \\\n","Bibitem Open \\bibfield {author} { \\bibinfo {author} { 9999999752 ~\\ bibname font { Chang } } , \\bibinfo {author} { 9999999750 ~ \\bibnamefont {Gerrish} } , \\bibinfo {author} { 9999999747 ~ \\bibnamefont {Wang} } , \\bibinfo {author} { \\bibfnamefont {J.~L.}  \\ \\bibnamefont {Boyd-graber} } , \\ and\\ \\bibinfo {author} { \\bibfnamefont {D.~M.}  \\ \\bibnamefont {Blei} } , \\ } in \\ \\ href@noop {} {\\ emph { \\bibinfo {booktitle} { Advances in neural information processing systems } } }\\ ( \\bibinfo {year} { 2009 } ) \\ pp.\\ \\bibinfo {pages} { 288--296 }\\\n","Bibitem Shut { NoStop } %\n","\\end{thebibliography} %\n","NOUN PHRASES:\n"," ['thebibliography', 'ifxundefined', ']', '%', '[', ']', '%', '[', ']', '%', 'href', 'sanitize', '@', 'Dirichlet', 'et~al', 'citename', 'Malewicz', 'Austern', 'Bik', 'Dehnert', 'Horn', 'Leiser', 'Czajkowski', ']', 'Malewicz', 'author', 'author', 'Malewicz', 'author', 'M.~H', 'author', 'A.~J', 'Bik', 'author', 'J.~C', 'Dehnert', 'bibinfo', 'author', 'Horn', 'author', 'Leiser', 'author', 'Czajkowski', 'booktitle', 'Proceedings', 'ACM SIGMOD International Conference', 'Management', 'data', 'organization', 'ACM', 'year', 'pages', 'NoStop', 'Zaharia', 'et~al', 'citename', 'Zaharia', 'Chowdhury', 'Franklin', 'Shenker', 'Stoica', ']', 'zaharia2010spark', 'author', 'author', 'Zaharia', 'author', 'M.', 'Chowdhury', 'author', 'M.~J', 'Franklin', 'bibinfo', 'author', 'author', 'booktitle', 'Proceedings', 'USENIX conference', 'Hot topics', 'cloud computing', 'volume', 'year', 'pages', 'NoStop', 'Lee', 'et~al', 'Lee', 'Lee', 'Choi', 'Chung', 'Moon', ']', 'Lee', 'PDP', 'author', 'author', 'K.-H.', 'Lee', 'author', 'Lee', 'author', 'H.', 'Choi', 'author', 'Y.~D', 'Chung', 'author', 'B', 'Moon', '/', 'SIGMOD Record', 'volume', 'pages', 'year', 'NoStop', 'Zou', 'et~al', 'Zou', 'Li', 'Jiang', 'Lin', 'Li', 'Chen', ']', 'zou2013survey', 'author', 'author', 'Q', 'Zou', 'author', 'Li', 'author', 'Jiang', 'bibinfo', 'author', 'Z.-Y', 'Lin', 'author', 'Li', 'author', 'Chen', 'Briefings', 'bioinformatics', 'pages', 'year', 'NoStop', 'McCune', 'et~al', 'McCune', 'Weninger', 'Madey', ']', 'McCune', 'author', 'author', 'McCune', 'author', 'Weninger', 'author', 'Madey', 'ACM Computing Surveys', 'NoStop', '%', 'et~al', 'citename', 'Mccallum', 'Mimno', 'Wallach', ']', 'Mccallum', 'author', 'author', 'Mccallum', 'author', 'D.~M', 'Mimno', 'author', 'H.~M', 'Wallach', 'booktitle', 'Advances', 'neural information', 'processing', 'systems', 'year', 'pp', 'pages', '%', 'et~al', 'Giles', 'Bollacker', 'Lawrence', ']', 'giles1998citeseer', 'author', 'author', 'C.~L', 'Giles', 'author', 'K.~D', 'Bollacker', 'author', 'Lawrence', 'booktitle', 'Proceedings', 'third ACM conference', 'Digital libraries', 'organization', 'ACM', 'year', 'pages', 'No Stop', 'Ley', 'ley2002dblp', 'author', 'author', 'Ley', 'booktitle', 'String', 'Processing', 'Information Retrieval', 'organization', 'Springer', 'year', 'pages', 'NoStop', 'Tang', 'et~al', 'Tang', 'Zhang', 'Yao', 'Li', 'Zhang', 'Su', ']', 'Tang', 'author', 'author', 'J', 'Tang', 'author', 'J', 'Zhang', 'author', 'L.', 'Yao', 'author', 'J', 'Li', 'author', 'L.', 'Zhang', 'author', 'Z', 'Su', 'booktitle', 'SIGKDD', 'address', 'New York', 'New York', 'USA', 'year', 'pages', 'No Stop', '%', 'Croft', 'Ponte', 'author', 'author', 'J.~M', 'Ponte', 'author', 'W.~B', 'Croft', 'booktitle', 'SIGIR', 'address', 'New York', 'New York', 'USA', 'year', 'pages', 'NoStop', 'Faloutsos', 'et~al', 'citename', 'Faloutsos', 'Koutra', 'Vogelstein', ']', 'Faloutsos', 'dn', 'author', 'Faloutsos', 'author', 'Koutra', 'author', 'J.~T', 'Vogelstein', 'SDM', 'pages', 'year', '%', 'Mccallum', 'author', 'author', 'A.~K', 'McCallum', 'year', 'Chang', 'et~al', 'Chang', 'Wang', 'Boyd-graber', 'Blei', ']', 'Chang', 'author', 'author', 'Chang', 'author', 'Gerrish', 'author', 'Wang', 'author', 'J.~L', 'Boyd-graber', 'author', 'D.~M', 'Blei', 'booktitle', 'Advances', 'neural information', 'processing', 'systems', 'year', 'pages', 'NoStop', '%', 'thebibliography', '%']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Using either the traditional inference model from Sec. ~ \\ref{sec:inference} or the high - throughput distributed sampling algorithm from Sec. ~ \\ref{sec:distinference} we basically sample a full hierarchy $\\textbf{c}_d$ by sampling paths for each node $c_d$ except the root and the words in the document $z_{d,n}$ .\n","Given the state of the sampler at some time $t$ , \\ie,            and            , we iteratively sample each variable conditioned on the others as illustrated in~\\ref{sec:generating} .\n","NOUN PHRASES:\n"," ['Using', 'traditional inference model', 'Sec', '~', 'sec', 'inference', 'throughput', 'distributed sampling', 'algorithm', 'Sec', '~', 'sec', 'distinference', 'basically sample', 'full hierarchy', 'c', '_d', 'sampling', 'paths', 'root', 'words', 'd', 'Given', 'state', 'sampler', 'time', 'iteratively sample', 'variable conditioned', 'others', 'sec', 'generating']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Stochastic block models ( SBM ) are an alternative line of network clustering research that partitions nodes into communities in order to generatively infer link probabilities ~ \\citep{Holland1983} .\n","Several extensions to the original SBM have since been proposed ( for a survey see ~ \\citep{Goldenberg2009} ) .\n","One downside to block - model processes is that they assign probabilities to every possible edge requiring $\\mathcal{O}(N^2)$ complexity in every sampling iteration .\n","Furthermore , SBM methods typically are not concerned with topical / conceptual properties of the nodes .\n","NOUN PHRASES:\n"," ['Stochastic block models', 'SBM', 'alternative line', 'network', 'clustering', 'research', 'partitions', 'nodes', 'communities', 'order', 'generatively infer', 'link probabilities', 'Holland1983', 'Several extensions', 'original SBM', 'have', 'Goldenberg2009', 'downside', 'block', 'model', 'assign', 'probabilities', 'possible edge', 'requiring', 'O', 'N^2', 'complexity', 'sampling iteration', 'Furthermore', 'SBM methods', 'not concerned', 'topical / conceptual properties', 'nodes']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The nested version of the CRP extends the original analogy as follows :\n","At each table in the Chinese restaurant are cards with the name of another Chinese restaurant .\n","When a customer sits at a given table , he reads the card , gets up and goes to that restaurant , where he is reseated according to the CRP .\n","Each customer visits $L$ restaurants until he is finally seated and is able to eat .\n","This process creates a tree with a depth of $L$ and a width determined by the $\\gamma$ parameter .\n","This process has also been called the Chinese Restaurant Franchise because of this analogy ~ \\citep{Blei2003} .\n","NOUN PHRASES:\n"," ['nested version', 'CRP', 'extends', 'original analogy', 'follows', 'table', 'Chinese restaurant', 'cards', 'name', 'Chinese restaurant', 'customer', 'sits', 'given', 'table', 'reads', 'card', 'gets', 'goes', 'restaurant', 'CRP', 'customer', 'visits', 'L', 'restaurants', 'finally seated', 'eat', 'process', 'creates', 'tree', 'depth', 'L', 'width', 'determined', 'parameter', 'process', 'has', 'Chinese Restaurant Franchise', 'Blei2003']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\ centering \\ small { \\begin{tabular} {l| c c c c } & Wikipedia ( cat ) & Wikipedia ( article ) & CompSci Web site & Bib. Network \\\\ \\noalign{\\hrule height 1.5pt} documents & 609 & 1,957 ,268 & 1,078 & 4,713 \\\\ tokens & 5,570,868 & 1,316,879,537 & 771,309 & 43,345 \\\\ links & 2,014 & 44,673,134 & 63,052 & 8,485\\\\ vocabulary & 146,624 & 4,225,765 & 15,101 & 3,908 \\\\ \\noalign{\\hrule height 1.5pt}  \\end{tabular} }\n","\\caption{Comparison of most probable words in top document (in            ) and in root topic (in hLDA)}  \\label{tab:topdoc}  \\end{table}\n","NOUN PHRASES:\n"," ['centering', 'l| c c', 'c', 'c', 'Wikipedia', 'cat', 'Wikipedia', 'article', 'CompSci Web site', 'Bib', 'height', 'documents', ',268', 'height', 'Comparison', 'probable words', 'top document', 'root topic', 'hLDA', 'tab', 'topdoc', 'table']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Beginning with a document graph $G=\\{V,E\\}$ of documents $V$ and edges $E$ .\n","Each document is a collection of words , where a word $w$ is an item in a vocabulary .\n","The basic assumption of HD TM and similar models is that each document can be generated by probabilistically mixing words from among topics .\n","Distributions over topics are represented by $z$ , which is a multinomial variable with an associated set of distributions over words $p(w|z,\\beta)$ , where $\\beta$ is a Dirichlet hyper-parameter .\n","Document - specific mixing proportions are denoted by the vector $\\theta$ .\n","Parametric - Bayes topic models also include a $K$ parameter that denotes the number of topics , wherein $z$ is one of $K$ possible values and $\\theta$ is a $K$ - D vector .\n","HDTM , and other non-parametric Bayesian models , do not require a $K$ parameter as input .\n","Instead , in HD TM there exist $|V|$ topics , one for each graph node , and each document is a mixture of the topics on the path between itself and the root document .\n","NOUN PHRASES:\n"," ['Beginning', 'document graph', 'V', 'documents', 'V', 'edges', 'E', 'document', 'collection', 'words', 'word', 'item', 'basic assumption', 'HD TM', 'similar models', 'document', 'probabilistically mixing', 'words', 'topics', 'Distributions', 'topics', 'multinomial variable', 'associated set', 'distributions', 'words', 'p', 'w|z', 'Dirichlet hyper-parameter', 'Document', 'specific mixing proportions', 'vector', 'Parametric', 'Bayes topic models', 'also include', 'K', 'parameter', 'denotes', 'number', 'topics', 'K', 'possible values', 'K', 'D vector', 'HDTM', 'other non-parametric Bayesian models', 'do', 'not require', 'K', 'parameter', 'input', 'HD TM', 'exist', 'topics', 'graph node', 'document', 'mixture', 'topics', 'path', 'root document']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The processes that typically defines most hierarchical clustering algorithms can be made to fit in a probabilistic setting that build bottom - up hierarchies based on Bayesian hypothesis testing ~ \\citep{Heller2005} .\n","On the other hand , a lot of recent work uses Bayesian generative models to find the most likely explanation of observed text and links .\n","The first of these hierarchical generative models was hierarchical latent Dirichlet allocation ( hLDA ) .\n","In hLDA each document sits at a leaf in a tree of fixed depth $L$ as illustrated in Figure ~ \\ref{fig:rwhlda} .\n","Note that all non-leave nodes in Figure ~ \\ref{fig:rwhlda} are conceptual topics containing word distribution instead of a document .\n","Each document is represented by a mixture of multinomials along the path through the taxonomy from the document to the root .\n","Documents are placed at their respective leaf nodes stochasically using the nested Chinese restaurant process ( nCRP ) along side an LDA - style word sampling process .\n","NOUN PHRASES:\n"," ['processes', 'typically defines', 'hierarchical clustering', 'algorithms', 'fit', 'probabilistic setting', 'build', 'hierarchies', 'based', 'Bayesian hypothesis', 'testing', 'Heller2005', 'other hand', 'lot', 'recent work', 'uses', 'Bayesian generative models', 'find', 'likely explanation', 'observed text', 'links', 'hierarchical generative models', 'hierarchical latent Dirichlet allocation', 'hLDA', 'document', 'sits', 'leaf', 'tree', 'L', 'illustrated', 'fig', 'rwhlda', 'Note', 'non-leave nodes', 'fig', 'rwhlda', 'conceptual topics', 'containing', 'word distribution', 'document', 'document', 'mixture', 'multinomials', 'path', 'taxonomy', 'document', 'root', 'Documents', 'respective leaf nodes', 'stochasically using', 'nested Chinese restaurant process', 'nCRP', 'side', 'LDA', 'style word', 'sampling', 'process']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," NCRP is a recursive version of the standard Chinese Restaurant Process ( CRP ) , which progresses according to the following analogy :\n","An empty Chinese restaurant has an infinite number of tables , and each table has an infinite number of chairs .\n","When the first customer arrives he sits in the first chair at the first table with probability of 1 .\n","The second customer can then chose to sit at an occupied table with probability of $\\frac{n_i}{\\gamma+n-1}$ or sit at a new , unoccupied table with probability of $\\frac{\\gamma}{\\gamma+n-1}$ , where $n$ is the current customer , $n_i$ is the number of customers currently sitting at table $i$ , and $\\gamma$ is a parameter that defines the affinity to sit at a previously occupied table .\n","NOUN PHRASES:\n"," ['NCRP', 'recursive version', 'standard Chinese Restaurant Process', 'CRP', 'progresses according', 'following analogy', 'empty Chinese restaurant', 'has', 'infinite number', 'tables', 'table', 'has', 'infinite number', 'chairs', 'first customer', 'arrives', 'sits', 'first chair', 'first table', 'probability', 'second customer', 'then chose', 'sit', 'occupied table', 'probability', 'sit', 'unoccupied table', 'probability', 'current customer', 'number', 'customers', 'currently sitting', 'parameter', 'defines', 'affinity', 'sit', 'previously occupied', 'table']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," In the original LDA model , a single document mixture distribution is $p(w|\\theta) = \\sum_{i=1}^{K}\\theta_i p(w|z=i, \\beta_i)$ .\n","The process for generating a document is ( 1 ) choose a $\\theta$ of topic proportions from a distribution $p(\\theta|\\alpha)$ , where $p(\\theta|\\alpha)$ is a Dirichlet distribution ; ( 2 ) sample words from the mixture distribution $p(w |\\theta)$ for the $\\theta$ chosen in step 1 .\n","NOUN PHRASES:\n"," ['original LDA model', 'single document mixture distribution', 'p', '=', '^', 'K', 'w|z=i', 'process', 'generating', 'document', 'choose', 'topic proportions', 'distribution', 'p', 'p', 'Dirichlet distribution', 'sample words', 'mixture distribution', 'p', 'chosen', 'step']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Say we wish to find the RWR - probability between some node $u$ and some target node $k$ .\n","We model this by a random walker visiting document $u$ at time $t$ .\n","In the next time step , the walker chooses a document $v_i$ from among $u$ 's outgoing neighbors $\\{v|u\\rightarrow_{T} v\\}$ in the hierarchy $T$ uniformly at random .\n","In other words , at time $t + 1$ , the walker lands at node $v_i \\in \\{v|u\\rightarrow_{T} v\\}$ with probability $1/deg(u)$ , where $deg(u)$ is the outdegree of some document $u\\in G$ .\n","If at any time , there exists an edge to $k \\in \\{v|u\\rightarrow_{G} v\\}$ , { \\em i.e}, an edge between the current node            and the target node            in the original graph            , then we record the probability of that new path possibility for later sampling. Alg.~\\ref{alg:rwr} describes this process algorithmically .\n","This procedure allows for new paths from the root $r\\leadsto k$ to be probabilistically generated based on the current hierarchy effectively allowing for documents to migrate up , down and through the hierarchy during sampling .\n","NOUN PHRASES:\n"," ['Say', 'wish', 'find', 'RWR', 'probability', 'target', 'model', 'random walker', 'visiting', 'time', 'next time step', 'walker', 'chooses', 'outgoing', 'neighbors', 'T', 'T', 'random', 'other words', 'time', 't +', 'walker', 'lands', 'T', 'probability', 'u', 'deg', 'outdegree', 'time', 'exists', 'edge', 'k', 'G', 'edge', 'current node', 'target node', 'original graph', 'record', 'probability', 'new path possibility', 'later sampling', 'alg', 'rwr', 'describes', 'process', 'procedure', 'allows', 'new paths', 'root', 'probabilistically generated based', 'current hierarchy', 'effectively allowing', 'documents', 'migrate', 'hierarchy', 'sampling']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\Input{Path Probs.            , Current Node            , Target            , Weight            }  \\Globals{Graph            , Hierarchy            , Restart Prob.            }  \\Output{            } \\ Blank Line \\ForEach(\\tcc*[f]{\\small{child of            in            } }) { $v_i \\in \\T.\\Ch(u)$ } { \\If{            } { $w \\gets w + \\operatorname{log}\\left(\\frac{1-\\gamma}{\\operatorname{len}\\left(\\T.\\Ch(u)\\right)}\\right)$ \\; \\RWR(            ,            ,            ,            )\\tcc*[r]{\\small{Recur} } } } \\If(\\tcc*[f]{\\small{Edge            to            exists in            } }) { $u \\rightarrow_{\\G} k$ } { $P.\\Put(u, w)$ \\; } \\caption{Random Walk with Restart}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['Path Probs', 'Current Node', 'Target', 'Weight', 'Graph', 'Hierarchy', 'Restart Prob', 'f', ']', 'child', 'w', 'log', '[ r ]', 'Recur', 'f', ']', 'Edge', 'exists', 'k', 'w', 'Random', 'Walk', 'Restart', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," HLDA is an extension of LDA in which the topics are situated in a taxonomy $T$ of fixed depth $L$ .\n","The hierarchy is generated by the nested Chinese restaurant process ( nCRP ) which represents $\\theta$ as an $L$ - dimensional vector , defining an $L$ - level path through $T$ from root to document .\n","Because of the nCRP process , every document lives at a leaf and the words in each document are a mixture of the topic - words on the path from it to the root .\n","NOUN PHRASES:\n"," ['HLDA', 'extension', 'LDA', 'topics', 'T', 'fixed', 'L', 'hierarchy', 'nested Chinese restaurant process', 'nCRP', 'represents', 'L', 'dimensional vector', 'defining', 'L', 'level path', 'T', 'root', 'document', 'nCRP process', 'document', 'lives', 'leaf', 'words', 'document', 'mixture', 'topic', 'words', 'path', 'root']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The first term in Eq. ~ \\ref{eq:pathsample} is the probability of a given word based on the current path $\\mathbf{c}$ and topic assignment $\\mathbf{z}$ :\n","\\begin{equation}\n","\\begin{split}\n","p(\\mathbf{w}_{d} | \\mathbf{c}, \\mathbf{w}_{-d}, \\mathbf{z}, \\eta ) =&\\prod_{k=1}^{\\max(\\mathbf{z}_d)}{\n","\\frac{ \\Gamma(\\sum_w{ \\#[\\mathbf{c}_{-d,k} = c_{d,k}, \\mathbf{w}_{-d} = w] + W\\eta}) }\n","{ \\prod_w{ \\Gamma( \\#[\\mathbf{c}_{-d,k} = c_{d,k}, \\mathbf{w}_{-d} = w] + \\eta}) } } \\times \\\\\n","&\\quad\\hspace{1.1cm} \\frac{ \\prod_w{\\Gamma( \\#[\\mathbf{z} = k, \\mathbf{c}_{k} = c_{d,k}, \\mathbf{w} = w] + \\eta) } }\n","{ \\Gamma( \\sum{ \\#[\\mathbf{z} = k, \\mathbf{c}_{k} = c_{d,k}, \\mathbf{w} = w]} + W\\eta) },\n","\\label{eq:pwgivenc}\n","\\end{split}\n","\\end{equation}\n","NOUN PHRASES:\n"," ['first term', 'Eq', '~', 'eq', 'pathsample', 'probability', 'given', 'word', 'based', 'current path', 'c', 'topic', 'z', 'equation', 'split', 'p', '_', 'd', 'c', 'w', '_', '-d', 'z', '=', 'k=1', '^', 'z', '_d', 'c', '_', '-d', 'k', '= c_', 'd', 'k', 'w', '_', '-d', 'c', '_', '-d', 'k', '= c_', 'd', 'k', 'w', '_', '-d', 'z', '= k', 'c', '_', 'k', '= c_', 'd', 'k', 'w', 'z', '= k', 'c', '_', 'k', '= c_', 'd', 'k', 'w', '= w ]', 'eq', 'pwgivenc', 'split', 'equation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\noindent where            counts the elements of an array that satisfy the given condition, and            is the maximum depth of the current hierarchy state. The expression            counts: (ii)            , {\\em i.e.} , the number of words $w$ that do not appear in $d$ , for each ( i ) $\\mathbf{c}_{-d,k} = c_{d,k}$ , { \\em i.e.}, the number of paths to the current document            except those where the path length is            . The expression            counts: (iii)            , {\\em i.e.} , the number of words $w$ such that , ( ii ) $\\mathbf{c}_{k} = c_{d,k}$ , { \\em i.e.}, the words appear in document            and are situated at the end of a path of length            , where (i)            , {\\em i.e.} , $k$ is one of the topics in $\\mathbf{z}$ .\n","$W$ is the size of the vocabulary .\n","Eq. ~ \\ref{eq:pwgivenc} is adapted from the standard ratio of normalizing constants for the Dirichlet distribution ~ \\citep{Blei2010} .\n","NOUN PHRASES:\n"," ['counts', 'elements', 'array', 'satisfy', 'given', 'condition', 'maximum depth', 'current hierarchy state', 'expression counts', 'ii', 'number', 'words', 'do', 'not appear', 'i', 'c', '_', '-d', 'k', '= c_', 'd', 'k', 'number', 'paths', 'current document', 'path length', 'expression counts', 'iii', 'number', 'words', 'ii', 'c', '_', 'k', '= c_', 'd', 'k', 'words', 'appear', 'document', 'end', 'path', 'length', 'i', 'topics', 'z', 'W', 'size', 'Eq', '~', 'eq', 'pwgivenc', 'standard ratio', 'normalizing', 'constants', 'Blei2010']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0.\n"," 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The nCRP stochastic process could not be used to infer document hierarchies because the nCRP process forces documents to the leaves in the tree .\n","HDTM replaces nCRP with random walk with restart ( RWR ) ( which is also known as Personalized PageRank ( PPR ) ) ~ \\citep{Bahmani2010} .\n","In contrast , random walk with teleportation ( aka PageRank ) random walks by selecting a random starting point , and , with probability $(1-\\gamma)$ , the walker randomly walks to a new , connected location or chooses to jump to a random location with probability $\\gamma$ , where $\\gamma$ is called the jumping probability \\footnote{Most related works denote the jumping probability as            , however, this would be ambiguous with the Dirichlet hyper-parameter            .} .\n","NOUN PHRASES:\n"," ['nCRP stochastic process', 'infer', 'document hierarchies', 'nCRP process forces documents', 'leaves', 'tree', 'HDTM', 'replaces', 'random walk', 'restart', 'RWR', 'also known', 'Personalized PageRank', 'PPR', '~', 'Bahmani2010', 'contrast', 'random walk', 'teleportation', 'aka PageRank', 'random walks', 'selecting', 'random', 'starting', 'point', 'walker', 'randomly walks', 'connected', 'location', 'chooses', 'jump', 'random location', 'probability', 'jumping probability', 'related', 'works', 'denote', 'jumping probability', 'Dirichlet hyper-parameter']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\label{sec:generating}\n","Because a document hierarchy is a tree , each document - node can only have one parent .\n","Selecting a path for a document $d$ in the graph $G$ is akin to selecting a parent $u = Pa(d)$ ( and grandparents , etc. ) from $\\{d|u\\rightarrow_G d\\}$ in the document graph $G$ .\n","HDTM creates and samples from a probability distribution over each documents ' parent , where the probability of document $u$ being the parent of $d$ is defined as :\n","NOUN PHRASES:\n"," ['sec', 'generating', 'document hierarchy', 'tree', 'document', 'node', 'only have', 'parent', 'Selecting', 'path', 'G', 'selecting', 'parent', 'u = Pa', 'd', 'grandparents', 'document', 'graph', 'G', 'HDTM', 'creates', 'samples', 'probability distribution', 'documents', 'parent', 'probability', 'parent']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n"," 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Sampling word levels}\n","Given the current state of all the variables , the word sampler must first pick an assignment $z$ for word $n$ in document $d$ .\n","The sampling distribution of $z_{d,n}$ is\n","\\begin{equation}\n","\\begin{split}\n","p(z_{d,n} | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}, \\eta, \\gamma) & \\propto p(w_{d,n}, z_{d,n} | \\mathbf{c}, \\mathbf{z}_{-(d,n)}, \\mathbf{w}_{-(d,n)}, \\eta, \\gamma) \\\\\n","&= p(w_{d,n} | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}_{-(d,n)}, \\eta) p(z_{d,n} | \\mathbf{z}_{d,-n}, \\mathbf{c}, \\gamma)\n","\\end{split}\n","\\label{eq:wordsample}\n","\\end{equation}\n","where $\\mathbf{z}_{d,-n} = \\{z_{d,\\cdot}\\} \\setminus z_{d,n}$ and $\\mathbf{w}_{-(d,n)} = \\{w\\} \\setminus w_{d,n}$ .\n","The first term is a distribution over word assignments :\n","\\begin{equation}\n","\\begin{split}\n","p(w_{d,n} | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}_{-(d,n)}, \\eta) &\\propto \\#[\\mathbf{z}_{-(d,n)} = z_{d,n}, \\mathbf{c}_{z_{d,n}} = c_{d, z_{d,n}}, \\mathbf{w}_{-(d,n)} = w_{d,n}] + \\eta\n","\\end{split}\n","\\end{equation}\n","which is the $\\eta$ - smoothed frequency of seeing word $w_{d,n}$ in the topic at level $z_{d,n}$ in the path $c_d$ .\n","NOUN PHRASES:\n"," ['Sampling', 'word levels', 'Given', 'current state', 'variables', 'word sampler', 'first pick', 'word', 'sampling', 'distribution', 'd', 'equation', 'split', 'p', 'd', 'c', 'z', 'w', 'p', 'd', 'd', 'c', 'z', '_', 'd', 'n', 'w', '_', 'd', 'n', '= p', 'd', 'c', 'z', 'w', '_', 'd', 'n', 'p', 'd', 'z', '_', 'd', '-n', 'c', 'split', 'eq', 'wordsample', 'equation', 'z', '_', 'd', 'z_', 'd', 'd', 'w', '_', 'd', 'n', 'd', 'first term', 'distribution', 'word assignments', 'equation', 'split', 'p', 'd', 'c', 'z', 'w', '_', 'd', 'n', 'z', '_', 'd', 'n', '= z_', 'd', 'c', '_', 'z_', 'd', '= c_', 'd', 'z_', 'd', 'w', '_', 'd', 'n', '=', 'd', 'split', 'equation', 'smoothed', 'frequency', 'seeing', 'word', 'd', 'topic', 'd']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The variables needed by the Gibbs sampler are : $w_{d,n}$ , the $n$ th word in document $d$ ; $z_{d,n}$ , the assignment of the $n$ th word in document $d$ ; and $c_{d,z}$ , the topic corresponding to document at the $z$ th level .\n","The $\\theta$ and $\\beta$ variables are integrated out forming a collapsed Gibbs sampler .\n","NOUN PHRASES:\n"," ['variables', 'needed', 'Gibbs sampler', 'd', 'th word', 'd', 'assignment', 'th word', 'c_', 'd', 'z', 'topic', 'corresponding', 'document', 'th level', 'variables', 'forming', 'collapsed Gibbs sampler']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $f(w;d)$ is the frequency of term $w$ in a document $d$ before propagation , $f^{\\prime}(w;d)$ is the frequency of term $w$ in document $d$ after propagation , $c$ is a child page of $d$ in the sitemap $\\mathcal{T}$ , and $\\alpha$ is a parameter to control the mixing factor of the children .\n","This propagation algorithm assumes that the sitemap , $\\mathcal{T}$ , is constructed ahead of time .\n","NOUN PHRASES:\n"," ['f', 'w', 'd', 'frequency', 'term', 'propagation', 'd', 'frequency', 'term', 'propagation', 'child page', 'T', 'parameter', 'control', 'mixing', 'factor', 'children', 'propagation', 'algorithm assumes', 'sitemap', 'T', 'time']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," For the purposes of Web information retrieval language models are often used to normalize and smooth word distributions .\n","For illustration purposes , we apply a Dirichlet prior smoothing function ~ \\citep{Zhai2004} to smooth the term distribution where the $f^{\\prime}(w;d)$ from above is used in place of the usual $c(w;d)$ from the original Dirichlet prior smoothing function yielding :\n","NOUN PHRASES:\n"," ['purposes', 'Web information retrieval language models', 'often used', 'normalize', 'smooth', 'word distributions', 'illustration purposes', 'apply', 'Dirichlet', 'prior smoothing', 'Zhai2004', 'smooth', 'term distribution', 'd', 'above', 'place', 'c', 'w', 'd', 'original Dirichlet', 'prior smoothing', 'function yielding']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The sampling is performed in two parts : ( 1 ) given the current level allocations of each word $z_{d,n}$ sample the path $c_{d,z}$ , ( 2 ) given the current state of the hierarchy , sample $z_{d,n}$ .\n","In other words , we use the topic distributions to inform the path selections that make up the hierarchy , and the hierarchy topology to inform the topic distributions .\n","NOUN PHRASES:\n"," ['sampling', 'parts', 'given', 'current level allocations', 'word', 'd', 'd', 'z', 'given', 'current state', 'hierarchy', 'd', 'other words', 'use', 'topic distributions', 'inform', 'path selections', 'make', 'hierarchy', 'hierarchy topology', 'inform', 'topic distributions']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The second term is the distribution over levels\n","\\begin{equation}\n","\\begin{split}\n","p(z_{d,n} = k | \\mathbf{z}_{d,-n}, \\mathbf{c}, \\gamma)\n","= & \\left(\\prod_{j=1}^{k-1}{ \\frac{1-\\gamma}{\\mathrm{deg}_T(d_{j-1})} \\frac{\\#[\\mathbf{z}_{d,-n} > j]}{\\#[\\mathbf{z}_{d,-n} \\ge j]} }\\right) \\times \\\\\n","& \\frac{1-\\gamma}{\\mathrm{deg}_T(d_{k-1})} \\frac{\\#[\\mathbf{z}_{d,-n} = k]}{\\#[\\mathbf{z}_{d,-n} \\ge k]},\n","\\end{split}\n","\\label{eq:abuse}\n","\\end{equation}\n","where $\\#[\\cdot]$ is the number of elements in the vector which satisfy the given condition .\n","Eq. ~ \\ref{eq:abuse} abuses notation so that the product from $j=1$ to $k-1$ combines terms representing nodes at the $j$ th level in the path $\\mathbf{c}$ down to the parent of $d_k$ , and the second set of terms represents document $d_k$ at level $k$ .\n","The $>$ symbol in Eq. ~ \\ref{eq:abuse} refers to terms representing all ancestors of a particular node , and $\\ge$ refers to the ancestors of a node including itself .\n","NOUN PHRASES:\n"," ['second term', 'distribution', 'levels', 'equation', 'split', 'p', 'd', 'z', '_', 'd', '-n', 'c', '=', 'j=1', '^', '_T', 'z', '_', 'd', '> j ]', 'z', '_', 'd', '_T', 'z', '_', 'd', '= k ]', 'z', '_', 'd', 'split', 'eq', 'abuse', 'equation', 'number', 'elements', 'vector', 'satisfy', 'given', 'condition', 'Eq', '~', 'eq', 'abuse', 'abuses', 'notation', 'product', 'combines terms', 'representing', 'nodes', 'th level', 'c', 'parent', 'second set', 'terms', 'represents', 'symbol', 'Eq', '~', 'eq', 'abuse', 'refers', 'terms', 'representing', 'ancestors', 'particular node', 'refers', 'ancestors', 'node', 'including']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $d_t$ is the walkers current position at time $t$ , $\\mathrm{dep}_{T}(d)$ is the depth of $d$ in $T$ , and $\\mathrm{deg}_{T}(d_t)$ is the outdegree of $d_t$ in the hierarchy $T$ .\n","In other words , the probability of landing at $d$ is the product of the emission probabilities from each document in the path through $T$ from $r$ to $d$ .\n","NOUN PHRASES:\n"," ['d_t', 'walkers current position', 'time', 'dep', '_', 'T', 'd', 'depth', 'T', 'deg', '_', 'T', 'd_t', 'outdegree', 'T', 'other words', 'probability', 'landing', 'product', 'emission probabilities', 'document', 'path', 'T']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Specifically , the second term represents the probability of drawing the path $c_{d,k}$ to document $d$ at depth $k$ from the RWR process .\n","Recall that each node has an emission probability of $1/\\mathrm{deg}_T(d)$ , and a restart probability of $\\gamma$ .\n","The probability is defined recursively :\n","\\begin{equation}\n","\\begin{split}\n","p(&c_{d,k} | \\mathbf{c}_{-d}, c_{d,1:(k-1)})=\\prod_{k=0}{\\frac{1-\\gamma}{\\mathrm{deg}_T(d_k)}}\n","\\end{split}\n","\\end{equation}\n","In other words , the probability of reaching $d$ is equal to the probability of a random walker with restart probability $\\gamma$ being at document $d$ at time $k$ .\n","NOUN PHRASES:\n"," ['second term', 'represents', 'probability', 'drawing', 'path', 'd', 'k', 'document', 'RWR process', 'Recall', 'node', 'has', 'emission probability', 'deg', '_T', 'd', 'restart probability', 'probability', 'equation', 'split', 'p', 'c_', 'd', 'k', 'c', '_', '-d', 'd,1', 'k-1', 'k=0', '_T', 'd_k', 'split', 'equation', 'other words', 'probability', 'reaching', 'probability', 'random walker', 'restart probability', 'time']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\Globals{Vertices            , Hierarchy            , Restart Prob.            }  \\Input{Messages received containing path probabilities            }            \\ Blank Line\n","NOUN PHRASES:\n"," ['Vertices', 'Hierarchy', 'Restart Prob', 'Messages', 'received containing', 'path probabilities']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," A Web site $G$ can be viewed as a directed graph with Web pages as vertices $V$ and hyperlinks as directed edges $E$ between Web pages $v_x \\rightarrow v_y$ -- excluding inter-site hyperlinks .\n","In most cases , designating the Web site entry page as the root $r$ allows for a Web site to be viewed as a rooted directed graph .\n","Web site creators and curators purposefully organize the hyperlinks between documents in a topically meaningful manner .\n","As a result , Web documents further away from the root document typically contain more specific topics than Web documents graphically close to the root document .\n","NOUN PHRASES:\n"," ['Web site', 'G', 'directed graph', 'Web pages', 'vertices', 'V', 'hyperlinks', 'directed edges', 'E', 'Web pages', 'excluding', 'inter-site hyperlinks', 'cases', 'designating', 'Web site entry page', 'root', 'r', 'allows', 'Web site', 'rooted', 'directed graph', 'Web site creators', 'curators', 'purposefully organize', 'hyperlinks', 'documents', 'meaningful manner', 'result', 'Web documents', 'root document', 'specific topics', 'Web documents', 'root document']\n","NOUN LOCATIONS:\n"," [0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\author{Baoxu Shi}  \\email{bshi@nd.edu}  \\author{Tim Weninger}            \\ affiliation { % $^{\\ast\\dag}$ Department of Computer Science and Engineering , University of Notre Dame , Notre Dame , Indiana , USA }\n","% \\date{\\today}\n","NOUN PHRASES:\n"," ['Baoxu Shi', 'bshi @ nd.edu', 'Tim Weninger', '%', 'Department', 'Computer Science', 'Engineering', 'University', 'Notre Dame', 'Notre Dame', 'Indiana', 'USA', '%']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The mechanism behind Gibbs sampling , and other Markov Chain Monte Carlo methods , requires sequential sampling steps , and execution of each step depends on the results of the previous step making Gibbs samplers , and MCMC method in general , difficult to parallelize .\n","Approximate Distributed LDA ( AD - LDA ) is one attempt to find approximate , distributed solutions to the serial inference problem by dividing documents into $P$ parts where $P$ is the number of processors and initializes the topic distribution $z$ globally .\n","Then , for every Gibbs iteration , each processor samples $\\frac{1}{P}^\\textrm{th}$ of the dataset using the $z_P$ from last Gibbs sampling iteration .\n","When all processors are finished , a global synchronization is performed and $z$ is updated ~ \\citep{newman2007distributed} .\n","NOUN PHRASES:\n"," ['mechanism', 'Gibbs sampling', 'other Markov Chain Monte Carlo methods', 'requires sequential sampling', 'steps', 'execution', 'step', 'depends', 'results', 'previous step', 'making', 'Gibbs samplers', 'MCMC method', 'parallelize', 'Approximate Distributed LDA', 'AD', 'LDA', 'attempt', 'find', 'distributed solutions', 'serial inference problem', 'dividing', 'documents', 'P', 'parts', 'P', 'number', 'processors', 'initializes', 'topic distribution', 'Gibbs iteration', 'processor', 'samples', 'P', 'th', 'dataset', 'using', 'last Gibbs', 'sampling', 'iteration', 'processors', 'global synchronization']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\subsubsection{Sampling document paths}\n","The first Gibbs sampling step is to draw a path from each document to the root through the graph .\n","The sampling distribution for a path $c_d$ is\n","\\begin{equation}\n","\\begin{split}\n","p(c_{d} | \\mathbf{c}_{-d}, \\mathbf{z}, \\mathbf{w}, \\eta, \\gamma) & \\propto p(c_d, \\mathbf{w}_d | \\mathbf{c}_{-d}, \\mathbf{z}, \\mathbf{w}_{-d}, \\gamma, \\eta) \\\\\n","& = p(\\mathbf{w}_d | \\mathbf{c}, \\mathbf{z}, \\mathbf{w}_{-d}, \\eta) p(c_d | \\mathbf{c}_{-d}),\n","\\label{eq:pathsample}\n","\\end{split}\n","\\end{equation}\n","\\ noindent where $\\mathbf{w}$ is the count of terms in document $d$ , and $\\mathbf{w}_{-d}$ are the words without document $d$ .\n","This equation is an expression of Bayes ' theorem where the first term represents the probability of data given some choice of path from the root , and the second term represents the probability of selecting some path .\n","NOUN PHRASES:\n"," ['Sampling', 'document paths', 'first Gibbs', 'sampling', 'step', 'draw', 'path', 'document', 'root', 'graph', 'sampling', 'distribution', 'equation', 'split', 'p', 'c', '_', '-d', 'z', 'w', 'p', 'c_d', 'w', 'c', '_', '-d', 'z', 'w', '_', '-d', '= p', 'c', 'z', 'w', '_', '-d', 'p', 'c', '_', '-d', 'eq', 'pathsample', 'split', 'equation', 'w', 'count', 'terms', 'w', '_', '-d', 'words', 'equation', 'expression', 'Bayes', 'theorem', 'first term', 'represents', 'probability', 'data', 'given', 'choice', 'path', 'root', 'second term', 'represents', 'probability', 'selecting', 'path']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{enumerate} \\ item\n","Each document $d \\in G$ is assigned a topic $\\beta_d \\sim $ Dir ( $\\eta$ ) :\n","\\ item For each document $d \\in G$ : \\begin{enumerate} \\ item Draw a path $\\mathbf{c}_d \\sim$ RWR ( $\\gamma$ )\n","\\ item\n","Draw an $L$ - dim topic proportion vector $\\theta$ from Dir ( $\\alpha$ ) , where $L = $ len $(\\mathbf{c}_d)$ . \\ item For each word $n \\in \\{1,\\ldots,N\\}$ : \\begin{enumerate} \\ item\n","Choose topic $z_{d,n}|\\theta \\sim$ Mult ( $\\theta_d$ ) .\n","\\ item\n","Choose word $w_{d,n} | \\{z_{d,n}, \\mathbf{c}_d, \\boldsymbol\\beta\\} \\sim$ Mult ( $\\beta_{\\mathbf{c}_d, z_{d,n} }$ ) , where $\\beta_{\\mathbf{c}_d, z_{d,n}}$ is the topic in the $z$ th position in $\\mathbf{c}_d$ .\n","\\end{enumerate}\n","\\end{enumerate}  \\end{enumerate}\n","NOUN PHRASES:\n"," ['enumerate', 'document', 'Dir', 'document', 'enumerate', 'c', 'RWR', 'L', 'dim topic proportion', 'Dir', 'L =', 'c', '_d', 'word', 'enumerate', 'd', 'Mult', 'd', 'z_', 'd', 'c', '_d', 'Mult', 'c', '_d', 'z_', 'd', 'c', '_d', 'z_', 'd', 'topic', 'th position', 'c', '_d', 'enumerate', 'enumerate', 'enumerate']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ForPar(\\tcc*[f]{\\small{each document in parallel} }) { vertex $d_k \\in V$ } { \\ForEach{            } { \\Sendmsg(            ) \\tcc*[f]{\\small{Send local            ,            to node            } } \\\\ } \\If{            } { \\ForEach{            } { $d_k.n \\gets u.n$ \\\\ $d_k.z \\gets u.z$ \\\\ } } } \\caption{Path-Global Update}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['document', 'parallel', '[ f ]', 'Send', 'node', 'Path-Global Update', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ noindent where $C$ is the distribution over all terms in $V$ , $\\mu$ is the smoothing parameter , and the length is modified by the propagation algorithm to be $ |d|^{\\prime} = (1+\\alpha)|d|$ .\n","NOUN PHRASES:\n"," ['C', 'distribution', 'terms', 'V', 'smoothing', 'parameter', 'length', 'propagation algorithm', '=', '|d|']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As a result of the upward propagation $p_{\\mu}$ function , the root document ( Web site entry page ) will contain all of the words from all of the Web pages in the Web site with different non-zero probabilities .\n","The most probable words are those that occur most frequently and most generally across all documents , and are thus propagated the most .\n","NOUN PHRASES:\n"," ['result', 'upward propagation', 'function', 'root document', 'Web site entry page', 'contain', 'words', 'Web pages', 'Web site', 'different non-zero probabilities', 'probable words', 'occur', 'documents', 'thus propagated']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The distributed HDTM inference algorithm is similar to procedural HD TM .\n","We do not detail the entire distributed HD TM inference algorithm in this paper ; however , the source code is referenced in the Section ~ \\ref{sec:conclusions} .\n","To fit HD TM to the vertex - programming model , changes in sampling sequence and attention to global synchronization were required .\n","Firstly , random walk with restart must be executed during each Gibbs iteration so that every visited node can have a random walk probability .\n","Next , every node gathers $\\mathbf{w}_{-d}$ , $\\mathbf{z}$ , and $\\mathbf{c}$ separately and decides their new path $\\mathbf{c}$ according to Eq ~ \\ref{eq:pathsample} and shown in Alg ~ \\ref{alg:dist_rwr} .\n","After a path is sampled , each node will pick sample assignments $z$ for each word $n$ across all documents / nodes $d$ in parallel according to Eq ~ \\ref{eq:wordsample} .\n","A global synchronization step is required so that each document / node can update the number of words $n$ and the topic assignments $z$ globally ; fortunately , because the topics and words are sampled according to the information provided in the { \\em path} from root            to the each document            (document            at level            ), it suffices to update the nodes on the path from            to            instead of an actual global update to all nodes in            . This vertex-programming based path-global update function is shown in Alg~\\ref{alg:dist_upate} .\n","Furthermore , this update is executed during the { \\em synchronization barrier } , which is built - in to most vertex - programming frameworks , is highly optimized , and does not lock the global system any more than the synchronization barrier already does .\n","NOUN PHRASES:\n"," ['distributed HDTM inference algorithm', 'procedural HD TM', 'do', 'not detail', 'entire distributed HD TM inference algorithm', 'paper', 'source code', 'sec', 'conclusions', 'fit', 'HD TM', 'vertex', 'programming model', 'changes', 'sampling', 'sequence', 'attention', 'global synchronization', 'random walk', 'restart', 'Gibbs iteration', 'visited node', 'have', 'random walk probability', 'node gathers', 'w', '_', '-d', 'z', 'c', 'decides', 'new path', 'c', 'according', 'eq', 'pathsample', 'shown', 'alg', 'dist_rwr', 'path', 'node', 'pick', 'sample assignments', 'word', 'documents', '/', 'parallel according', 'eq', 'wordsample', 'global synchronization step', 'document', 'node', 'update', 'number', 'words', 'topic', 'assignments', 'topics', 'words', 'information', 'provided', 'root', 'document', 'document', 'level', 'suffices', 'update', 'nodes', 'path', 'actual global update', 'nodes', 'vertex-programming based', 'path-global update function', 'alg', 'dist_upate', 'update', 'programming frameworks', 'highly optimized', 'does', 'not lock', 'global system', 'synchronization barrier', 'already does']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Hyperparameters also play an important role in the shape and character of the hierarchy .\n","The $\\alpha$ parameter affects the smoothing on topic distributions , and the $\\eta$ parameter affects the smoothing on word distributions .\n","The $\\gamma$ parameter is perhaps the most important parameter because it affects the depth of the hierarchy .\n","Specifically , if $\\gamma$ is set to be large ( { \\em e.g.},            ) then resulting hierarchy is shallow. Low values ({\\em e.g.} , $\\gamma = 0.05$ ) may result in deep hierarchies , because there is a smaller probabilistic penalty for each step that the random walker takes .\n","NOUN PHRASES:\n"," ['Hyperparameters', 'also play', 'important role', 'shape', 'character', 'hierarchy', 'parameter', 'affects', 'smoothing', 'topic distributions', 'parameter', 'affects', 'smoothing', 'word distributions', 'parameter', 'important parameter', 'affects', 'depth', 'hierarchy', 'e.g', 'then resulting', 'hierarchy', 'Low values', 'e.g', 'result', 'deep hierarchies', 'probabilistic penalty', 'step', 'random walker', 'takes']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\Globals{Vertices            , Hierarchy            , Restart Prob.            }  \\Input{Messages received containing path probabilities            }            \\ Blank Line\n","NOUN PHRASES:\n"," ['Vertices', 'Hierarchy', 'Restart Prob', 'Messages', 'received containing', 'path probabilities']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Algorithmically , HD TM infers document hierarchies by drawing paths $\\mathbf{c}_d$ from the $r$ to the document $d$ .\n","Thus , the documents are drawn from the following generative process :\n","NOUN PHRASES:\n"," ['HD TM infers document hierarchies', 'drawing', 'c', '_d', 'Thus', 'documents', 'following', 'generative process']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," notre & computer & computer \\\\ dame & engineering & engineering \\\\ engineering & science & science \\\\ university & notre & notre \\\\ science & dame & dame \\\\ computer & department & department \\\\ \t\\noalign{\\hrule height 1.5pt}  \\end{tabular} } \\caption{Comparison of most probable words in top document (in            ), and in root topic of hLDA and HDTM}  \\label{tab:inh_lm}  \\end{table}\n","NOUN PHRASES:\n"," ['notre', 'computer', 'engineering', 'science', 'notre', 'dame', 'department', 'height', 'Comparison', 'probable words', 'top document', 'root topic', 'hLDA', 'HDTM', 'tab', 'inh_lm', 'table']\n","NOUN LOCATIONS:\n"," [1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{table} \\ centering \\ small { \\begin{tabular} { c | c | c } $p_{\\mu}~\\alpha = .5$ & hLDA $\\gamma=1$ & HD TM $\\gamma=0.95$ \\\\ \\noalign{\\hrule height 1.5pt}\n","NOUN PHRASES:\n"," ['centering', 'c | c | c', 'hLDA', 'HD TM', 'height']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," This random walker function assigns higher probabilities to parents that are at a shallower depth than those at deeper positions .\n","This is in line with the intuition that flatter hierarchies are easier for human understanding than deep hierarchies ~ \\citep{Ho2012} .\n","Simply put , the restart probability $\\gamma$ controls how much resistance there is to placing a document at successive depths .\n","NOUN PHRASES:\n"," ['random walker function', 'probabilities', 'parents', 'depth', 'positions', 'line', 'intuition', 'flatter hierarchies', 'human', 'understanding', 'deep hierarchies', 'Ho2012', 'Simply', 'put', 'restart probability', 'controls', 'much resistance', 'placing', 'document', 'successive depths']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\ForPar(\\tcc*[f]{each document in parallel} ) { vertex $d_k \\in V$ } { \\If{            } { $c_{d,k} \\gets c_{d,k} + \\sum(m)$ \\tcc*[f]{\\small{Add path-probs from incoming message            } } \\\\ \\ForEach{            } { \\tcc*[f]{\\small{Send prob message to children of            } } \\\\ \\ Send msg ( $child$ , $c_{d,k} + \\operatorname{log}\\left(\\frac{1-\\gamma}{\\operatorname{len}\\left(\\T.\\Ch(d_k)\\right)}\\right)$ ) \\\\ } } } \\caption{Distributed Random Walk with Restart}  \\end{algorithm}\n","NOUN PHRASES:\n"," ['document', 'c_', 'd', 'k', 'd', 'k', 'm', 'Add path-probs', 'incoming', 'message', 'Send', 'message', 'children', 'child', 'd', 'k', 'log', 'd_k', 'Distributed Random Walk', 'Restart', 'algorithm']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Recall that HDTM uses the $\\gamma$ parameter to balance the weight of topological links and topical similarities .\n","Hence when specific nodes like \\textsf{Barack Obama} are chosen as the root , higher $\\gamma$ will reduce Jaccard coefficient for high certainty nodes .\n","This is because higher $\\gamma$ values favor topology rather than content during the path and parent selection process .\n","NOUN PHRASES:\n"," ['Recall', 'HDTM', 'uses', 'parameter', 'balance', 'weight', 'topological links', 'topical similarities', 'Hence', 'specific nodes', 'Barack Obama', 'root', 'reduce', 'Jaccard coefficient', 'high certainty nodes', 'values', 'favor', 'topology', 'content', 'path', 'parent selection process']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n"," 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Recall that the HDTM , in its most basic form , generates a hierarchy by picking the best parent for each node / document ( except the root ) .\n","In the iterative Gibbs sampling process it is an almost certainty that a given node will pick different parents during different iterations .\n","For example , say node $d$ has two parents $x$ and $y$ , it is possible that during iterations 1 -- 5 node $d$ samples node $x$ as its parent , but then in iterations 6 -- 20 node $d$ samples node $y$ to be its parent .\n","Two questions come to mind : 1 ) which parent should be ultimately picked for node $d$ in the final output graph ?\n","and 2 ) what can the distribution of samples say about the certainty of our inferred graph ?\n","NOUN PHRASES:\n"," ['Recall', 'HDTM', 'basic form', 'generates', 'hierarchy', 'picking', 'parent', 'node', 'document', 'root', 'iterative Gibbs', 'sampling', 'process', 'certainty', 'given', 'node', 'pick', 'different parents', 'different iterations', 'example', 'say', 'has', 'parents', 'iterations', 'samples', 'parent', 'iterations', 'samples', 'parent', 'questions', 'come', 'mind', 'parent', 'ultimately picked', 'final output graph', 'distribution', 'samples', 'say', 'certainty', 'inferred graph']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Table ~ \\ref{tab:deltacon} shows that the hierarchy inferred by HDTM is indeed most similar to the Wikipedia category graph , followed by the random article hierarchy .\n","Recall that DeltaCon looks at graph topology similarity , thus the HDTM hierarchy is expected to more topologically similar to another hierarchy ( even if random ) than the article graph .\n","These results demonstrate that HDTM can identify and preserve critical topological features when inferring a hierarchy from the graph .\n","As usual , an increase in the number of LDA topics increases the goodness of fit score ( in this case measured by DeltaCon instead of likelihood ) .\n","Interestingly , the $\\gamma$ parameter and the selection of different roots did not significantly influence DeltaCon score .\n","To understand why this is , recall that different root - nodes and $\\gamma$ values result in different classification perspectives , but the different perspectives are still subsets of the same category graph with similar topological properties .\n","Hence they should have similar scores using the DeltaCon metric .\n","NOUN PHRASES:\n"," ['tab', 'deltacon', 'shows', 'hierarchy', 'inferred', 'HDTM', 'Wikipedia category graph', 'followed', 'random article hierarchy', 'Recall', 'DeltaCon', 'looks', 'graph topology similarity', 'HDTM hierarchy', 'hierarchy', 'random', 'article graph', 'results', 'demonstrate', 'HDTM', 'identify', 'preserve', 'critical topological features', 'inferring', 'hierarchy', 'graph', 'increase', 'number', 'LDA topics', 'increases', 'goodness', 'fit score', 'case', 'measured', 'DeltaCon', 'likelihood', 'parameter', 'selection', 'different roots', 'did', 'not significantly influence', 'DeltaCon score', 'understand', 'recall', 'different root', 'nodes', 'values', 'result', 'different classification perspectives', 'different perspectives', 'subsets', 'same category graph', 'similar topological properties', 'Hence', 'have', 'similar scores', 'using', 'DeltaCon']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," for each document $d$ that belongs to a set of categories $C_d$ , and where $C_{pa}$ is the union of the corresponding categories of the nodes in $d$ 's ancestors $\\mathbf{c}_d$ .\n","The Jaccard coefficient is useful here because it gives an quantitative measure describing how well the inferred hierarchical structure matches with the human - annotated categorical structure of Wikipedia .\n","NOUN PHRASES:\n"," ['document', 'belongs', 'set', 'categories', 'C_d', 'C_', 'pa', 'union', 'corresponding categories', 'nodes', 'ancestors', 'c', '_d', 'Jaccard coefficient', 'gives', 'quantitative measure', 'describing', 'inferred hierarchical structure matches', 'annotated categorical structure', 'Wikipedia']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\centering \\ small { \\begin{tabular} { c | c c c } & CompSci Web site & Wikipedia ( Cat ) & Bib. Network\n","\\\\ \\noalign{\\hrule height 1.5pt} HDTM $\\gamma=0.05$ & - 1.8570 & - 148.071 & - 0.4758 \\\\ HDTM $\\gamma=0.95$ & - 9.2412 & - 148.166 & - 0.5183 \\\\ HLDA\n","$\\gamma=1.0$ & - 8.5306 & - 50.6732 & - 8.5448 \\\\ Topic Block $\\gamma=1.0$ & \\textbf{-0.2404} & - 2.9827 &            \\\\\n","TSSB $k=10$ & - 0.5689 & \\textbf{-0.0336} & - 0.4655 \\\\ fsLDA & - 48.9149 & - 149.622 & - 0.6602 \\\\ \\noalign{\\hrule height 1.5pt}  \\end{tabular} } \\caption{Log likelihood results of the best sample from among 5,000 Gibbs iterations. Values are            . Higher values are better. Best results are in bold.}  \\label{tab:LLResults}  \\end{table}\n","NOUN PHRASES:\n"," ['c | c c c', 'CompSci Web site', 'Wikipedia', 'Cat', 'Bib', 'height', 'HDTM', '-0.2404', 'k=10', '-0.0336', 'height', 'Log', 'likelihood', 'results', 'sample', 'Gibbs iterations', 'Values', 'values', 'results', 'tab', 'LLResults']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Figure ~ \\ref{fig:wiki_certainty_density} shows the probability density functions for the certainties in HD TMs parent sampling process .\n","In the figures on both left and right we find that the probability densities appear to be polynomial distributions with interesting plateaus that end just before the 50 \\%, 66\\%, and 75\\% certainty scores corresponding to the near-certain (but not perfectly-certain) scores for nodes with indegree values of 2, 3, and 4 respectively. In these results, and others not displayed in this paper, we find that when the root is a general document like \\textsf{Science} , then changing $\\gamma$ does not affect the certainty distribution .\n","Given a relatively specific root like \\textsf{Barack Obama} , larger $\\gamma$ values increase certainty overall .\n","NOUN PHRASES:\n"," ['Figure', '~', 'fig', 'wiki_certainty_density', 'shows', 'probability density functions', 'certainties', 'HD TMs parent', 'sampling', 'process', 'figures', 'left', 'find', 'probability', 'densities appear', 'polynomial distributions', 'interesting plateaus', 'end', '%', '%', '% certainty', 'scores corresponding', 'scores', 'nodes', 'indegree values', 'results', 'others', 'not displayed', 'paper', 'find', 'root', 'general document', 'Science', 'then changing', 'does', 'not affect', 'certainty distribution', 'Given', 'specific root', 'Barack Obama', 'values', 'increase', 'certainty']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," As discussed earlier , measuring topical similarity in hierarchies can be precarious because , in many cases , documents that are correctly situated under a topically unrelated parent may still have a strong contextual association with the parent document that outweighs the topical / language - oriented similarity as determined by the $\\gamma$ parameter .\n","As an example , consider the Wiki - articles \\textsf{Honest Leadership and Open Government Act} and \\textsf{Alexi Giannoulias} : even though these two articles are topically dissimilar , HDTM parameterized with a high $\\gamma$ value is likely to place both articles as children of the hierarchy 's root \\textsf{Barack Obama} because the high $\\gamma$ values weigh the topological link as more important that the documents ' inferred topicality .\n","If , on the other hand , HDTM was parameterized with a very low $\\gamma$ value , then \\textsf{Alexi Giannoulias} is more likely to be situated with other state senators , and the \\textsf{Honest Leadership and Open Government Act} is more likely to be situated with other legislation .\n","NOUN PHRASES:\n"," ['discussed', 'measuring', 'topical similarity', 'hierarchies', 'many cases', 'documents', 'correctly situated', 'unrelated parent', 'still have', 'strong contextual association', 'parent document', 'outweighs', 'topical / language', 'oriented', 'similarity', 'determined', 'parameter', 'example', 'consider', 'Wiki', 'articles', 'Honest Leadership', 'Open Government Act', 'Alexi Giannoulias', 'articles', 'HDTM', 'parameterized', 'value', 'place', 'articles', 'children', 'hierarchy', 'Barack Obama', 'values', 'weigh', 'topological link', 'documents', 'inferred topicality', 'other hand', 'HDTM', 'value', 'Alexi Giannoulias', 'other state senators', 'Honest Leadership', 'Open Government Act', 'other legislation']\n","NOUN LOCATIONS:\n"," [0. 0. 0. ... 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n","            \\centering \\begin{tabular} { r | c } & DeltaCon \\\\ \\noalign{\\hrule height 1.5pt} HD TM ( $r$ = \\textsf{Science} , $\\gamma=0.05$ ) & 0.046852 \\\\ HD TM ( $r$ = \\textsf{Science} , $\\gamma=0.95$ ) & 0.046851 \\\\ HD TM ( $r$ = \\textsf{Barack Obama} , $\\gamma=0.05$ ) & 0.046857 \\\\ HD TM ( $r$ = \\textsf{Barack Obama} , $\\gamma=0.95$ ) & 0.046856 \\\\ Random Article Hierarchy & 0.046700 \\\\ Article Graph & 0.044208 \\\\ LDA ( $k=10$ ) & 0.026770 \\\\ LDA ( $k=50$ ) & 0.037949 \\\\ \\end{tabular}  \\caption{Comparison of the Wikipedia category graph to other generated hierarchies. DeltaCon scoring means higher is better. The random article hierarchy is generated by randomly picking one parent among all possible parents, \\ie, references, for each article. The differences in DeltaCon scores show that HDTM can preserve crucial connectivity information when constructing a new hierarchical structure from the original graph.}  \\label{tab:deltacon}  \\end{table}\n","NOUN PHRASES:\n"," ['r | c', 'height', 'HD TM', 'r', 'Science', 'r', 'Science', 'r', 'Barack Obama', 'r', 'Barack Obama', 'k=10', 'k=50', 'Comparison', 'Wikipedia category graph', 'other generated hierarchies', 'DeltaCon', 'scoring', 'means', 'random article hierarchy', 'randomly picking', 'parent', 'possible parents', 'references', 'article', 'differences', 'DeltaCon scores', 'show', 'HDTM', 'preserve', 'crucial connectivity information', 'constructing', 'new hierarchical structure', 'original graph', 'tab', 'deltacon', 'table']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," $$\n","\\mathcal{L}^{(t)} = \\log p\\left(\\textbf{c}_{1:D}^{(t)}, \\textbf{z}_{1:D}^{(t)}, \\textbf{w}_{1:D} | \\gamma, \\eta \\right).\n","$$\n","NOUN PHRASES:\n"," ['L', '^', 't', 'c', '_', 'D', '^', 't', 'z', '_', 'D', '^', 't', 'w', '_', 'D']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure} \\ centering \\ subfigure [ CompSci Web site ] { \\includegraphics[width=.65\\textwidth]{fig/csturk}  \\label{fig:csturk} } \\ subfigure [ Wikipedia ] { \\includegraphics[width=.65\\textwidth]{fig/wikiturk}  \\label{fig:wikiturk} } \\ subfigure [ Bib. Network ] { \\includegraphics[width=.65\\textwidth]{fig/dblpturk}  \\label{fig:dblpturk} } \\caption{The model precision for five models on three document-graph collections. Higher is better.            and            represents statistical significance from HDTM            and            respectively.}  \\label{fig:turks}  \\end{figure}\n","NOUN PHRASES:\n"," ['figure', '[', 'fig/csturk', 'fig', 'csturk', '[', 'fig/wikiturk', 'fig', 'wikiturk', 'Network ]', '[', 'fig/dblpturk', 'fig', 'dblpturk', 'model precision', 'models', 'document-graph collections', 'represents', 'statistical significance', 'HDTM', 'fig', 'turks', 'figure']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," Applying those lessons to our experiments , recall that HDTM has as many topics as there are documents , and non-root document topics are mixtures of the topics on the path to the root .\n","Also recall that HLDA , TopicBlock and TSSB all generate a large number of latent topics .\n","In HLDA and TopicBlock , there are infinitely many topics / tables in the nCRP ; and practically speaking , the number of topics in the final model is much larger than the number of documents ( conditioned on the $\\gamma$ parameter ) .\n","In TSSB , the topic generation is said to be an interleaving of two stick breaking processes ; practically , this generates even larger topic hierarchies .\n","The fsLDA algorithm has as many topics as there are in hLDA , however , the fsLDA hierarchy is not redrawn during Gibbs iterations to fit the word distributions resulting in a lower likelihood .\n","Simply put , the number of topics in HDTM and fsLDA $=|V|$ $\\ll$ hPAM , hLDA and TopicBlock $\\ll$ TSSB .\n","NOUN PHRASES:\n"," ['Applying', 'lessons', 'experiments', 'recall', 'HDTM', 'has', 'many topics', 'documents', 'non-root document topics', 'mixtures', 'topics', 'path', 'root', 'Also recall', 'HLDA', 'TopicBlock', 'TSSB', 'generate', 'large number', 'latent topics', 'HLDA', 'TopicBlock', 'many topics / tables', 'nCRP', 'speaking', 'number', 'topics', 'final model', 'number', 'documents', 'conditioned', 'parameter', 'TSSB', 'topic generation', 'interleaving', 'stick breaking', 'processes', 'generates', 'topic hierarchies', 'fsLDA algorithm', 'has', 'many topics', 'hLDA', 'fsLDA hierarchy', 'Gibbs iterations', 'fit', 'word distributions', 'resulting', 'likelihood', 'Simply', 'put', 'number', 'topics', 'HDTM', 'hPAM', 'hLDA', 'TopicBlock', 'TSSB']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The collected `` samples '' from the Markov chain are full hierarchies constructed from the selection of a path for each node $c_d$ and a word for each document $z_{d,n}$ .\n","Therefore each sampled hierarchy contains one estimation about position of each document in the hierarchy and the position of each word in a document .\n","For a given sampled hierarchy , we can assess the goodness of the hierarchy by measuring the log probability of that hierarchy and the observed words conditioned on the hyperparameters :\n","NOUN PHRASES:\n"," ['collected', 'samples', 'Markov chain', 'full hierarchies', 'constructed', 'selection', 'path', 'word', 'document', 'd', 'Therefore', 'sampled', 'hierarchy', 'contains', 'estimation', 'position', 'document', 'hierarchy', 'position', 'word', 'document', 'given', 'sampled hierarchy', 'assess', 'goodness', 'hierarchy', 'measuring', 'log probability', 'hierarchy', 'observed words', 'conditioned', 'hyperparameters']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The intruder detection tasks described above were offered on Amazon Mechanical Turk .\n","No specialized training is expected of the judges .\n","50 tasks were created for each data set and model combination ; each user was presented with 5 tasks at a time at a cost of \\$0.07 per task. Each task was evaluated by 15 separate judges. In order to measure the trustworthiness of a judge, 5 easy tasks were selected, {\\em i.e.} , groupings with clear intruders , and gold - standard answers were created .\n","Judges who did not answer 80 \\ % of the gold - standard answers correctly are thrown out and not paid .\n","In total the solicitation attracted 31,494 judgments , across 14 models of 50 tasks each .\n","Of these , 13,165 judgments were found to be from trustworthy judges .\n","NOUN PHRASES:\n"," ['intruder detection tasks', 'described', 'Amazon Mechanical Turk', 'No specialized training', 'judges', 'tasks', 'data set', 'model combination', 'user', 'tasks', 'time', 'cost', 'task', 'task', 'separate judges', 'order', 'measure', 'trustworthiness', 'judge', 'easy tasks', 'groupings', 'clear intruders', 'gold', 'standard answers', 'Judges', 'did', 'not answer', '%', 'gold', 'standard answers', 'not paid', 'solicitation', 'attracted', 'judgments', 'models', 'tasks', 'judgments', 'trustworthy judges']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," \\begin{figure} \\ centering \\includegraphics[width=.65\\textwidth]{fig/bestCumLL}  \\caption{Best cumulative log complete likelihood for each tested            value. Lower            values result in deeper hierarchies.}  \\label{fig:bestcumLL}  \\end{figure}\n","NOUN PHRASES:\n"," ['figure', 'fig/bestCumLL', 'Best cumulative log complete likelihood', 'tested', 'value', 'values', 'result', 'hierarchies', 'fig', 'bestcumLL', 'figure']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The Gibbs sampling algorithm was run on HD TM for various values of $\\gamma$ , and Figure ~ \\ref{fig:bestcumLL} shows the best cumulative log likelihood for each of the tested values of $\\gamma$ .\n","We observe that HD TM with $\\gamma = 0.05$ achieved the best likelihood score .\n","Likelihood scores decreased steadily for increasing $\\gamma$ values , and HD TM with $\\gamma = 0.95$ achieved the worst likelihood score .\n","NOUN PHRASES:\n"," ['Gibbs', 'sampling', 'algorithm', 'HD TM', 'various values', 'fig', 'bestcumLL', 'shows', 'cumulative log likelihood', 'tested values', 'observe', 'HD TM', '=', 'achieved', 'likelihood score', 'Likelihood scores', 'decreased', 'increasing', 'values', 'HD TM', '=', 'achieved', 'likelihood score']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n","PARAGRAPH:\n"," The { \\em model precision } is measured based on how well the intruders were detected by the judges .\n","Specifically , if the intruder word $w^m_k$ is from model $m$ and task $k$ , and $i^m_{k,j}$ is the intruder selected by the human judge $j$ on task $k$ in model $m$ then\n","NOUN PHRASES:\n"," ['intruders', 'judges', 'intruder word', 'task', 'k', 'k', 'j', 'intruder', 'selected', 'human judge', 'task', 'model']\n","NOUN LOCATIONS:\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0.]\n","****************************************************************************************************************************************************************************************************************************************************************************************\n","\n"]}]},{"cell_type":"code","source":["compute(\"\"\"We defer to \\\\cref{sec:compstat_general} the systematic analysis of when a wishful Receiver would take an action more / less often than a Bayesian .\\nHowever , we can already point out that such differences in behavior between a Bayesian and a wishful Receiver , i.e. how sets $\\\\Delta_a^W$ are distorted with respect to $\\\\Delta_a^B$ for every $a\\\\in A$ are systematically related to the asymmetry of the payoff matrix and will be all the more pronounced the closer the cost parameter $\\\\theta$ is to $0$ .\\n\\\\end{comment}  \\\\begin{comment}  \\\\subsubsection{Investors application with            }  \\\\label{secap:investor_infinite_state}\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"YbVVuQiY9Umf","executionInfo":{"status":"ok","timestamp":1642284340516,"user_tz":480,"elapsed":11,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"b72dfc93-0c56-4f15-a1b2-a8fbcff1f827"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'We defer to \\\\cref{sec:compstat_general} the systematic analysis of when a wishful Receiver would take an action more / less often than a Bayesian .\\nHowever , we can already point out that such differences in behavior between a Bayesian and a wishful Receiver , i.e. how sets $\\\\Delta_a^W$ are distorted with respect to $\\\\Delta_a^B$ for every $a\\\\in A$ are systematically related to the asymmetry of the payoff matrix and will be all the more pronounced the closer the cost parameter $\\\\theta$ is to $0$ .\\n\\\\end{comment}  \\\\begin{comment}  \\\\subsubsection{Investors application with            }  \\\\label{secap:investor_infinite_state}'"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def create_arrays(expected, predicted, len_text):\n","  expected_array = np.zeros(len_text)  \n","  predicted_array = np.zeros(len_text)\n","  for key in expected.keys():\n","    start_index = expected[key][\"start\"]\n","    end_index = expected[key][\"end\"]\n","    expected_array[start_index : end_index + 1] = 1\n","\n","  \n","  # for key in predicted.keys():\n","  #   start_index = predicted[key][\"start\"]\n","  #   end_index = predicted[key][\"end\"]\n","  #   predicted_array[start_index : end_index + 1] = 1\n","\n","  return (expected_array, predicted_array)"],"metadata":{"id":"hXlFMmaEC4ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare(expected, predicted): #Computes the F1 score between expected and predicted\n","  expected_array = np.array(len())"],"metadata":{"id":"UIlWJDCB-_AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def framework(df):\n","  results = []\n","  for column in df:\n","    paragraph = df[column][\"text\"]\n","    expected = df[column][\"entity\"]\n","    primary_description = extract_primary_description(expected)\n","    predicted = compute(paragraph)\n","    # expected_array, predicted_array = create_arrays(primary_description, predicted, len(paragraph))\n","    expected_array, predicted_array = create_arrays(primary_description, predicted, len(paragraph))\n","    predicted_array = findNounsWithLocs(paragraph)\n","    result = f1_score(expected_array, predicted_array, average= \"binary\", zero_division = 1)\n","    results.append(result)\n","  print(\"Average F1 score is:\", sum(results) / len(results))\n"],"metadata":{"id":"Fuy6yJ6569ZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\"The current F1 score of 0.355 arises when we leave the predicted_array all full of zeroes and compare with the expected.\"\n","# 0.261 is our current best implementing NLP techniques w/ <25\n","framework(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JJo6VII_N-d","executionInfo":{"status":"ok","timestamp":1642284343790,"user_tz":480,"elapsed":3280,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"8bc55d6a-1213-471a-a31c-4cae01eea247"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average F1 score is: 0.2611429499175073\n"]}]},{"cell_type":"code","source":["dataFiles = []\n","pwd = '/content/drive/My Drive/primary_description/Practice'\n","for filename in os.listdir(pwd):\n","    if filename.endswith(\"json\"): \n","        dataFiles.append(pwd + '/' + filename)\n","for filename in dataFiles:\n","  print(filename.replace('/content/drive/My Drive/primary_description/Practice/', \"\"))\n","  framework(pd.read_json(filename))\n","  print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PktY8mjkTvfD","executionInfo":{"status":"ok","timestamp":1642284395906,"user_tz":480,"elapsed":52118,"user":{"displayName":"Ishan Khare","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07661744089927050695"}},"outputId":"a420b873-26dd-452d-de92-4425c7927110"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["physics.atom_ph-ann10.json\n","Average F1 score is: 0.24100718166495289\n","\n","q_bio.qm-ann11.json\n","Average F1 score is: 0.24836546363431322\n","\n","cs.ai-ann0.json\n","Average F1 score is: 0.2611429499175073\n","\n","cs.ai-ann3.json\n","Average F1 score is: 0.292149992910222\n","\n","econ.th-ann6.json\n","Average F1 score is: 0.18309014547466393\n","\n","physics.atom_ph-ann8.json\n","Average F1 score is: 0.10158271561332784\n","\n","econ.th-ann5.json\n","Average F1 score is: 0.10712998732570396\n","\n","physics.atom_ph-ann9.json\n","Average F1 score is: 0.2622680957747418\n","\n","math.co-ann7.json\n","Average F1 score is: 0.27230732143308856\n","\n","cs.ai-ann2.json\n","Average F1 score is: 0.2836446512781167\n","\n","econ.th-ann4.json\n","Average F1 score is: 0.23045668243813608\n","\n"]}]}]}